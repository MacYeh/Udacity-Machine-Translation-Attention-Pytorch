{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Translation Project (PyTorch Framework)\n",
    "\n",
    "## Introduction\n",
    "In this notebook, the machine translation end2end pipeline is implemented; two DL models are implemented with **PyTorch** Frameworks; the goal is to translate from English to French\n",
    "\n",
    "- **Preprocess Pipeline** - Convert text to sequence of integers.\n",
    "- **Model 1** Bi-Directional RNN with (GRU/LSTM) cells; the neural network includes word embedding, encoder/decoder\n",
    "- **Model 2** Implement Attention Model with Bi-Directional RNN cells \n",
    "- **Prediction** Run the model on English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport helper, tests\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "import project_tests as tests\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import logger\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify access to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We begin by investigating the dataset that will be used to train and evaluate your pipeline.  The most common datasets used for machine translation are from [WMT](http://www.statmt.org/).  \n",
    "### Load Data\n",
    "The data is located in `data/small_vocab_en` and `data/small_vocab_fr`. The `small_vocab_en` file contains English sentences with their French translations in the `small_vocab_fr` file. Load the English and French data from these files from running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "# Load English data\n",
    "english_sentences = helper.load_data('data/small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = helper.load_data('data/small_vocab_fr')\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files\n",
    "Each line in `small_vocab_en` contains an English sentence with the respective translation in each line of `small_vocab_fr`.  View the first two lines from each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "small_vocab_fr Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "small_vocab_en Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "small_vocab_fr Line 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137861, 137861)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_sentences), len(french_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "Three steps in the text preprocess\n",
    "\n",
    "- 1. **Vocabulary Creation**\n",
    "- 2. **Tokenize** Implemented with Keras\n",
    "- 3. **Padding to the same length** Implemented with Keras\n",
    "\n",
    "The complexity of the problem is determined by the complexity of the vocabulary.  A more complex vocabulary is a more complex problem.  Let's look at the complexity of the dataset we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    text_tokenizer = Tokenizer()\n",
    "    text_tokenizer.fit_on_texts(x)\n",
    "    text_tokenized = text_tokenizer.texts_to_sequences(x)\n",
    "    \n",
    "    return text_tokenized, text_tokenizer\n",
    "\n",
    "tests.test_tokenize(tokenize)\n",
    "\n",
    "# Tokenize Example output\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    max_length = 0\n",
    "    \n",
    "    if length!=None:\n",
    "        max_length = length\n",
    "    else:\n",
    "        for i in x:\n",
    "            if len(i) > max_length:\n",
    "                max_length = len(i)\n",
    "                \n",
    "    return pad_sequences(x, maxlen=max_length, padding='post')\n",
    "\n",
    "tests.test_pad(pad)\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 344\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders\n",
    "\n",
    "To write the dataloaders format in Pytorch\n",
    "- Split Dataset to Train and Validation and Test\n",
    "- Applied into customized dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split(x_data, y_data, split_ratio=0.2, index=[0]):\n",
    "    \n",
    "    assert(x_data.shape[0] == y_data.shape[0])\n",
    "    data_length = x_data.shape[0]\n",
    "    if len(index)<2:\n",
    "        index = np.random.permutation(data_length).tolist()\n",
    "    train_data_x = x_data[index[0:int(data_length*(1-split_ratio))], :]\n",
    "    train_data_y = y_data[index[0:int(data_length*(1-split_ratio))], :]\n",
    "    test_data_x = x_data[index[int(data_length*(1-split_ratio)):-1], :]\n",
    "    test_data_y = y_data[index[int(data_length*(1-split_ratio)):-1], :]\n",
    "    train_data = (train_data_x, train_data_y)\n",
    "    test_data = (test_data_x, test_data_y)\n",
    "    \n",
    "    return train_data, test_data, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    ### Sequence Dataset\n",
    "    \n",
    "    def __init__(self, sequences_in, sequences_out):\n",
    "        super().__init__()\n",
    "        self.len = sequences_in.shape[0]\n",
    "        self.x_data = torch.from_numpy(sequences_in).long()\n",
    "        self.y_data = torch.from_numpy(sequences_out).long()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample file exists\n"
     ]
    }
   ],
   "source": [
    "file_path = 'random_sample.npy'\n",
    "if os.path.exists(file_path):\n",
    "    print('sample file exists')\n",
    "    index = np.load(file_path)\n",
    "    train_data, test_data, index = train_valid_split(preproc_english_sentences, preproc_french_sentences, split_ratio=0.2, \n",
    "                                                    index=index)\n",
    "else:\n",
    "    print('sample file doesn\\'t exist, regenerate it')\n",
    "    train_data, test_data, index = train_valid_split(preproc_english_sentences, preproc_french_sentences, split_ratio=0.2, \n",
    "                                                    index=[0])\n",
    "    np.save(file_path, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([116900,  45967,   4282, ...,  66781,  53707,  61948])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110288, 21)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = train_data[1].reshape(-1, 21)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110288\n",
      "27572\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TimeSeriesDataset(train_data[0], train_data[1].reshape(-1, max_french_sequence_length))\n",
    "test_dataset = TimeSeriesDataset(test_data[0], test_data[1].reshape(-1, max_french_sequence_length))\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': 27572, 'train': 110288}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = {'train':len(train_dataset), 'test':len(test_dataset)}\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = dict()\n",
    "dataloaders['train'] = train_loader\n",
    "dataloaders['test'] = test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "- **Model 1** Bi-Directional RNN with (GRU/LSTM) cells; the neural network includes word embedding, encoder/decoder\n",
    "- **Model 2** Implement Attention Model with Bi-Directional RNN cells \n",
    "\n",
    "After experimenting with the four simple architectures, you will construct a deeper architecture that is designed to outperform all four models.\n",
    "### Ids Back to Text\n",
    "The neural network will be translating the input to words ids, which isn't the final form we want.  We want the French translation.  The function `logits_to_text` will bridge the gab between the logits from the neural network to the French translation.  You'll be using this function to better understand the output of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define help Model (Customized TimeDistributed Model)\n",
    "class TimeDistributed(nn.Module):\n",
    "    \n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size)\n",
    "\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().reshape(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Feedthrough Model\n",
    "class Model1(nn.Module):\n",
    "    \n",
    "    def __init__(self, english_vocab_size, french_vocab_size, max_french_sequence_length, \n",
    "                 embedding_dim=200, hidden_dim=100, rnn_module = nn.LSTM):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.emb_vector = nn.Embedding(english_vocab_size+1\n",
    "                                       , embedding_dim)\n",
    "        self.enc_rnn_1 = rnn_module(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=1, \n",
    "                                    batch_first=True, bidirectional=True)\n",
    "        self.dec_rnn_1 = rnn_module(input_size=2*hidden_dim, hidden_size=hidden_dim, num_layers=1, \n",
    "                                    batch_first=True, bidirectional=True)\n",
    "        #self.bn = nn.BatchNorm1d(2*hidden_dim)\n",
    "        self.fc = nn.Linear(2*hidden_dim, french_vocab_size+1)\n",
    "        #self.time_series = TimeDistributed(self.fc, batch_first=True)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        embd_inputs = self.emb_vector(inputs)\n",
    "        en_out, en_hn = self.enc_rnn_1(embd_inputs)\n",
    "        en_out_end = en_out[:, -1]\n",
    "        decode_inputs = en_out_end.view(en_out_end.size()[0], 1, -1)\n",
    "        decode_inputs = decode_inputs.repeat(1, max_french_sequence_length, 1)\n",
    "        de_out, dn_hn = self.dec_rnn_1(decode_inputs)\n",
    "        # Add Batch Norm \n",
    "        bn_1 = nn.BatchNorm1d(de_out.shape[2]).to(device)\n",
    "        b_de_out = bn_1(de_out.contiguous().view(de_out.shape[0], de_out.shape[2], de_out.shape[1])) \n",
    "        #shape=(batch, catogories, time-series-length)\n",
    "        # softmax along the catogories axis\n",
    "        outputs = self.fc(b_de_out.view(de_out.shape[0], de_out.shape[1], de_out.shape[2]))\n",
    "        #outputs = F.softmax(self.fc(b_de_out.view(de_out.shape[0], de_out.shape[1], de_out.shape[2])), \n",
    "                            #dim=2) #shape=(batch, time-series-length, catogories)\n",
    "        #outputs = F.softmax(self.time_series(b_de_out.contiguous().\n",
    "        #                                    reshape(de_out.shape[0], de_out.shape[1], de_out.shape[2])), dim=2)\n",
    "        #outputs = F.softmax(TimeDistributed(self.fc, batch_first=True)(de_out), dim=2)\n",
    "        #outputs = torch.exp(outputs)\n",
    "        \n",
    "        return outputs.view(-1, outputs.shape[2], outputs.shape[1]) #shape=(batch, catogories, time-series-length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define help Model (Customized TimeDistributed Model)\n",
    "class TimeDistributed(nn.Module):\n",
    "    \n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size)\n",
    "\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Scoring Function\n",
    "def score_multiply(hx, enc_h):\n",
    "    score = F.softmax(torch.matmul(enc_h, \n",
    "                                   hx.view(hx.shape[0], hx.shape[1], 1)),\n",
    "                      dim=1)\n",
    "    batch = score.shape[0]\n",
    "    seq_length = score.shape[1]\n",
    "    enc_h_new = torch.mul(enc_h, score)\n",
    "    atten_vec = torch.sum(enc_h_new, dim=1)\n",
    "    \n",
    "    return atten_vec\n",
    "\n",
    "## Define Attention_Decoder_Model\n",
    "class Attention_Decode(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_h, input_dim, hidden_dim, rnn_module=nn.LSTMCell):\n",
    "        \n",
    "        # enc_h is encoded hidden tensor \n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.enc_h = enc_h\n",
    "        self.dec_rnn = rnn_module(input_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, ini_x, ini_hc):\n",
    "        \n",
    "        # ini_x is the tensor for the initial input \n",
    "        # ini_hc is the tensor for the initial hidden layer; LSTMcell will be h and c states\n",
    "        # hx, cx = LSTMcell(ini_x, ini_hc)\n",
    "        # atten_vec = score(hx, enc_h) \n",
    "        # Glued atten_vec to hx => cat(atten_vec, hx)\n",
    "        # x_next = tanh(wc[Glued_vector])\n",
    "        # hc_next = (hx, cx)\n",
    "        # foward (x_next, h_next) to create the next layer (repeat length times)\n",
    "        \n",
    "        hx, cx = self.dec_rnn(ini_x, (ini_hc[0], ini_hc[1]))\n",
    "        atten_vec = score_multiply(hx, self.enc_h)\n",
    "        glued_vector = torch.cat((atten_vec, hx), dim=1)\n",
    "        x_next = F.tanh(nn.Linear(glued_vector.shape[1], self.input_dim)(glued_vector))\n",
    "        hc_next = (hx, cx)\n",
    "  \n",
    "        return x_next, hc_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Feedthrough Model\n",
    "class Model2(nn.Module):\n",
    "    \n",
    "    def __init__(self, english_vocab_size, french_vocab_size, max_french_sequence_length, \n",
    "                 embedding_dim=200, hidden_dim=100, rnn_encode_module=nn.LSTM, rnn_decode_cell=nn.LSTMCell):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_vector = nn.Embedding(english_vocab_size+1, embedding_dim)\n",
    "        self.enc_rnn_1 = rnn_encode_module(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=1, \n",
    "                                    batch_first=True, bidirectional=True)\n",
    "        self.rnn_decode_cell = rnn_decode_cell\n",
    "        self.fc = nn.Linear(embedding_dim, french_vocab_size+1)\n",
    "        #self.time_series = TimeDistributed(self.fc, batch_first=True)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        embd_inputs = self.emb_vector(inputs)\n",
    "        en_out, en_hn = self.enc_rnn_1(embd_inputs)\n",
    "        enc_h = en_out # Treat the output as the hidden state\n",
    "        #Initiate Attention Decode Module        \n",
    "        Decode_process = nn.ModuleList([Attention_Decode(enc_h, input_dim=self.embedding_dim, hidden_dim=self.hidden_dim*2, \n",
    "                                                         rnn_module=nn.LSTMCell).to(device) \n",
    "                                        for i in range(max_french_sequence_length)])\n",
    "        # 1st x_next and h_next\n",
    "        # x_next is <END>\n",
    "        # hc_next is zero tensor with the correct dimension \n",
    "        h_0 = torch.zeros((inputs.shape[0], self.hidden_dim*2))\n",
    "        c_0 = torch.zeros((inputs.shape[0], self.hidden_dim*2))\n",
    "        # i_0 should implement with the <END> embed matrix (batch, embd_dim)\n",
    "        i_0 = self.emb_vector(torch.zeros((inputs.shape[0])).type(torch.LongTensor))\n",
    "        x_next = i_0\n",
    "        hc_next = (h_0, c_0)\n",
    "        x_sequence = list()\n",
    "        hc_sequence = list() # h contains (h, c)\n",
    "        \n",
    "        for i, decode in enumerate(Decode_process):\n",
    "            x_next, hc_next = decode(x_next, hc_next)\n",
    "            x_sequence.append(x_next)\n",
    "            hc_sequence.append(hc_next)\n",
    "        \n",
    "        # stack x_sequence and hc_sequence\n",
    "        x_sequence = [i.view(i.shape[0], 1, -1) for i in x_sequence]\n",
    "        outputs_c = torch.cat(x_sequence, dim=1)\n",
    "        # BatchNorm\n",
    "        bn_1 = nn.BatchNorm1d(outputs_c.shape[2]).to(device)\n",
    "        outputs_c_bn = bn_1(outputs_c.contiguous().view(outputs_c.shape[0], outputs_c.shape[2], outputs_c.shape[1])) \n",
    "        # softmax along the final matrix\n",
    "        #\n",
    "        outputs = F.softmax(self.fc(outputs_c_bn.view\n",
    "                                    (outputs_c.shape[0], outputs_c.shape[1], outputs_c.shape[2])), dim=2)    \n",
    "        #outputs = F.softmax(self.time_series(outputs_c_bn.view\n",
    "                                             #(outputs_c.shape[0], outputs_c.shape[1], outputs_c.shape[2])), dim=2)\n",
    "        \n",
    "        return outputs.view(-1, outputs.shape[2], outputs.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loss/Accuracy and Optimization/LRrate Function Setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model1(english_vocab_size, french_vocab_size, max_french_sequence_length, \n",
    "                 embedding_dim=200, hidden_dim=100, rnn_module = nn.LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning_Rate Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mac_lr_finder(model, criterion, lr_logrange=[-5, 0]):\n",
    "    \n",
    "    # numbers of iterations\n",
    "    n_iter = len(list(dataloaders['train'])) #108\n",
    "    \n",
    "    # lr rate matrix \n",
    "    lr_matrix = np.logspace(lr_logrange[0], lr_logrange[1], n_iter)\n",
    "    loss_matrix = np.zeros(n_iter)\n",
    "    \n",
    "    for epoch in range(1):\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode; won't alter to different dropout and BatchNorm weights\n",
    "\n",
    "            idx = 0\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Set Optimizer\n",
    "                #optimizer = optim.SGD(model.parameters(), \n",
    "                 #                     lr=lr_matrix[idx], \n",
    "                  #                    momentum=1,\n",
    "                   #                   nesterov=True)\n",
    "                \n",
    "                optimizer = optim.Adam(model.parameters(),\n",
    "                                       lr=lr_matrix[idx],\n",
    "                                       amsgrad=True)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward\n",
    "                # turn on the tracking history in train and turn off the tracking history in others\n",
    "                with torch.set_grad_enabled(phase == 'train'): #set gradient calculation enabled\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    loss_matrix[idx] = loss.item()\n",
    "                \n",
    "                idx = idx+1\n",
    "                        \n",
    "    lr_matrix = lr_matrix.reshape((lr_matrix.shape[0], 1))\n",
    "    loss_matrix = loss_matrix.reshape((loss_matrix.shape[0], 1))\n",
    "\n",
    "    return np.concatenate((lr_matrix, loss_matrix), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_loss_matrix = mac_lr_finder(model=model, criterion=criterion, lr_logrange=[-5, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x19980061a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_loss_matrix[:, 0], lr_loss_matrix[:, 1])\n",
    "plt.xlabel('learning rate(log scale)')\n",
    "plt.ylabel('loss')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEOCAYAAACEiBAqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XmcnFWd7/HPr6qr973TSWffQ8IaICCIsoMojDCKgDqKijLujM6oeL0zV8c7yui4oF4VHL0yVxEQUBBRBCQsQ1gCCSEhIYHsS6e703tXVdd27h/1VC/p7qRDuqq6qr7v16tf/dTzPFV1TndS3z7nPM855pxDREQKly/bBRARkexSEIiIFDgFgYhIgVMQiIgUOAWBiEiBUxCIiBQ4BYGISIFTEIiIFDgFgYhIgVMQiIgUuKJsF2A8pkyZ4ubNm5ftYoiI5JQXXnihzTnXeLjzciII5s2bx+rVq7NdDBGRnGJmO8ZznrqGREQKnIJARKTAKQhERAqcgkBEpMApCERECpyCQESkwCkIRESy7LWWXsLReNbeX0EgIpJF4WicS3/wJHc8tzNrZVAQiIhkUVcoSn8sQXN3f9bKoCAQEcmi7lA0+T0czVoZFAQiIlnUHY4lv4cUBCIiBSnVEujxAiEbFAQiIlmkriERkQKnriERkQLXE061CNQ1JCJSkLpDahGIiBS01NhAfyxBf2zw7uLXWnq55fHXOdCb/vsLFAQiIlk0tCUw9Mqhdbs7+eafNmXkaiIFgYhIFg39oB8aCu19EQDqKorTXgYFgYhIFg29bHTogHFHMILfZ1SXpn9peQWBiEgWdYeiTKsuGdhOae+LUldejJmlvQwKAhGRLOoOx5hVV+5tDwZBR1+E+opARsqgIBARyaKecJSZtWXe9mDXUHswQl15+scHQEEgIpI1/bE44WiCmXXJIDh4sLg+AwPFoCAQEcmaVAugqboUv89GdA1l4oohUBCIiGRNqgVQUxagurRo4C7jRMLREYxQr64hEZH8lmoRVJUWUV0WGGgRdIejJBzqGhIRyXepD/7qsgDVpYGBYEjdTKYgEBHJc6muoOrSAFWlRQNdRR3BzN1VDAoCEZGsGWwRFFFdOtg11N6X/J43YwRm5jezNWb2gPd4vpk9a2ZbzOxOM8tMTUVEJpnUWgRVpQGqywYHizsG5hnKnxvKbgA2Dnn878D3nHOLgQ7gugyUQURk0ukOxfAZVBT7h7cIgnk0RmBms4BLgf/0HhtwPnC3d8ptwBXpLIOIyGTVHY5SXRbAzKgqDRCMxInGE3T0RSgp8lEW8GekHOluEXwf+CKQ8B43AJ3OudR91LuBmWkug4jIpNQdilJdmuz+qS5LzjLaG44N3FWciQnnII1BYGaXAS3OuReG7h7lVDfG8683s9Vmtrq1tTUtZRQRyaaecIwqb5rpVCB0h6O092VuniFIb4vgLOCdZrYduINkl9D3gVozS02wPQvYO9qTnXO3OudWOOdWNDY2prGYIiLZ0R0e2iLwgiAUoz0YoaEyD4LAOfdl59ws59w84Brgr8659wOPAVd6p10L3JeuMoiITGbdodhAl1BqAZrucDQ5z1CetAjG8iXg82b2Gskxg59noQwiIlk3tEVQleoaCkUzOvMoQPrXQAOccyuBld72VuD0TLyviMhklhwjGD5Y3B6M0B2O5X2LQESk4MXiCXr7h3QNeWMEO9uDABlbnQwUBCIiWdHbPzjPEEBlcRFmsKMtGQSZmmcIFAQiIlkxMOGc1xLw+YyqkiK2H+gDMjfPECgIRESyontgnqHBodqq0gA7DqhFICJSEAZmHi0dHAuoLgsQisaBzM0zBAoCEZGsGOwaGmwRVA9pHeiqIRGRPDdWiwCgqqSI4qLMfTwrCEREsiC1LOWwIPC2Mzk+AAoCEZGsSC1LWTlssDi5rSAQESkA3eEoVSVF+H2DkzKnuobqyzN3MxkoCEREsqI7FBt26SgMDharRSAiUgB6vNXJhhpsESgIRETy3tCZR1M0WCwiUkCGrkWQkuoayuTNZKAgEBHJiu5wdGAK6pSmmlIAZtWVZbQsGVmPQEREBjnnONAboeGgv/wXNFby8OfOZtHUyoyWR0EgIpJhfZE4oWicKVUlI44tnlaV8fKoa0hEJMPaevoBaKwcGQTZoCAQEcmw1l4vCEZpEWSDgkBEJMNSLYIpahGIiBQmtQhERApcW08/Psv8/QJjURCIiGRYa28/9RUlwyacyyYFgYhIhrX2RJhSOTlaA6AgEBHJuNbe/kkzPgAKAhGRjGvr6Z809xCAgkBEJKOcc2oRiIgUsp7+GJFYYtLcQwAKAhGRjGrtmVz3EICCQEQkoxQEIiIFrq13ck0vAQoCEZGMUotARKTAtfX24/cZtQctXJ9NCgIRkQxq7elnSmUxvkkyvQQoCEREMqqtNzKpxgdAQSAiklGtPZPrZjJQEIiIZFRbb3/htAjMrNTMnjOzl8xsg5l9zds/38yeNbMtZnanmU2eKfhERNLIOUfbJJteAtLbIugHznfOnQQsBy4xszOAfwe+55xbDHQA16WxDCIik0ZXKEo07gqnReCSer2HAe/LAecDd3v7bwOuSFcZREQmk8l4DwGkeYzAzPxmthZoAR4GXgc6nXMx75TdwMx0lkFEZLIYWKu4UFoEAM65uHNuOTALOB1YNtppoz3XzK43s9Vmtrq1tTWdxRQRyYjBFsHkGhrNyFVDzrlOYCVwBlBrZkXeoVnA3jGec6tzboVzbkVjY2MmiikiklZtvREAGitLs1yS4dJ51VCjmdV622XAhcBG4DHgSu+0a4H70lUGEZHJpLWnn2K/j+qyosOfnEHpLM104DYz85MMnLuccw+Y2SvAHWb2v4E1wM/TWAYRkUkjeQ9BMWaTZ3oJSGMQOOfWASePsn8ryfECEZGCsr87zJRJdsUQ6M5iEZGM2dkeZE59ebaLMYKCQEQkAyKxBLvagyyYUpHtooygIBARyYBdHUESDuYpCERECtO21j4A5isIREQK07Y2BYGISEHbdqCPuvIAteWT665iUBCIiGTEtta+STk+AAoCEZGM2H6gb1J2C4GCQEQk7YKRGPu6wpPy0lFQEIiIpN32tiAwOS8dBQWBiEjabT8wea8YAgWBiEjapS4dndegIBARKUjb2vqYVl1CRcnkmn46RUEgIpJm29r6Jm1rAMYZBGZ2g5lVW9LPzexFM7s43YUTEclF//1aGxd8ZyV7O0NAMggWNOZ4EAAfcc51AxcDjcCHgZvSVioRkRy2bncXr7f28aV71tEVjNLeF5m0A8Uw/iBILafzDuD/OudeGrJPRESG6Awl1yZ+cksb33hwIzB5B4ph/CuUvWBmfwHmA182syogkb5iiYjkru5QlCmVxSxtqubO1bsA8qJr6DrgRuA051wQCJDsHhIRkYN0BqPUlhfz71eeSGVJET6D2ZNwZbKU8bYIzgTWOuf6zOzvgFOAm9NXLBGR3NUVilJbFmBmbRnfveokXtzZSUmRP9vFGtN4WwQ/AYJmdhLwRWAH8F9pK5WISA7rDEapKQsAcPFxTdz49qVZLtGhjTcIYs45B1wO3OycuxmoSl+xRERyV1coSk15INvFGLfxdg31mNmXgQ8AbzUzP8lxAhEROUhXaLBFkAvG2yK4GugneT9BMzAT+HbaSiUikqOi8QS9/TFqyybfSmRjGVcQeB/+vwZqzOwyIOyc0xiBiMhBukNRAGrKJue8QqMZ7xQTVwHPAe8BrgKeNbMr01kwEZFc1OUFwWRcm3gs442sr5C8h6AFwMwagUeAu9NVMBGRXNQ50CLIvzECXyoEPAeO4LkiIgUj1SLIx6uG/mxmDwG/8R5fDTyYniKJiOSurmDutQjGFQTOuS+Y2buBs0hONnerc+53aS2ZiEgOGhgjyLcgAHDO3QPck8ayiIjkvE6vRVCdL0FgZj2AG+0Q4Jxz1WkplYhIjuoKRaksKSLgz51h1EMGgXNO00iIiByBzlAkp8YHQFf+iIhMqO4cm14CFAQiIhNq6MyjuUJBICIygbpCUWpz6B4CUBCIiEyoTnUNDTKz2Wb2mJltNLMNZnaDt7/ezB42sy3e97p0lUFEJJOcczm3FgGkt0UQA/7RObcMOAP4lJkdS3Lt40edc4uBR73HIiI5LxxNEIkl1CJIcc7tc8696G33ABtJrmNwOXCbd9ptwBXpKoOISCYN3lWcOzOPQobGCMxsHnAy8CwwzTm3D5JhAUwd4znXm9lqM1vd2tqaiWKKiByVzlAEyK15hiADQWBmlSSnpvgH51z3eJ/nnLvVObfCObeisbExfQUUEZkgqQnndNXQEGYWIBkCv3bO3evt3m9m073j04GWsZ4vIjLZPf16G+FoHBgyBbVaBElmZsDPgY3Oue8OOXQ/cK23fS1wX7rKICKSTi3dYd73s2f59bM7gdxclAbS2yI4C/gAcL6ZrfW+3gHcBFxkZluAi7zHIiI5p6WnH4AXd3QAQ9YrzrGuobStruyce4rkLKWjuSBd7ysikimpKafX7OwYeOz3GVUlubNwPejOYhGRN6wjmLxKaG9XmOauMF2hKNWlRSR7xnOHgkBE5A1KjQkArN3VQWcoSm15bt1DAAoCEZE3rLMv2SII+I01OzuTLYIcGyiGNI4RiIjku45glIpiP0uaqlizs5P+WFwtAhGRQtIZjFBbXszJs+tYt6eTtt7cW50MFAQiIm9YRzBCXUWAk+fUEo4m2NMZyrm7ikFBICLyhnUEo9SVF7N8du3APrUIREQKSKpraFZdGVMqSwAFgYhIQUm2CAKYGSfPSbYKFAQiIgUinnB0h6PUeh/8qSDQVUMiIgWiKxTFucEP/rcuasTvM+Y1lGe5ZEdO9xGIiLwBqekl6iqSLYITZtXw8lcvprw49z5W1SIQEXkDOr0gGNoVlIshAAoCEZE3pKMvOc9QXQ6OCRxMQSAi8gYMdA3l4A1kB1MQiIiMIRJL0NcfG/VY58D6xGoRiIjkra8/8Ap/88OnSCTciGMdwQh+n1FdmpvjAkMpCERExvDM1gNsbevj6dcPjDjWEUzeQ5Bri9CMRkEgIjKKvv4Yr7X2AnDX6l0jjneFIjk5wdxoFAQiIqPYsLcb52DBlAr+vKGZrmB02PGOvmheXDEECgIRkVG9vKcLgH++7FgisQT3v7Rn2PEOb8K5fKAgEBEZxcu7O2mqLuXcYxpZNr2a376we9jxTm/CuXygIBARGcXLe7o4fmYNZsZ7Tp3Fut1dbNzXPXA8uSiNWgQiIjlt54Egzo28NLQnHGVrWx8nzqoB4IqTZxLwG79dnWwVhCJx+mMJDRaLiOSy7W19nPMfj/HIxpYRx1IDxSd4QVBfUcxbFk1h5ebkuYN3FatFICKSszY19+AcrNnZMeLYem+g+ISZNQP73rSgga2tfbT29OfV9BKgaahFpEDtbO8DkoFwsHW7u5hRUzqw/CTA6fPrAVi9vZ1qbzGamjK1CEREctb2A0EANg0ZAE5JDRQPdfyMGsoCfp7d1j5iLYJcpyAQkYK00wuCvV3hYTeLdYejbBsyUJxSXOTjlLm1PLetnY5g/kxBDQoCEclTsXiCWDwx5vEd7X3Ue5d/bmoebBWkxgcObhEAnD6vgY3N3exqT4aIrhoSEZnEbrhzLR/8xXOjHovEEuzpCHHRsmnA8HGCl3aNHChOOW1+Hc7BI6/sp7zYT0mRPw0lzzwFgYjknX1dIf708j5WbT3Agd7+Ecf3dIZIODhtfj115YFhLYKVr7awtKmKhiEDxSknz64j4De2tvXlTbcQKAhEJA/d9fxuEg6cg6deaxtxfPuB5BVDcxvKWdpUzcZ9yRZBVyjK6h0dnL906qivW1bs58RZtUD+dAuBgkBE8kw84bhr9S7evLCBuvIAj7/aOuKc1EDx3IZylk6v4tXmHhIJxxObW4knHBcsGz0IYPAyUrUIREQmqSe3tLKnM8T73jSHs5c08sSW1hErjO04EKS82E9jZQnLmqoJRePsbA/y100t1JUHWD67bszXTwWBWgQiIpPUHc/tor6imIuOncY5Sxpp642wYe/wewV2HOhjTn05ZsbS6VVAclqJx15t4bxjpuL3jb3q2Klz6/AZA1cc5YO0BYGZ/cLMWsxs/ZB99Wb2sJlt8b6PHbsiIkeotaefRzbu592nzKSkyM9bFzcC8Pjm4fMJ7WgPMrehHIAl06rwGfzmuZ10BqOcN8b4QEp1aYDvXb2cD545Ly11yIZ0tgh+CVxy0L4bgUedc4uBR73HIiIT4r61e4glHFefNgeAxqoSjp9ZzeObB8cJEgnHzvYg8xoqACgN+Jk/pYKnXmvD7zPOXtJ42Pe5fPlMFk2tTE8lsiBtQeCcewJoP2j35cBt3vZtwBXpen8RKTybmnuYVl0y7EP6nCWNvLizk65Q8m7g5u4wkViCOV6LAGDp9GoATptXR01Z/vT9j1emxwimOef2AXjfD90GExE5Avu7wzTVlA3bd86SqcQTjqe9y0hTl46mWgQAy5qS4wQXLJ2WoZJOLpN2sNjMrjez1Wa2urV15OVfIiIHa+4K01Q9/EawU+bUUlVaxP0v7QUGLx2dUz/YIjhz4RSqSop423FNmSvsJJLpINhvZtMBvO8jV4TwOOdudc6tcM6taGw8fJ+diEhzd5im6tJh+4r8Pj5y1nz+tL6ZRzfuZ0d7kIDfmFE72HI4dW4d67568bDuokKS6SC4H7jW274WuC/D7y8ieSoYidETjjGtpnTEsU+dt4ilTVX8j9+9zPo9XcyuKx9xiajZ2JeM5rt0Xj76G2AVcIyZ7Taz64CbgIvMbAtwkfdYROSoNXeFAZg+ShAUF/n49pUn0dYb4cktbQX7l/9Y0rZCmXPuvWMcuiBd7ykihSsVBNOqRwYBJNcfvv7sBfxk5evDBopFS1WKSJ5o7k4GwcFjBEPdcMFidneECnZQeCwKAhHJCwNBMErXUEppwM8P33typoqUMybt5aMiIkdif1eYqtIiyov19+2RUhCISF5o7g6POlAsh6cgEJGc09Id5t0/eZrdHcGBfc3d/WMOFMuhKQhEJOes2nqAF3Z0DJtMbn/XyJvJZHwUBCKSc7bs7wUYWGcgFk/Q0hM+5ECxjE1BICI5Z0tLco3hDXu6AGjrjZBwY99DIIemIBCRnJNqEWxq7iEWT4zrHgIZm4JARHJKfyzOdm+pyf5Ygtdb+wbuKlbX0BujIBCRnLK1tY+EgyuWzwBgw94u9o/jZjIZm4JARHLKlpZkt9DFxzVRUuRj/Z5umrvDBPxGfXn+LCifSQoCEckpr+3vwe8zFk+rZOn0ajbs7aK5K8zUqlJ8vsKdSvpo5PW92M9vb6e1p5/kvw0jlkgQjiboj8UJ+HyUFfspDfjxe3FoGKUBP+XFfgJ+H9F4gmg8QX8sQTgaJxSN4xz4fYbfZ1SXBmioLKauvJhwNE5XKEpffwyz5PGA3ygv9lNWXERZwE+R3wj4fJiBcxB3jraefnZ1BNnbGaK4yEd9RQkNFcXMm1JBZUle/3pEeK2llz+u28dnL1g07vUANu/vZW5DOSVFfo6fUc39L+3FTN1CRyOvP2n+z2OvsfLV3F3mcnpNKbPqyuiPJegNxwhG4iScI+HADAI+I1Dko7TIT3mJn8qSIsqL/VR434P9cTpDUUKROAunVnDcjBqOnV7NgsYKqkqTC3RH4wn2dITw+4xZdWUFvTiHZN63H9rEQxv2c97SRk6cVTuu52xp6WGxtzj9cTNq+PWzO1m3u4vzlmoJ9Dcqr4Pg65cfT18khnOQcI6AP/mhWVyU/Gt/6F/5kDwnFI0TisSJxhME/D6Ki3wU+wdbDz6DWMIRizu6Q1EO9EXoCEYoLfJTXRagqjT5I40lHJFYglA0TrA/RjgaJ5ZwROOOhHP4fYbPoK68mNn15cysLSMaT9DeF6Glp59tbX281tLLns4Q9RXFzG2ooCzg81ZVMiD5Wql6BCNxevtjtHT309sfIxiJUV5cRF1FgIDfx31r9vKrZ3YO/Gwaq0qoKPazuyNELJH8AVSWFHFMUxUBv9HeF6EzGCXg91Fe7KeqNHns+Jk1LGysJBRJtoBae/rZ3RFkV0cIgMXTKlkytYpjmqpYMq2K4qJkc8s5R1coSjTucDh8ZtSUJcsmham5K8wjG5Or1f55ffO4giASS7D9QJC3Hz8dgONmVAMQjMR16ehRyOsgmF2fe6sQLUjT8syJhGNXR5BNzT1sbe1ja2svwWicS0+cztyGCmJxx6bmbjY195BIwPwpFdSWFRN3jmAkRkdflAdfbuY3z+0a8dpVJUXMqi/HOcdTW9qIxBMABPzGwsZKIrEEezpD9McSI55bXVrE7Ppyrlg+k3edMpOGyuELj4ejcbrDURoqSkYsLSi57Y7ndxJPOBZNreShDc188ZKlh33OtrY+4gnH4mnJFsExTVX4fUY84RQERyGvg0AG+XzG3IYK5h7FykzOOXa1h9h+oI/K0iJqywI0VJRQXVY00KUUiyf/YtvU3M0re7t5tbmH0oCfC5ZNpammbKCFkEg4OoNR2vv6Wbeni397cCPfemgTy6ZX09cfoysUozscJeKFx8zaMt5/xhyuOW0OxUU+Xm/ppbk7zGnz6qmv0JUiuSYWT3DHc7s4e0kjFy2byj/ft4HXWnpYNLXqkM9L3VG82DuvNOBn8dRKNjX3jLpWsYyPgkDGzcyY01B+yPVei/w+Fk2tZNHUSi47cca4X3vz/h7ufH4Xm/f3MLuunOqyIqpLA1SXBSgv9vPwK/v51p9f5Tt/2Uzc68qC5MD9mxc2cNGx02ioKKG8xE9POMaLOzp4cWcH9RXFfOUdy1g8LfnB4Zxj3e4uIvEE1aUBasoCTK0q0dUmGfbophaau8N87fLjOGlWLf983wb+vL6ZT59/mCDY34vPYEHj4B80x86oZlNzj1oER0FBIJPCkmlV/PNlx455/MNnzWfz/h5+v2YPFSVFLJpaSX1FMY9tauGBdfv4l/s2DDu/LODnxFk1rNnZydtvfpKPnb2AuvIAdzy3i61tfcPOLSnyMa+hgmOaqvjgmXNZMa8+LXWUQb96ZgfTa0q5YOlUivw+Tp5Ty0Mb9vPp8xcf8nlbWnqY21BBacA/sO/EmTXc++IeZtaVpbvYeUtBIDljybSqEf3Ip82r5wtvO4Z9XWF6+2P09ccI+H3eoLePtt5+vvngJn6y8nUAVsyt4xPnLqSpppTuUIz2YISdB/rY1tbHk1tauf+lvZy1qIFrTptDOBrnQF+EKZUlXL58RkEMbL/W0ktDRTF1aexu297Wx5Nb2vjchUso8n6mbzuuiZv+tIndHUFm1ZXT3BXm+e3tbG3tY8eBPuY0lPPOk2awZX8vi7wrhlKuOX0OCxormVmrIHijzDl3+LOybMWKFW716tXZLobksI37ugn47ZB90MFIjF8/s5Nbnnidtt7IsGPzGsr5/MXHcNkJ0/O2GykYifGmbzzKKXPquO0jp6ftfT59+4s8snE/T3zhPKZ63Tnb2vo47z9W8pnzF9HXH+dXz+wYuOhgWnUJLT39A1f3ffLcheMaWBYwsxeccysOd55aBFIQlk2vPuw55cVFfOzsBfzdGXPZvL+HuvJi6iuLeW7bAb7151f57G/WcPMjm3nv6XN49ymz8PmMF3d0sGFvF2csaODUuXU5fR/GH9ftoycc4/HNrazb3Tnu6/qPxAs7Onhg3T4+e8HigRCA5FVqx0yr4od/fQ2fwXtOnc0HzpzLwsZKyor97OsK8cd1+3h8c+vApaMycdQiEBmHeMLxwLq9/PLp7azZ2UnAb8QSjqH/fU6aXctH3zKfS45vylg30tpdnVSVFrGwsfLwJx/GVT9dRXN3mM5ghDMWNHDrBw/9h2R3OMrOA0GOn1kzbH9POEpxkY+SIv+w/YmE410/eZq9nSFWfuHcEYvM/3HdPh7dtJ9PnLNwYHBfjo5aBCITyO8zLl8+k8uXz2RTcze/W7OHiuIiVsyrY2lTNX9ct5efP7WNz/xmDdOqS3jv6XN47+lzhi2Usn5PF997eDOReIJPnLOQMxc2HFULYs3ODq6+5RlqygP85R/OHrNfPxZPDPTFj2Vray/PbW/nS5csJRyNc/OjW9jU3M3SptFbUpFYgmt/8Rxrd3Xy/auXc/nymQBs2d/DNbc+w4zaMu76+zMpKx4Mgz+s28vaXZ18+8oTR4QAwKUnTufSE/XXfjaoRSAyQeIJx8pXW/ivVTsG1tJd2lTFGQsaaO+LcP9Le6ktD1BS5GN/dz+nzavjtHn19PXHCEcTnLe0kbcd1zSucGjpDvM3P3oKnxmtPf2844Tp/OC9J484794Xd/M/f7+eb77rhIEP69Hc9KdN/OzJray68XyKi3ycddNfOX/ZNL5/9XIefqWZla+2cvVpszl5Th0A/+u+9dy2agcLGivYeSDIzz64gjkN5Vx9yzPEEwk6Q1EuO3EGP7hmOWbG3s4Q7/npKuoqAtz/qbfk7TjLZKMWgUiG+X3GBcumccGyaWxv6+OBdXt5Zms7dzyfnNrjU+ct5O/PWUix38ddq3fx05Wvs2bnViq8yQXvXL2L42dW8/mLlnDOkqlj3kndH4vz8V+9QHcoxr2ffDMPv7Kf7z68mUuOb+IdJwz+Rf3Cjg5uvOdlMPj8XS9RGvDztuOaRrxeLJ7gnhd3c94xjQP99h84cx63PvE6a3Z2sNubi+q3L+zmcxcuZkZtGbet2sFH3zKfz164mPf97Bk+8esXvPmrHL/9+Jn8xbvvY2lTFVOrSvjXP7xCLOH4wXuXKwQmIbUIRNIsEksQSyRGdIek/u+ZGbF4gvvW7uX7j25mV3uIuvIAb13cyFmLGphaXUpNWYBQJM7Tr7fx6MYWNjX38OP3n8I7TphONJ7gXT9+mj2dIe79xJuZN6WCvZ0h3vmj/6a82M/tH3sTn759Da/s7eaWD57KuUsah7U6HnllPx/9r9Xc+oFTudgLirbefi7+3hMsbKzgurcs4E3z6/mX+zfwh5f2AnDavDpu/9gZBPw+DvT2855bVtHRF+GO68/kmKYqnHPccMda7vfOP31+Pf9x5UmHvBlRJt54WwQKApHBcYPuAAAJz0lEQVRJJBpP8NCGZv66qYUnNreOuIzV7zNOnl3LVafN5qoVswf2b9nfw6U/fIpILEFTdSk+g+5wjN998s0snlZFVzDKNT97ho37uikN+JhRU0ZFSRHd4eTEgeXFRaz68vnDBrmdc8MCwznHvS/u4f6X9vKtK08cNv4RjMTojyaGjVOEInH+x+9e5oSZNXzozfPUEsgCBYFIjkskHDvag3QEI3SFovjMOHVu3ZjrVLze2ssTm1tZu6uTV5t7uPHtSzn3mMGpmTuDEX63Zg97OkLs6w7T1x8bmGbjgmVTh50r+UFBICJS4MYbBPl/z7yIiBySgkBEpMApCERECpyCQESkwCkIREQKnIJARKTAKQhERAqcgkBEpMDlxA1lZtYK7ABqgK4hh4Y+Hmt7CtA2AcU4+L2P5tyxjh+qfod7nIt1Hs++XK3zeH/Ho+1TnVXn1PbR1neuc67xsGc553LmC7h1rMeH2F6djvc+mnPHOn6o+h1h/XOizuPZl6t1Hu/vWHVWnQ9V54mq7+G+cq1r6A+HeDzWdrre+2jOHev4oep3uMe5WOfx7MvVOo/3dzzaPtVZdU5HnceUE11DR8PMVrtxzLWRT1TnwqA6579M1TfXWgRvxK3ZLkAWqM6FQXXOfxmpb963CERE5NAKoUUgIiKHoCAQESlwCgIRkQJX0EFgZuea2ZNm9lMzOzfb5ckUM6swsxfM7LJslyXdzGyZ9/u928w+ke3yZIKZXWFmPzOz+8zs4myXJxPMbIGZ/dzM7s52WdLJ+797m/f7ff9EvW7OBoGZ/cLMWsxs/UH7LzGzV83sNTO78TAv44BeoBTYna6yTpQJqjPAl4C70lPKiTMR9XXObXTOfRy4Cpj0lx1OUJ1/75z7GPAh4Oo0FndCTFCdtzrnrktvSdPjCOv/LuBu7/f7zgkrRCbuWkvHF3A2cAqwfsg+P/A6sAAoBl4CjgVOAB446Gsq4POeNw34dbbrlKE6XwhcQ/JD4rJs1ynd9fWe807gaeB92a5TpursPe87wCnZrlOG63x3tuuT5vp/GVjunXP7RJWhiBzlnHvCzOYdtPt04DXn3FYAM7sDuNw5903gUN0gHUBJOso5kSaizmZ2HlBB8h9VyMwedM4l0lrwN2iifsfOufuB+83sj8Dt6Svx0Zug37EBNwF/cs69mN4SH70J/r+cc46k/iR7LmYBa5nAHp2cDYIxzAR2DXm8G3jTWCeb2buAtwG1wI/SW7S0OaI6O+e+AmBmHwLaJmsIHMKR/o7PJdmcLgEeTGvJ0ueI6gx8hmTLr8bMFjnnfprOwqXJkf6eG4B/A042sy97gZHLxqr/D4AfmdmlTOA0FPkWBDbKvjHvmHPO3Qvcm77iZMQR1XngBOd+OfFFyYgj/R2vBFamqzAZcqR1/gHJD4xcdqR1PgB8PH3FybhR6++c6wM+PNFvlrODxWPYDcwe8ngWsDdLZcmUQqtzodUXVGcojDoPldH651sQPA8sNrP5ZlZMclD0/iyXKd0Krc6FVl9QnQulzkNltP45GwRm9htgFXCMme02s+ucczHg08BDwEbgLufchmyWcyIVWp0Lrb6gOhdKnYeaDPXXpHMiIgUuZ1sEIiIyMRQEIiIFTkEgIlLgFAQiIgVOQSAiUuAUBCIiBU5BIGljZr0ZeI93jnPq7Yl8z3PN7M3jPPcKM/sXb/urZvZP6S3d+Hh1eOAw55xgZr/MUJEki/JtriHJQ2bmd87FRzuWmlk0De9Z5N3UM5pzSa5j8fQ4XuqLTOS88RnknHvZzGaZ2Rzn3M5sl0fSRy0CyQgz+4KZPW9m68zsa0P2/96Sq6VtMLPrh+zvNbN/NbNngTPNbLuZfc3MXjSzl81sqXfeh8zsR972L83sB2b2tJltNbMrvf0+M/ux9x4PmNmDqWMHlXGlmX3DzB4HbjCzvzGzZ81sjZk9YmbTvOmCPw58zszWmtlbzazRzO7x6ve8mZ3lvd4SoN851zbKey03s2e8n8fvzKzO23+at2+VmX3bDlqsxDtnupk94b3/ejN7q7f/Eu/n85KZPertO937eazxvh8zyutVWHJxlOe98y4fcvgPJKc3kHyW7UUZ9JW/X0Cv9/1i4FaSMyr6SC4mcrZ3rN77XgasBxq8xw64ashrbQc+421/EvhPb/tDwI+87V8Cv/Xe41iS87kDXElyCmof0ERy/YkrRynvSuDHQx7XMXj3/UeB73jbXwX+ach5twNv8bbnABu97Q+nnnPw84B1wDne9r8C3/e21wNv9rZvYshiJUNe5x+Br3jbfqAKaCQ5bfH8g36u1UCRt30hcI+3fS7wgLf9DeDvvO1aYDNQ4T0+C/hDtv8t6Su9X+oakky42Pta4z2uBBYDTwCfNbO/9fbP9vYfAOLAPQe9TmrK8BdIrjEwmt+75BoLr5jZNG/fW4DfevubzeyxQ5T1ziHbs4A7zWw6yVWito3xnAuBY80GZg6uNrMqYDrQevDJZlYD1DrnHvd23Qb81sxqgSrnXKrL6XZGX4TleeAXZhbw6rvWkusuPOGc2wbgnGv3zq0BbjOzxSTDNTDK610MvHPI+EUpXqABLcCMMeoteUJBIJlgwDedc7cM25n88LoQONM5FzSzlSQ/hADCbuS4QL/3Pc7Y/3b7h2zbQd/Ho2/I9g+B7zrn7vfK+tUxnuMjWYfQ0J1mFiL5QTxe4yqnS65odTZwKfD/zOzbQCejz9f/deAx59zfet1aK8d433c7514d5VgpEBplv+QRjRFIJjwEfMTMKgHMbKaZTSX5IdnhhcBS4Iw0vf9TwLu9sYJpJLtFxqMG2ONtXztkfw/J7piUv5CcKRJI9v97mxuBRQe/qHOuC+hI9e0DHwAed851AD1mlvo5jNo3b2ZzgRbn3M+An5Nc73YVcI6ZzffOqR+lDh8ao54PAZ8xr0ljZicPObaEZHeV5DEFgaSdc+4vJLs5VpnZy8DdJD9I/wwUmdk6kn+5PpOmItxDcqGP9cAtwLNA1zie91WSXTZPAkMHfP8A/G1qsBj4LLDCG+R9hcGVsp4guXTiaH/pXwt826v7cpLjBADXAbea2SqSf6mPVs5zgbVmtgZ4N3Czc64VuB6418xeYrCL61vAN83sv0mOJ4zm6yS7jNZ5g9NfH3LsPOCPYzxP8oSmoZaCYGaVzrleS65t+xxwlnOuOQPvezPJwdZHxnl+pXOu19u+EZjunLshnWU8RFlKgMdJDoSPdSmt5AGNEUiheMAbjC0Gvp6JEPB8g0MvNH+wS83syyT/b+5g7O6cTJgD3KgQyH9qEYiIFDiNEYiIFDgFgYhIgVMQiIgUOAWBiEiBUxCIiBQ4BYGISIH7/7ByOTqxPbXEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f03b499e550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_loss_matrix[:, 0], lr_loss_matrix[:, 1])\n",
    "plt.xlabel('learning rate(log scale)')\n",
    "plt.ylabel('loss')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model1(english_vocab_size, french_vocab_size, max_french_sequence_length, \n",
    "                 embedding_dim=200, hidden_dim=100, rnn_module = nn.LSTM)\n",
    "model_adam = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_adam = optim.Adam(model_adam.parameters(), lr=0.03, amsgrad=True)\n",
    "optimizer_sgd = optim.SGD(model.parameters(), lr=0.5, momentum=1, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_sgd = optim.SGD(model.parameters(), lr=0.5, momentum=1, nesterov=True)\n",
    "\n",
    "#exp_lr_scheduler_anneal = lr_scheduler.CosineAnnealingLR(optimizer=optimizer_sgd, \n",
    "#                                                         T_max=400, \n",
    "#                                                         eta_min=7.5e-3, \n",
    "#                                                         last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_decay(epoch):\n",
    "    lr_matrix = np.ones(1200)\n",
    "    lr_matrix[0:300] = 0.03\n",
    "    lr_matrix[300:600] = 0.01\n",
    "    lr_matrix[600:900] = 7.5e-3\n",
    "    lr_matrix[900:] = 3e-3\n",
    "    \n",
    "    return lr_matrix[epoch]\n",
    "\n",
    "def lr_decay_sgd(epoch):\n",
    "    lr_matrix = np.ones(1200)\n",
    "    lr_matrix[0:300] = 0.5\n",
    "    lr_matrix[300:600] = 0.25\n",
    "    lr_matrix[600:900] = 0.1\n",
    "    lr_matrix[900:] = 0.05\n",
    "    \n",
    "    return lr_matrix[epoch]\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.LambdaLR(optimizer=optimizer_adam, lr_lambda=lr_decay, last_epoch=-1)\n",
    "#exp_lr_scheduler_sgd = lr_scheduler.LambdaLR(optimizer=optimizer_sgd, lr_lambda=lr_decay_sgd, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Validation Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Input and Output Tensor Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 27\n"
     ]
    }
   ],
   "source": [
    "a = list(dataloaders['train'])\n",
    "b = list(dataloaders['test'])\n",
    "print(len(a), len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, label = a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15])\n",
      "torch.Size([1, 21])\n"
     ]
    }
   ],
   "source": [
    "print(input[0].view(1,-1).shape)\n",
    "print(label[0].view(1,-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_adam(input.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 345, 21])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mac_train_model(model, criterion, optimizer, scheduler, num_epochs=18):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    loss_matrix = dict()\n",
    "    loss_matrix['train'] = np.ones(num_epochs)\n",
    "    loss_matrix['test'] = np.ones(num_epochs)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 24)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode; won't alter to different dropout and BatchNorm weights\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # turn on the tracking history in train and turn off the tracking history in others\n",
    "                with torch.set_grad_enabled(phase == 'train'): #set gradient calculation enabled\n",
    "                    outputs = model(inputs)\n",
    "                    max_tensor, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.shape[0]\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_loss = running_loss / dataset_size[phase]\n",
    "            epoch_acc = running_corrects.double()*100 / (dataset_size[phase]*max_french_sequence_length)\n",
    "            \n",
    "            # loss matrix to monitor\n",
    "            loss_matrix[phase][epoch] = epoch_loss\n",
    "            \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    # Monitor loss matrix\n",
    "    plt.plot(range(num_epochs), loss_matrix['train'], label='train')\n",
    "    plt.plot(range(num_epochs), loss_matrix['test'], label='test')\n",
    "    plt.title('loss optimization monitor')\n",
    "    plt.xlabel('# of epoch')\n",
    "    plt.ylabel('optimized loss')\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with Adam with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1199\n",
      "------------------------\n",
      "train Loss: 3.3676 Acc: 42.0450\n",
      "test Loss: 2.3153 Acc: 54.9508\n",
      "\n",
      "Epoch 1/1199\n",
      "------------------------\n",
      "train Loss: 2.0877 Acc: 57.4596\n",
      "test Loss: 1.8834 Acc: 59.5362\n",
      "\n",
      "Epoch 2/1199\n",
      "------------------------\n",
      "train Loss: 1.7560 Acc: 61.5070\n",
      "test Loss: 1.6068 Acc: 63.2006\n",
      "\n",
      "Epoch 3/1199\n",
      "------------------------\n",
      "train Loss: 1.5358 Acc: 64.6209\n",
      "test Loss: 1.4386 Acc: 65.9399\n",
      "\n",
      "Epoch 4/1199\n",
      "------------------------\n",
      "train Loss: 1.3444 Acc: 67.8425\n",
      "test Loss: 1.2839 Acc: 69.4393\n",
      "\n",
      "Epoch 5/1199\n",
      "------------------------\n",
      "train Loss: 1.2427 Acc: 70.5196\n",
      "test Loss: 1.1787 Acc: 71.4059\n",
      "\n",
      "Epoch 6/1199\n",
      "------------------------\n",
      "train Loss: 1.1292 Acc: 72.9053\n",
      "test Loss: 1.0532 Acc: 74.3665\n",
      "\n",
      "Epoch 7/1199\n",
      "------------------------\n",
      "train Loss: 1.0155 Acc: 75.4686\n",
      "test Loss: 0.9921 Acc: 75.8780\n",
      "\n",
      "Epoch 8/1199\n",
      "------------------------\n",
      "train Loss: 0.9533 Acc: 77.0854\n",
      "test Loss: 0.9080 Acc: 77.7429\n",
      "\n",
      "Epoch 9/1199\n",
      "------------------------\n",
      "train Loss: 0.8814 Acc: 78.6971\n",
      "test Loss: 0.8451 Acc: 79.4913\n",
      "\n",
      "Epoch 10/1199\n",
      "------------------------\n",
      "train Loss: 0.8347 Acc: 80.0064\n",
      "test Loss: 0.8162 Acc: 80.7553\n",
      "\n",
      "Epoch 11/1199\n",
      "------------------------\n",
      "train Loss: 0.7641 Acc: 81.5484\n",
      "test Loss: 0.7321 Acc: 81.7650\n",
      "\n",
      "Epoch 12/1199\n",
      "------------------------\n",
      "train Loss: 0.6986 Acc: 83.0866\n",
      "test Loss: 0.6979 Acc: 83.2385\n",
      "\n",
      "Epoch 13/1199\n",
      "------------------------\n",
      "train Loss: 0.6635 Acc: 83.9920\n",
      "test Loss: 0.6496 Acc: 83.9107\n",
      "\n",
      "Epoch 14/1199\n",
      "------------------------\n",
      "train Loss: 0.6262 Acc: 84.9368\n",
      "test Loss: 0.6098 Acc: 85.2564\n",
      "\n",
      "Epoch 15/1199\n",
      "------------------------\n",
      "train Loss: 0.5747 Acc: 86.1237\n",
      "test Loss: 0.5714 Acc: 86.1892\n",
      "\n",
      "Epoch 16/1199\n",
      "------------------------\n",
      "train Loss: 0.5480 Acc: 86.8530\n",
      "test Loss: 0.5363 Acc: 87.2068\n",
      "\n",
      "Epoch 17/1199\n",
      "------------------------\n",
      "train Loss: 0.5157 Acc: 87.5979\n",
      "test Loss: 0.5282 Acc: 87.3528\n",
      "\n",
      "Epoch 18/1199\n",
      "------------------------\n",
      "train Loss: 0.4958 Acc: 88.2078\n",
      "test Loss: 0.4989 Acc: 88.3123\n",
      "\n",
      "Epoch 19/1199\n",
      "------------------------\n",
      "train Loss: 0.4710 Acc: 88.8502\n",
      "test Loss: 0.4741 Acc: 89.0374\n",
      "\n",
      "Epoch 20/1199\n",
      "------------------------\n",
      "train Loss: 0.4453 Acc: 89.4374\n",
      "test Loss: 0.4471 Acc: 89.5617\n",
      "\n",
      "Epoch 21/1199\n",
      "------------------------\n",
      "train Loss: 0.4261 Acc: 89.9077\n",
      "test Loss: 0.4493 Acc: 89.6254\n",
      "\n",
      "Epoch 22/1199\n",
      "------------------------\n",
      "train Loss: 0.4173 Acc: 90.2727\n",
      "test Loss: 0.4118 Acc: 90.3558\n",
      "\n",
      "Epoch 23/1199\n",
      "------------------------\n",
      "train Loss: 0.3883 Acc: 90.9542\n",
      "test Loss: 0.3888 Acc: 90.8817\n",
      "\n",
      "Epoch 24/1199\n",
      "------------------------\n",
      "train Loss: 0.3837 Acc: 91.1706\n",
      "test Loss: 0.3840 Acc: 90.9601\n",
      "\n",
      "Epoch 25/1199\n",
      "------------------------\n",
      "train Loss: 0.3681 Acc: 91.5324\n",
      "test Loss: 0.3658 Acc: 91.6005\n",
      "\n",
      "Epoch 26/1199\n",
      "------------------------\n",
      "train Loss: 0.3466 Acc: 92.0241\n",
      "test Loss: 0.3660 Acc: 91.5415\n",
      "\n",
      "Epoch 27/1199\n",
      "------------------------\n",
      "train Loss: 0.3376 Acc: 92.1908\n",
      "test Loss: 0.3656 Acc: 91.7330\n",
      "\n",
      "Epoch 28/1199\n",
      "------------------------\n",
      "train Loss: 0.3278 Acc: 92.4645\n",
      "test Loss: 0.3343 Acc: 92.2521\n",
      "\n",
      "Epoch 29/1199\n",
      "------------------------\n",
      "train Loss: 0.3248 Acc: 92.5792\n",
      "test Loss: 0.3229 Acc: 92.5698\n",
      "\n",
      "Epoch 30/1199\n",
      "------------------------\n",
      "train Loss: 0.3051 Acc: 93.0115\n",
      "test Loss: 0.3064 Acc: 92.8590\n",
      "\n",
      "Epoch 31/1199\n",
      "------------------------\n",
      "train Loss: 0.2929 Acc: 93.2662\n",
      "test Loss: 0.3097 Acc: 92.9635\n",
      "\n",
      "Epoch 32/1199\n",
      "------------------------\n",
      "train Loss: 0.2870 Acc: 93.4296\n",
      "test Loss: 0.3196 Acc: 92.6490\n",
      "\n",
      "Epoch 33/1199\n",
      "------------------------\n",
      "train Loss: 0.2782 Acc: 93.5948\n",
      "test Loss: 0.2929 Acc: 93.2912\n",
      "\n",
      "Epoch 34/1199\n",
      "------------------------\n",
      "train Loss: 0.2813 Acc: 93.5736\n",
      "test Loss: 0.2957 Acc: 93.2017\n",
      "\n",
      "Epoch 35/1199\n",
      "------------------------\n",
      "train Loss: 0.2672 Acc: 93.8370\n",
      "test Loss: 0.2863 Acc: 93.5148\n",
      "\n",
      "Epoch 36/1199\n",
      "------------------------\n",
      "train Loss: 0.2600 Acc: 94.0819\n",
      "test Loss: 0.2847 Acc: 93.4595\n",
      "\n",
      "Epoch 37/1199\n",
      "------------------------\n",
      "train Loss: 0.2479 Acc: 94.2773\n",
      "test Loss: 0.2863 Acc: 93.5197\n",
      "\n",
      "Epoch 38/1199\n",
      "------------------------\n",
      "train Loss: 0.2409 Acc: 94.4828\n",
      "test Loss: 0.2556 Acc: 94.0884\n",
      "\n",
      "Epoch 39/1199\n",
      "------------------------\n",
      "train Loss: 0.2409 Acc: 94.5284\n",
      "test Loss: 0.2546 Acc: 94.2143\n",
      "\n",
      "Epoch 40/1199\n",
      "------------------------\n",
      "train Loss: 0.2301 Acc: 94.7394\n",
      "test Loss: 0.2522 Acc: 94.2112\n",
      "\n",
      "Epoch 41/1199\n",
      "------------------------\n",
      "train Loss: 0.2290 Acc: 94.7559\n",
      "test Loss: 0.2532 Acc: 94.0575\n",
      "\n",
      "Epoch 42/1199\n",
      "------------------------\n",
      "train Loss: 0.2192 Acc: 94.9586\n",
      "test Loss: 0.2445 Acc: 94.4542\n",
      "\n",
      "Epoch 43/1199\n",
      "------------------------\n",
      "train Loss: 0.2240 Acc: 94.8670\n",
      "test Loss: 0.2464 Acc: 94.4842\n",
      "\n",
      "Epoch 44/1199\n",
      "------------------------\n",
      "train Loss: 0.2200 Acc: 95.0151\n",
      "test Loss: 0.2355 Acc: 94.6619\n",
      "\n",
      "Epoch 45/1199\n",
      "------------------------\n",
      "train Loss: 0.2064 Acc: 95.2767\n",
      "test Loss: 0.2272 Acc: 94.6991\n",
      "\n",
      "Epoch 46/1199\n",
      "------------------------\n",
      "train Loss: 0.2066 Acc: 95.2673\n",
      "test Loss: 0.2250 Acc: 94.9410\n",
      "\n",
      "Epoch 47/1199\n",
      "------------------------\n",
      "train Loss: 0.1978 Acc: 95.4336\n",
      "test Loss: 0.2295 Acc: 94.7884\n",
      "\n",
      "Epoch 48/1199\n",
      "------------------------\n",
      "train Loss: 0.1928 Acc: 95.5774\n",
      "test Loss: 0.2169 Acc: 95.0505\n",
      "\n",
      "Epoch 49/1199\n",
      "------------------------\n",
      "train Loss: 0.1985 Acc: 95.4818\n",
      "test Loss: 0.2226 Acc: 94.8113\n",
      "\n",
      "Epoch 50/1199\n",
      "------------------------\n",
      "train Loss: 0.1932 Acc: 95.6326\n",
      "test Loss: 0.2160 Acc: 94.9352\n",
      "\n",
      "Epoch 51/1199\n",
      "------------------------\n",
      "train Loss: 0.1927 Acc: 95.6301\n",
      "test Loss: 0.2189 Acc: 94.9049\n",
      "\n",
      "Epoch 52/1199\n",
      "------------------------\n",
      "train Loss: 0.1892 Acc: 95.6802\n",
      "test Loss: 0.2009 Acc: 95.3326\n",
      "\n",
      "Epoch 53/1199\n",
      "------------------------\n",
      "train Loss: 0.1801 Acc: 95.8500\n",
      "test Loss: 0.2142 Acc: 95.0675\n",
      "\n",
      "Epoch 54/1199\n",
      "------------------------\n",
      "train Loss: 0.1812 Acc: 95.8559\n",
      "test Loss: 0.2067 Acc: 95.2352\n",
      "\n",
      "Epoch 55/1199\n",
      "------------------------\n",
      "train Loss: 0.1783 Acc: 95.8946\n",
      "test Loss: 0.2009 Acc: 95.3944\n",
      "\n",
      "Epoch 56/1199\n",
      "------------------------\n",
      "train Loss: 0.1943 Acc: 95.5836\n",
      "test Loss: 0.1983 Acc: 95.4813\n",
      "\n",
      "Epoch 57/1199\n",
      "------------------------\n",
      "train Loss: 0.1741 Acc: 96.0079\n",
      "test Loss: 0.1975 Acc: 95.4585\n",
      "\n",
      "Epoch 58/1199\n",
      "------------------------\n",
      "train Loss: 0.1689 Acc: 96.0931\n",
      "test Loss: 0.1961 Acc: 95.4191\n",
      "\n",
      "Epoch 59/1199\n",
      "------------------------\n",
      "train Loss: 0.1641 Acc: 96.2022\n",
      "test Loss: 0.2044 Acc: 95.3343\n",
      "\n",
      "Epoch 60/1199\n",
      "------------------------\n",
      "train Loss: 0.1637 Acc: 96.2243\n",
      "test Loss: 0.2006 Acc: 95.4578\n",
      "\n",
      "Epoch 61/1199\n",
      "------------------------\n",
      "train Loss: 0.1578 Acc: 96.3675\n",
      "test Loss: 0.1887 Acc: 95.6426\n",
      "\n",
      "Epoch 62/1199\n",
      "------------------------\n",
      "train Loss: 0.1622 Acc: 96.2810\n",
      "test Loss: 0.1891 Acc: 95.6586\n",
      "\n",
      "Epoch 63/1199\n",
      "------------------------\n",
      "train Loss: 0.1531 Acc: 96.4768\n",
      "test Loss: 0.1913 Acc: 95.6695\n",
      "\n",
      "Epoch 64/1199\n",
      "------------------------\n",
      "train Loss: 0.1525 Acc: 96.5192\n",
      "test Loss: 0.1868 Acc: 95.7061\n",
      "\n",
      "Epoch 65/1199\n",
      "------------------------\n",
      "train Loss: 0.1503 Acc: 96.4996\n",
      "test Loss: 0.1887 Acc: 95.7046\n",
      "\n",
      "Epoch 66/1199\n",
      "------------------------\n",
      "train Loss: 0.1492 Acc: 96.5426\n",
      "test Loss: 0.1849 Acc: 95.7595\n",
      "\n",
      "Epoch 67/1199\n",
      "------------------------\n",
      "train Loss: 0.1512 Acc: 96.5506\n",
      "test Loss: 0.1766 Acc: 95.9139\n",
      "\n",
      "Epoch 68/1199\n",
      "------------------------\n",
      "train Loss: 0.1416 Acc: 96.7427\n",
      "test Loss: 0.1750 Acc: 95.9989\n",
      "\n",
      "Epoch 69/1199\n",
      "------------------------\n",
      "train Loss: 0.1446 Acc: 96.6925\n",
      "test Loss: 0.1661 Acc: 96.1503\n",
      "\n",
      "Epoch 70/1199\n",
      "------------------------\n",
      "train Loss: 0.1361 Acc: 96.8402\n",
      "test Loss: 0.1721 Acc: 96.0208\n",
      "\n",
      "Epoch 71/1199\n",
      "------------------------\n",
      "train Loss: 0.1460 Acc: 96.6198\n",
      "test Loss: 0.1726 Acc: 95.9555\n",
      "\n",
      "Epoch 72/1199\n",
      "------------------------\n",
      "train Loss: 0.1418 Acc: 96.7563\n",
      "test Loss: 0.1812 Acc: 95.8522\n",
      "\n",
      "Epoch 73/1199\n",
      "------------------------\n",
      "train Loss: 0.1333 Acc: 96.8913\n",
      "test Loss: 0.1750 Acc: 95.9904\n",
      "\n",
      "Epoch 74/1199\n",
      "------------------------\n",
      "train Loss: 0.1403 Acc: 96.7684\n",
      "test Loss: 0.1619 Acc: 96.2464\n",
      "\n",
      "Epoch 75/1199\n",
      "------------------------\n",
      "train Loss: 0.1360 Acc: 96.8978\n",
      "test Loss: 0.1676 Acc: 96.1935\n",
      "\n",
      "Epoch 76/1199\n",
      "------------------------\n",
      "train Loss: 0.1281 Acc: 97.0412\n",
      "test Loss: 0.1628 Acc: 96.2939\n",
      "\n",
      "Epoch 77/1199\n",
      "------------------------\n",
      "train Loss: 0.1244 Acc: 97.0942\n",
      "test Loss: 0.1633 Acc: 96.3286\n",
      "\n",
      "Epoch 78/1199\n",
      "------------------------\n",
      "train Loss: 0.1245 Acc: 97.0979\n",
      "test Loss: 0.1615 Acc: 96.2635\n",
      "\n",
      "Epoch 79/1199\n",
      "------------------------\n",
      "train Loss: 0.1301 Acc: 96.9802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1749 Acc: 96.0597\n",
      "\n",
      "Epoch 80/1199\n",
      "------------------------\n",
      "train Loss: 0.1249 Acc: 97.1018\n",
      "test Loss: 0.1512 Acc: 96.4458\n",
      "\n",
      "Epoch 81/1199\n",
      "------------------------\n",
      "train Loss: 0.1201 Acc: 97.2083\n",
      "test Loss: 0.1579 Acc: 96.4123\n",
      "\n",
      "Epoch 82/1199\n",
      "------------------------\n",
      "train Loss: 0.1211 Acc: 97.1996\n",
      "test Loss: 0.1560 Acc: 96.4811\n",
      "\n",
      "Epoch 83/1199\n",
      "------------------------\n",
      "train Loss: 0.1194 Acc: 97.2482\n",
      "test Loss: 0.1458 Acc: 96.6153\n",
      "\n",
      "Epoch 84/1199\n",
      "------------------------\n",
      "train Loss: 0.1174 Acc: 97.2769\n",
      "test Loss: 0.1571 Acc: 96.4581\n",
      "\n",
      "Epoch 85/1199\n",
      "------------------------\n",
      "train Loss: 0.1173 Acc: 97.2746\n",
      "test Loss: 0.1536 Acc: 96.4287\n",
      "\n",
      "Epoch 86/1199\n",
      "------------------------\n",
      "train Loss: 0.1161 Acc: 97.2832\n",
      "test Loss: 0.1590 Acc: 96.3735\n",
      "\n",
      "Epoch 87/1199\n",
      "------------------------\n",
      "train Loss: 0.1159 Acc: 97.2984\n",
      "test Loss: 0.1482 Acc: 96.6068\n",
      "\n",
      "Epoch 88/1199\n",
      "------------------------\n",
      "train Loss: 0.1139 Acc: 97.3873\n",
      "test Loss: 0.1517 Acc: 96.5186\n",
      "\n",
      "Epoch 89/1199\n",
      "------------------------\n",
      "train Loss: 0.1168 Acc: 97.3180\n",
      "test Loss: 0.1440 Acc: 96.7165\n",
      "\n",
      "Epoch 90/1199\n",
      "------------------------\n",
      "train Loss: 0.1099 Acc: 97.4161\n",
      "test Loss: 0.1452 Acc: 96.6638\n",
      "\n",
      "Epoch 91/1199\n",
      "------------------------\n",
      "train Loss: 0.1101 Acc: 97.4719\n",
      "test Loss: 0.1475 Acc: 96.6438\n",
      "\n",
      "Epoch 92/1199\n",
      "------------------------\n",
      "train Loss: 0.1085 Acc: 97.4995\n",
      "test Loss: 0.1479 Acc: 96.6436\n",
      "\n",
      "Epoch 93/1199\n",
      "------------------------\n",
      "train Loss: 0.1105 Acc: 97.4380\n",
      "test Loss: 0.1582 Acc: 96.4925\n",
      "\n",
      "Epoch 94/1199\n",
      "------------------------\n",
      "train Loss: 0.1080 Acc: 97.4828\n",
      "test Loss: 0.1394 Acc: 96.8135\n",
      "\n",
      "Epoch 95/1199\n",
      "------------------------\n",
      "train Loss: 0.1056 Acc: 97.5070\n",
      "test Loss: 0.1441 Acc: 96.7122\n",
      "\n",
      "Epoch 96/1199\n",
      "------------------------\n",
      "train Loss: 0.1126 Acc: 97.3490\n",
      "test Loss: 0.1481 Acc: 96.5591\n",
      "\n",
      "Epoch 97/1199\n",
      "------------------------\n",
      "train Loss: 0.1070 Acc: 97.5204\n",
      "test Loss: 0.1435 Acc: 96.6429\n",
      "\n",
      "Epoch 98/1199\n",
      "------------------------\n",
      "train Loss: 0.1025 Acc: 97.5972\n",
      "test Loss: 0.1402 Acc: 96.7921\n",
      "\n",
      "Epoch 99/1199\n",
      "------------------------\n",
      "train Loss: 0.1021 Acc: 97.6335\n",
      "test Loss: 0.1422 Acc: 96.7115\n",
      "\n",
      "Epoch 100/1199\n",
      "------------------------\n",
      "train Loss: 0.0980 Acc: 97.7357\n",
      "test Loss: 0.1418 Acc: 96.8130\n",
      "\n",
      "Epoch 101/1199\n",
      "------------------------\n",
      "train Loss: 0.0995 Acc: 97.7343\n",
      "test Loss: 0.1355 Acc: 96.8850\n",
      "\n",
      "Epoch 102/1199\n",
      "------------------------\n",
      "train Loss: 0.0991 Acc: 97.7339\n",
      "test Loss: 0.1389 Acc: 96.8177\n",
      "\n",
      "Epoch 103/1199\n",
      "------------------------\n",
      "train Loss: 0.0963 Acc: 97.7385\n",
      "test Loss: 0.1414 Acc: 96.7289\n",
      "\n",
      "Epoch 104/1199\n",
      "------------------------\n",
      "train Loss: 0.0972 Acc: 97.7366\n",
      "test Loss: 0.1411 Acc: 96.7016\n",
      "\n",
      "Epoch 105/1199\n",
      "------------------------\n",
      "train Loss: 0.0937 Acc: 97.8040\n",
      "test Loss: 0.1337 Acc: 97.0103\n",
      "\n",
      "Epoch 106/1199\n",
      "------------------------\n",
      "train Loss: 0.0935 Acc: 97.8130\n",
      "test Loss: 0.1387 Acc: 96.8650\n",
      "\n",
      "Epoch 107/1199\n",
      "------------------------\n",
      "train Loss: 0.0926 Acc: 97.8489\n",
      "test Loss: 0.1296 Acc: 97.0194\n",
      "\n",
      "Epoch 108/1199\n",
      "------------------------\n",
      "train Loss: 0.0936 Acc: 97.8288\n",
      "test Loss: 0.1351 Acc: 96.9369\n",
      "\n",
      "Epoch 109/1199\n",
      "------------------------\n",
      "train Loss: 0.0955 Acc: 97.7959\n",
      "test Loss: 0.1278 Acc: 97.1042\n",
      "\n",
      "Epoch 110/1199\n",
      "------------------------\n",
      "train Loss: 0.0938 Acc: 97.8507\n",
      "test Loss: 0.1306 Acc: 96.9913\n",
      "\n",
      "Epoch 111/1199\n",
      "------------------------\n",
      "train Loss: 0.0916 Acc: 97.8685\n",
      "test Loss: 0.1363 Acc: 96.8937\n",
      "\n",
      "Epoch 112/1199\n",
      "------------------------\n",
      "train Loss: 0.0865 Acc: 97.9597\n",
      "test Loss: 0.1297 Acc: 97.1130\n",
      "\n",
      "Epoch 113/1199\n",
      "------------------------\n",
      "train Loss: 0.0893 Acc: 97.9114\n",
      "test Loss: 0.1327 Acc: 96.9951\n",
      "\n",
      "Epoch 114/1199\n",
      "------------------------\n",
      "train Loss: 0.0888 Acc: 97.9203\n",
      "test Loss: 0.1279 Acc: 97.0830\n",
      "\n",
      "Epoch 115/1199\n",
      "------------------------\n",
      "train Loss: 0.0895 Acc: 97.9501\n",
      "test Loss: 0.1306 Acc: 97.0600\n",
      "\n",
      "Epoch 116/1199\n",
      "------------------------\n",
      "train Loss: 0.0947 Acc: 97.7855\n",
      "test Loss: 0.1359 Acc: 96.9707\n",
      "\n",
      "Epoch 117/1199\n",
      "------------------------\n",
      "train Loss: 0.0934 Acc: 97.8060\n",
      "test Loss: 0.1284 Acc: 97.0343\n",
      "\n",
      "Epoch 118/1199\n",
      "------------------------\n",
      "train Loss: 0.0855 Acc: 97.9797\n",
      "test Loss: 0.1407 Acc: 96.8598\n",
      "\n",
      "Epoch 119/1199\n",
      "------------------------\n",
      "train Loss: 0.0882 Acc: 97.9538\n",
      "test Loss: 0.1272 Acc: 97.1270\n",
      "\n",
      "Epoch 120/1199\n",
      "------------------------\n",
      "train Loss: 0.0853 Acc: 97.9979\n",
      "test Loss: 0.1352 Acc: 96.9123\n",
      "\n",
      "Epoch 121/1199\n",
      "------------------------\n",
      "train Loss: 0.0911 Acc: 97.8468\n",
      "test Loss: 0.1361 Acc: 96.9194\n",
      "\n",
      "Epoch 122/1199\n",
      "------------------------\n",
      "train Loss: 0.0893 Acc: 97.9034\n",
      "test Loss: 0.1261 Acc: 97.1862\n",
      "\n",
      "Epoch 123/1199\n",
      "------------------------\n",
      "train Loss: 0.0851 Acc: 98.0342\n",
      "test Loss: 0.1280 Acc: 97.1382\n",
      "\n",
      "Epoch 124/1199\n",
      "------------------------\n",
      "train Loss: 0.0851 Acc: 98.0098\n",
      "test Loss: 0.1321 Acc: 97.0163\n",
      "\n",
      "Epoch 125/1199\n",
      "------------------------\n",
      "train Loss: 0.0810 Acc: 98.1006\n",
      "test Loss: 0.1300 Acc: 97.0643\n",
      "\n",
      "Epoch 126/1199\n",
      "------------------------\n",
      "train Loss: 0.0791 Acc: 98.1541\n",
      "test Loss: 0.1286 Acc: 97.0381\n",
      "\n",
      "Epoch 127/1199\n",
      "------------------------\n",
      "train Loss: 0.0822 Acc: 98.0723\n",
      "test Loss: 0.1228 Acc: 97.2579\n",
      "\n",
      "Epoch 128/1199\n",
      "------------------------\n",
      "train Loss: 0.0789 Acc: 98.1515\n",
      "test Loss: 0.1247 Acc: 97.2020\n",
      "\n",
      "Epoch 129/1199\n",
      "------------------------\n",
      "train Loss: 0.0766 Acc: 98.2192\n",
      "test Loss: 0.1274 Acc: 97.1522\n",
      "\n",
      "Epoch 130/1199\n",
      "------------------------\n",
      "train Loss: 0.0780 Acc: 98.1743\n",
      "test Loss: 0.1232 Acc: 97.2503\n",
      "\n",
      "Epoch 131/1199\n",
      "------------------------\n",
      "train Loss: 0.0815 Acc: 98.1187\n",
      "test Loss: 0.1296 Acc: 97.1144\n",
      "\n",
      "Epoch 132/1199\n",
      "------------------------\n",
      "train Loss: 0.0757 Acc: 98.2278\n",
      "test Loss: 0.1256 Acc: 97.2260\n",
      "\n",
      "Epoch 133/1199\n",
      "------------------------\n",
      "train Loss: 0.0791 Acc: 98.1900\n",
      "test Loss: 0.1186 Acc: 97.3439\n",
      "\n",
      "Epoch 134/1199\n",
      "------------------------\n",
      "train Loss: 0.0741 Acc: 98.2884\n",
      "test Loss: 0.1252 Acc: 97.2881\n",
      "\n",
      "Epoch 135/1199\n",
      "------------------------\n",
      "train Loss: 0.0752 Acc: 98.2631\n",
      "test Loss: 0.1269 Acc: 97.2626\n",
      "\n",
      "Epoch 136/1199\n",
      "------------------------\n",
      "train Loss: 0.0762 Acc: 98.2389\n",
      "test Loss: 0.1199 Acc: 97.3358\n",
      "\n",
      "Epoch 137/1199\n",
      "------------------------\n",
      "train Loss: 0.0759 Acc: 98.2523\n",
      "test Loss: 0.1260 Acc: 97.1918\n",
      "\n",
      "Epoch 138/1199\n",
      "------------------------\n",
      "train Loss: 0.0737 Acc: 98.2744\n",
      "test Loss: 0.1259 Acc: 97.1439\n",
      "\n",
      "Epoch 139/1199\n",
      "------------------------\n",
      "train Loss: 0.0727 Acc: 98.3198\n",
      "test Loss: 0.1250 Acc: 97.2819\n",
      "\n",
      "Epoch 140/1199\n",
      "------------------------\n",
      "train Loss: 0.0694 Acc: 98.3914\n",
      "test Loss: 0.1288 Acc: 97.1590\n",
      "\n",
      "Epoch 141/1199\n",
      "------------------------\n",
      "train Loss: 0.0714 Acc: 98.3313\n",
      "test Loss: 0.1207 Acc: 97.3232\n",
      "\n",
      "Epoch 142/1199\n",
      "------------------------\n",
      "train Loss: 0.0748 Acc: 98.2567\n",
      "test Loss: 0.1194 Acc: 97.3560\n",
      "\n",
      "Epoch 143/1199\n",
      "------------------------\n",
      "train Loss: 0.0729 Acc: 98.2805\n",
      "test Loss: 0.1225 Acc: 97.2626\n",
      "\n",
      "Epoch 144/1199\n",
      "------------------------\n",
      "train Loss: 0.0712 Acc: 98.3506\n",
      "test Loss: 0.1322 Acc: 97.1363\n",
      "\n",
      "Epoch 145/1199\n",
      "------------------------\n",
      "train Loss: 0.0710 Acc: 98.3218\n",
      "test Loss: 0.1204 Acc: 97.3536\n",
      "\n",
      "Epoch 146/1199\n",
      "------------------------\n",
      "train Loss: 0.0716 Acc: 98.3362\n",
      "test Loss: 0.1217 Acc: 97.3624\n",
      "\n",
      "Epoch 147/1199\n",
      "------------------------\n",
      "train Loss: 0.0716 Acc: 98.3520\n",
      "test Loss: 0.1143 Acc: 97.5520\n",
      "\n",
      "Epoch 148/1199\n",
      "------------------------\n",
      "train Loss: 0.0664 Acc: 98.4702\n",
      "test Loss: 0.1122 Acc: 97.4800\n",
      "\n",
      "Epoch 149/1199\n",
      "------------------------\n",
      "train Loss: 0.0654 Acc: 98.4837\n",
      "test Loss: 0.1180 Acc: 97.4225\n",
      "\n",
      "Epoch 150/1199\n",
      "------------------------\n",
      "train Loss: 0.0696 Acc: 98.4126\n",
      "test Loss: 0.1196 Acc: 97.3254\n",
      "\n",
      "Epoch 151/1199\n",
      "------------------------\n",
      "train Loss: 0.0715 Acc: 98.3168\n",
      "test Loss: 0.1252 Acc: 97.2560\n",
      "\n",
      "Epoch 152/1199\n",
      "------------------------\n",
      "train Loss: 0.0698 Acc: 98.3742\n",
      "test Loss: 0.1200 Acc: 97.3679\n",
      "\n",
      "Epoch 153/1199\n",
      "------------------------\n",
      "train Loss: 0.0680 Acc: 98.4209\n",
      "test Loss: 0.1212 Acc: 97.3261\n",
      "\n",
      "Epoch 154/1199\n",
      "------------------------\n",
      "train Loss: 0.0636 Acc: 98.5122\n",
      "test Loss: 0.1125 Acc: 97.5190\n",
      "\n",
      "Epoch 155/1199\n",
      "------------------------\n",
      "train Loss: 0.0670 Acc: 98.4730\n",
      "test Loss: 0.1173 Acc: 97.4071\n",
      "\n",
      "Epoch 156/1199\n",
      "------------------------\n",
      "train Loss: 0.0638 Acc: 98.4902\n",
      "test Loss: 0.1190 Acc: 97.4496\n",
      "\n",
      "Epoch 157/1199\n",
      "------------------------\n",
      "train Loss: 0.0673 Acc: 98.4580\n",
      "test Loss: 0.1195 Acc: 97.3329\n",
      "\n",
      "Epoch 158/1199\n",
      "------------------------\n",
      "train Loss: 0.0628 Acc: 98.5453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1163 Acc: 97.4942\n",
      "\n",
      "Epoch 159/1199\n",
      "------------------------\n",
      "train Loss: 0.0622 Acc: 98.5598\n",
      "test Loss: 0.1176 Acc: 97.4197\n",
      "\n",
      "Epoch 160/1199\n",
      "------------------------\n",
      "train Loss: 0.0606 Acc: 98.5674\n",
      "test Loss: 0.1163 Acc: 97.4121\n",
      "\n",
      "Epoch 161/1199\n",
      "------------------------\n",
      "train Loss: 0.0593 Acc: 98.6196\n",
      "test Loss: 0.1144 Acc: 97.5325\n",
      "\n",
      "Epoch 162/1199\n",
      "------------------------\n",
      "train Loss: 0.0606 Acc: 98.5961\n",
      "test Loss: 0.1186 Acc: 97.4339\n",
      "\n",
      "Epoch 163/1199\n",
      "------------------------\n",
      "train Loss: 0.0621 Acc: 98.5498\n",
      "test Loss: 0.1117 Acc: 97.5370\n",
      "\n",
      "Epoch 164/1199\n",
      "------------------------\n",
      "train Loss: 0.0609 Acc: 98.5775\n",
      "test Loss: 0.1136 Acc: 97.5187\n",
      "\n",
      "Epoch 165/1199\n",
      "------------------------\n",
      "train Loss: 0.0610 Acc: 98.5996\n",
      "test Loss: 0.1138 Acc: 97.5546\n",
      "\n",
      "Epoch 166/1199\n",
      "------------------------\n",
      "train Loss: 0.0574 Acc: 98.6691\n",
      "test Loss: 0.1157 Acc: 97.5125\n",
      "\n",
      "Epoch 167/1199\n",
      "------------------------\n",
      "train Loss: 0.0599 Acc: 98.6215\n",
      "test Loss: 0.1132 Acc: 97.5759\n",
      "\n",
      "Epoch 168/1199\n",
      "------------------------\n",
      "train Loss: 0.0599 Acc: 98.6414\n",
      "test Loss: 0.1141 Acc: 97.5779\n",
      "\n",
      "Epoch 169/1199\n",
      "------------------------\n",
      "train Loss: 0.0580 Acc: 98.6592\n",
      "test Loss: 0.1108 Acc: 97.6225\n",
      "\n",
      "Epoch 170/1199\n",
      "------------------------\n",
      "train Loss: 0.0575 Acc: 98.7009\n",
      "test Loss: 0.1134 Acc: 97.5897\n",
      "\n",
      "Epoch 171/1199\n",
      "------------------------\n",
      "train Loss: 0.0540 Acc: 98.7665\n",
      "test Loss: 0.1141 Acc: 97.6215\n",
      "\n",
      "Epoch 172/1199\n",
      "------------------------\n",
      "train Loss: 0.0564 Acc: 98.7141\n",
      "test Loss: 0.1131 Acc: 97.5550\n",
      "\n",
      "Epoch 173/1199\n",
      "------------------------\n",
      "train Loss: 0.0583 Acc: 98.6532\n",
      "test Loss: 0.1213 Acc: 97.3899\n",
      "\n",
      "Epoch 174/1199\n",
      "------------------------\n",
      "train Loss: 0.0683 Acc: 98.4123\n",
      "test Loss: 0.1316 Acc: 97.1590\n",
      "\n",
      "Epoch 175/1199\n",
      "------------------------\n",
      "train Loss: 0.0708 Acc: 98.3401\n",
      "test Loss: 0.1225 Acc: 97.3766\n",
      "\n",
      "Epoch 176/1199\n",
      "------------------------\n",
      "train Loss: 0.0676 Acc: 98.3940\n",
      "test Loss: 0.1251 Acc: 97.3121\n",
      "\n",
      "Epoch 177/1199\n",
      "------------------------\n",
      "train Loss: 0.0630 Acc: 98.5205\n",
      "test Loss: 0.1137 Acc: 97.4945\n",
      "\n",
      "Epoch 178/1199\n",
      "------------------------\n",
      "train Loss: 0.0551 Acc: 98.7241\n",
      "test Loss: 0.1141 Acc: 97.5035\n",
      "\n",
      "Epoch 179/1199\n",
      "------------------------\n",
      "train Loss: 0.0554 Acc: 98.7335\n",
      "test Loss: 0.1130 Acc: 97.6018\n",
      "\n",
      "Epoch 180/1199\n",
      "------------------------\n",
      "train Loss: 0.0573 Acc: 98.6756\n",
      "test Loss: 0.1158 Acc: 97.5042\n",
      "\n",
      "Epoch 181/1199\n",
      "------------------------\n",
      "train Loss: 0.0564 Acc: 98.6984\n",
      "test Loss: 0.1188 Acc: 97.4938\n",
      "\n",
      "Epoch 182/1199\n",
      "------------------------\n",
      "train Loss: 0.0538 Acc: 98.7541\n",
      "test Loss: 0.1118 Acc: 97.6538\n",
      "\n",
      "Epoch 183/1199\n",
      "------------------------\n",
      "train Loss: 0.0558 Acc: 98.6929\n",
      "test Loss: 0.1094 Acc: 97.6937\n",
      "\n",
      "Epoch 184/1199\n",
      "------------------------\n",
      "train Loss: 0.0556 Acc: 98.7112\n",
      "test Loss: 0.1150 Acc: 97.5557\n",
      "\n",
      "Epoch 185/1199\n",
      "------------------------\n",
      "train Loss: 0.0549 Acc: 98.7314\n",
      "test Loss: 0.1132 Acc: 97.6025\n",
      "\n",
      "Epoch 186/1199\n",
      "------------------------\n",
      "train Loss: 0.0554 Acc: 98.7360\n",
      "test Loss: 0.1099 Acc: 97.7025\n",
      "\n",
      "Epoch 187/1199\n",
      "------------------------\n",
      "train Loss: 0.0527 Acc: 98.7755\n",
      "test Loss: 0.1103 Acc: 97.6622\n",
      "\n",
      "Epoch 188/1199\n",
      "------------------------\n",
      "train Loss: 0.0536 Acc: 98.7677\n",
      "test Loss: 0.1109 Acc: 97.6750\n",
      "\n",
      "Epoch 189/1199\n",
      "------------------------\n",
      "train Loss: 0.0521 Acc: 98.8005\n",
      "test Loss: 0.1121 Acc: 97.6463\n",
      "\n",
      "Epoch 190/1199\n",
      "------------------------\n",
      "train Loss: 0.0532 Acc: 98.7663\n",
      "test Loss: 0.1097 Acc: 97.6684\n",
      "\n",
      "Epoch 191/1199\n",
      "------------------------\n",
      "train Loss: 0.0530 Acc: 98.7624\n",
      "test Loss: 0.1091 Acc: 97.6987\n",
      "\n",
      "Epoch 192/1199\n",
      "------------------------\n",
      "train Loss: 0.0539 Acc: 98.7600\n",
      "test Loss: 0.1213 Acc: 97.4377\n",
      "\n",
      "Epoch 193/1199\n",
      "------------------------\n",
      "train Loss: 0.0549 Acc: 98.7141\n",
      "test Loss: 0.1117 Acc: 97.6303\n",
      "\n",
      "Epoch 194/1199\n",
      "------------------------\n",
      "train Loss: 0.0568 Acc: 98.6895\n",
      "test Loss: 0.1111 Acc: 97.6406\n",
      "\n",
      "Epoch 195/1199\n",
      "------------------------\n",
      "train Loss: 0.0531 Acc: 98.7649\n",
      "test Loss: 0.1098 Acc: 97.6990\n",
      "\n",
      "Epoch 196/1199\n",
      "------------------------\n",
      "train Loss: 0.0511 Acc: 98.8330\n",
      "test Loss: 0.1105 Acc: 97.6551\n",
      "\n",
      "Epoch 197/1199\n",
      "------------------------\n",
      "train Loss: 0.0488 Acc: 98.8954\n",
      "test Loss: 0.1090 Acc: 97.8201\n",
      "\n",
      "Epoch 198/1199\n",
      "------------------------\n",
      "train Loss: 0.0481 Acc: 98.9217\n",
      "test Loss: 0.1101 Acc: 97.7304\n",
      "\n",
      "Epoch 199/1199\n",
      "------------------------\n",
      "train Loss: 0.0467 Acc: 98.9485\n",
      "test Loss: 0.1079 Acc: 97.7764\n",
      "\n",
      "Epoch 200/1199\n",
      "------------------------\n",
      "train Loss: 0.0446 Acc: 98.9759\n",
      "test Loss: 0.1104 Acc: 97.7722\n",
      "\n",
      "Epoch 201/1199\n",
      "------------------------\n",
      "train Loss: 0.0475 Acc: 98.9234\n",
      "test Loss: 0.1050 Acc: 97.8177\n",
      "\n",
      "Epoch 202/1199\n",
      "------------------------\n",
      "train Loss: 0.0452 Acc: 98.9604\n",
      "test Loss: 0.1155 Acc: 97.5498\n",
      "\n",
      "Epoch 203/1199\n",
      "------------------------\n",
      "train Loss: 0.0518 Acc: 98.8239\n",
      "test Loss: 0.1178 Acc: 97.5636\n",
      "\n",
      "Epoch 204/1199\n",
      "------------------------\n",
      "train Loss: 0.0559 Acc: 98.6785\n",
      "test Loss: 0.1143 Acc: 97.6405\n",
      "\n",
      "Epoch 205/1199\n",
      "------------------------\n",
      "train Loss: 0.0521 Acc: 98.7874\n",
      "test Loss: 0.1145 Acc: 97.5522\n",
      "\n",
      "Epoch 206/1199\n",
      "------------------------\n",
      "train Loss: 0.0526 Acc: 98.7872\n",
      "test Loss: 0.1122 Acc: 97.6208\n",
      "\n",
      "Epoch 207/1199\n",
      "------------------------\n",
      "train Loss: 0.0480 Acc: 98.8634\n",
      "test Loss: 0.1108 Acc: 97.7336\n",
      "\n",
      "Epoch 208/1199\n",
      "------------------------\n",
      "train Loss: 0.0495 Acc: 98.8613\n",
      "test Loss: 0.1068 Acc: 97.8332\n",
      "\n",
      "Epoch 209/1199\n",
      "------------------------\n",
      "train Loss: 0.0490 Acc: 98.8822\n",
      "test Loss: 0.1137 Acc: 97.6821\n",
      "\n",
      "Epoch 210/1199\n",
      "------------------------\n",
      "train Loss: 0.0470 Acc: 98.9100\n",
      "test Loss: 0.1077 Acc: 97.7486\n",
      "\n",
      "Epoch 211/1199\n",
      "------------------------\n",
      "train Loss: 0.0463 Acc: 98.9517\n",
      "test Loss: 0.1075 Acc: 97.8260\n",
      "\n",
      "Epoch 212/1199\n",
      "------------------------\n",
      "train Loss: 0.0433 Acc: 99.0117\n",
      "test Loss: 0.1083 Acc: 97.7736\n",
      "\n",
      "Epoch 213/1199\n",
      "------------------------\n",
      "train Loss: 0.0446 Acc: 99.0061\n",
      "test Loss: 0.1032 Acc: 97.8904\n",
      "\n",
      "Epoch 214/1199\n",
      "------------------------\n",
      "train Loss: 0.0445 Acc: 98.9945\n",
      "test Loss: 0.1054 Acc: 97.8634\n",
      "\n",
      "Epoch 215/1199\n",
      "------------------------\n",
      "train Loss: 0.0449 Acc: 98.9796\n",
      "test Loss: 0.1087 Acc: 97.8242\n",
      "\n",
      "Epoch 216/1199\n",
      "------------------------\n",
      "train Loss: 0.0489 Acc: 98.9121\n",
      "test Loss: 0.1133 Acc: 97.7000\n",
      "\n",
      "Epoch 217/1199\n",
      "------------------------\n",
      "train Loss: 0.0520 Acc: 98.7902\n",
      "test Loss: 0.1111 Acc: 97.6686\n",
      "\n",
      "Epoch 218/1199\n",
      "------------------------\n",
      "train Loss: 0.0454 Acc: 98.9362\n",
      "test Loss: 0.1109 Acc: 97.6643\n",
      "\n",
      "Epoch 219/1199\n",
      "------------------------\n",
      "train Loss: 0.0491 Acc: 98.8707\n",
      "test Loss: 0.1133 Acc: 97.6755\n",
      "\n",
      "Epoch 220/1199\n",
      "------------------------\n",
      "train Loss: 0.0473 Acc: 98.8957\n",
      "test Loss: 0.1070 Acc: 97.7437\n",
      "\n",
      "Epoch 221/1199\n",
      "------------------------\n",
      "train Loss: 0.0490 Acc: 98.8996\n",
      "test Loss: 0.1177 Acc: 97.6589\n",
      "\n",
      "Epoch 222/1199\n",
      "------------------------\n",
      "train Loss: 0.0558 Acc: 98.7289\n",
      "test Loss: 0.1158 Acc: 97.6049\n",
      "\n",
      "Epoch 223/1199\n",
      "------------------------\n",
      "train Loss: 0.0550 Acc: 98.6843\n",
      "test Loss: 0.1135 Acc: 97.6762\n",
      "\n",
      "Epoch 224/1199\n",
      "------------------------\n",
      "train Loss: 0.0530 Acc: 98.7284\n",
      "test Loss: 0.1183 Acc: 97.5002\n",
      "\n",
      "Epoch 225/1199\n",
      "------------------------\n",
      "train Loss: 0.0527 Acc: 98.7586\n",
      "test Loss: 0.1191 Acc: 97.5223\n",
      "\n",
      "Epoch 226/1199\n",
      "------------------------\n",
      "train Loss: 0.0497 Acc: 98.8563\n",
      "test Loss: 0.1073 Acc: 97.7451\n",
      "\n",
      "Epoch 227/1199\n",
      "------------------------\n",
      "train Loss: 0.0473 Acc: 98.9210\n",
      "test Loss: 0.1037 Acc: 97.8574\n",
      "\n",
      "Epoch 228/1199\n",
      "------------------------\n",
      "train Loss: 0.0405 Acc: 99.0595\n",
      "test Loss: 0.1074 Acc: 97.7905\n",
      "\n",
      "Epoch 229/1199\n",
      "------------------------\n",
      "train Loss: 0.0440 Acc: 98.9968\n",
      "test Loss: 0.1059 Acc: 97.8544\n",
      "\n",
      "Epoch 230/1199\n",
      "------------------------\n",
      "train Loss: 0.0411 Acc: 99.0676\n",
      "test Loss: 0.1047 Acc: 97.9239\n",
      "\n",
      "Epoch 231/1199\n",
      "------------------------\n",
      "train Loss: 0.0410 Acc: 99.0780\n",
      "test Loss: 0.1009 Acc: 97.9128\n",
      "\n",
      "Epoch 232/1199\n",
      "------------------------\n",
      "train Loss: 0.0425 Acc: 99.0290\n",
      "test Loss: 0.1178 Acc: 97.6209\n",
      "\n",
      "Epoch 233/1199\n",
      "------------------------\n",
      "train Loss: 0.0500 Acc: 98.8309\n",
      "test Loss: 0.1060 Acc: 97.8303\n",
      "\n",
      "Epoch 234/1199\n",
      "------------------------\n",
      "train Loss: 0.0410 Acc: 99.0354\n",
      "test Loss: 0.1085 Acc: 97.7893\n",
      "\n",
      "Epoch 235/1199\n",
      "------------------------\n",
      "train Loss: 0.0440 Acc: 99.0030\n",
      "test Loss: 0.1061 Acc: 97.8572\n",
      "\n",
      "Epoch 236/1199\n",
      "------------------------\n",
      "train Loss: 0.0426 Acc: 99.0349\n",
      "test Loss: 0.1050 Acc: 97.8313\n",
      "\n",
      "Epoch 237/1199\n",
      "------------------------\n",
      "train Loss: 0.0405 Acc: 99.0726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1087 Acc: 97.8413\n",
      "\n",
      "Epoch 238/1199\n",
      "------------------------\n",
      "train Loss: 0.0418 Acc: 99.0427\n",
      "test Loss: 0.1096 Acc: 97.7144\n",
      "\n",
      "Epoch 239/1199\n",
      "------------------------\n",
      "train Loss: 0.0388 Acc: 99.1106\n",
      "test Loss: 0.1008 Acc: 97.9850\n",
      "\n",
      "Epoch 240/1199\n",
      "------------------------\n",
      "train Loss: 0.0393 Acc: 99.1211\n",
      "test Loss: 0.1038 Acc: 97.8994\n",
      "\n",
      "Epoch 241/1199\n",
      "------------------------\n",
      "train Loss: 0.0402 Acc: 99.0935\n",
      "test Loss: 0.1069 Acc: 97.8467\n",
      "\n",
      "Epoch 242/1199\n",
      "------------------------\n",
      "train Loss: 0.0401 Acc: 99.1067\n",
      "test Loss: 0.1001 Acc: 97.9728\n",
      "\n",
      "Epoch 243/1199\n",
      "------------------------\n",
      "train Loss: 0.0383 Acc: 99.1417\n",
      "test Loss: 0.1013 Acc: 98.0021\n",
      "\n",
      "Epoch 244/1199\n",
      "------------------------\n",
      "train Loss: 0.0375 Acc: 99.1691\n",
      "test Loss: 0.1037 Acc: 97.9584\n",
      "\n",
      "Epoch 245/1199\n",
      "------------------------\n",
      "train Loss: 0.0368 Acc: 99.1805\n",
      "test Loss: 0.1041 Acc: 97.9790\n",
      "\n",
      "Epoch 246/1199\n",
      "------------------------\n",
      "train Loss: 0.0361 Acc: 99.1699\n",
      "test Loss: 0.1080 Acc: 97.8420\n",
      "\n",
      "Epoch 247/1199\n",
      "------------------------\n",
      "train Loss: 0.0387 Acc: 99.1232\n",
      "test Loss: 0.1062 Acc: 97.9686\n",
      "\n",
      "Epoch 248/1199\n",
      "------------------------\n",
      "train Loss: 0.0381 Acc: 99.1485\n",
      "test Loss: 0.1085 Acc: 97.8216\n",
      "\n",
      "Epoch 249/1199\n",
      "------------------------\n",
      "train Loss: 0.0337 Acc: 99.2274\n",
      "test Loss: 0.1035 Acc: 97.9335\n",
      "\n",
      "Epoch 250/1199\n",
      "------------------------\n",
      "train Loss: 0.0365 Acc: 99.1713\n",
      "test Loss: 0.1038 Acc: 98.0088\n",
      "\n",
      "Epoch 251/1199\n",
      "------------------------\n",
      "train Loss: 0.0350 Acc: 99.2102\n",
      "test Loss: 0.1016 Acc: 97.9501\n",
      "\n",
      "Epoch 252/1199\n",
      "------------------------\n",
      "train Loss: 0.0347 Acc: 99.2212\n",
      "test Loss: 0.1044 Acc: 97.9607\n",
      "\n",
      "Epoch 253/1199\n",
      "------------------------\n",
      "train Loss: 0.0339 Acc: 99.2319\n",
      "test Loss: 0.1042 Acc: 97.9460\n",
      "\n",
      "Epoch 254/1199\n",
      "------------------------\n",
      "train Loss: 0.0356 Acc: 99.2039\n",
      "test Loss: 0.1033 Acc: 97.9888\n",
      "\n",
      "Epoch 255/1199\n",
      "------------------------\n",
      "train Loss: 0.0345 Acc: 99.2246\n",
      "test Loss: 0.1081 Acc: 97.9287\n",
      "\n",
      "Epoch 256/1199\n",
      "------------------------\n",
      "train Loss: 0.0369 Acc: 99.1834\n",
      "test Loss: 0.1030 Acc: 98.0139\n",
      "\n",
      "Epoch 257/1199\n",
      "------------------------\n",
      "train Loss: 0.0332 Acc: 99.2587\n",
      "test Loss: 0.1027 Acc: 97.9582\n",
      "\n",
      "Epoch 258/1199\n",
      "------------------------\n",
      "train Loss: 0.0357 Acc: 99.1982\n",
      "test Loss: 0.1116 Acc: 97.7764\n",
      "\n",
      "Epoch 259/1199\n",
      "------------------------\n",
      "train Loss: 0.0478 Acc: 98.9070\n",
      "test Loss: 0.1231 Acc: 97.5981\n",
      "\n",
      "Epoch 260/1199\n",
      "------------------------\n",
      "train Loss: 0.0620 Acc: 98.5613\n",
      "test Loss: 0.1324 Acc: 97.3648\n",
      "\n",
      "Epoch 261/1199\n",
      "------------------------\n",
      "train Loss: 0.0551 Acc: 98.6868\n",
      "test Loss: 0.1084 Acc: 97.7671\n",
      "\n",
      "Epoch 262/1199\n",
      "------------------------\n",
      "train Loss: 0.0436 Acc: 98.9571\n",
      "test Loss: 0.1079 Acc: 97.8729\n",
      "\n",
      "Epoch 263/1199\n",
      "------------------------\n",
      "train Loss: 0.0402 Acc: 99.0672\n",
      "test Loss: 0.1028 Acc: 97.9220\n",
      "\n",
      "Epoch 264/1199\n",
      "------------------------\n",
      "train Loss: 0.0357 Acc: 99.1954\n",
      "test Loss: 0.1052 Acc: 97.9237\n",
      "\n",
      "Epoch 265/1199\n",
      "------------------------\n",
      "train Loss: 0.0359 Acc: 99.1828\n",
      "test Loss: 0.1052 Acc: 97.9356\n",
      "\n",
      "Epoch 266/1199\n",
      "------------------------\n",
      "train Loss: 0.0361 Acc: 99.1971\n",
      "test Loss: 0.1062 Acc: 97.9325\n",
      "\n",
      "Epoch 267/1199\n",
      "------------------------\n",
      "train Loss: 0.0380 Acc: 99.1283\n",
      "test Loss: 0.1067 Acc: 97.9146\n",
      "\n",
      "Epoch 268/1199\n",
      "------------------------\n",
      "train Loss: 0.0354 Acc: 99.1963\n",
      "test Loss: 0.1015 Acc: 97.9672\n",
      "\n",
      "Epoch 269/1199\n",
      "------------------------\n",
      "train Loss: 0.0366 Acc: 99.1649\n",
      "test Loss: 0.1040 Acc: 97.9424\n",
      "\n",
      "Epoch 270/1199\n",
      "------------------------\n",
      "train Loss: 0.0332 Acc: 99.2480\n",
      "test Loss: 0.1032 Acc: 97.9458\n",
      "\n",
      "Epoch 271/1199\n",
      "------------------------\n",
      "train Loss: 0.0314 Acc: 99.2823\n",
      "test Loss: 0.1024 Acc: 98.0002\n",
      "\n",
      "Epoch 272/1199\n",
      "------------------------\n",
      "train Loss: 0.0336 Acc: 99.2432\n",
      "test Loss: 0.1031 Acc: 98.0161\n",
      "\n",
      "Epoch 273/1199\n",
      "------------------------\n",
      "train Loss: 0.0336 Acc: 99.2520\n",
      "test Loss: 0.1004 Acc: 98.0543\n",
      "\n",
      "Epoch 274/1199\n",
      "------------------------\n",
      "train Loss: 0.0353 Acc: 99.2068\n",
      "test Loss: 0.1089 Acc: 97.8728\n",
      "\n",
      "Epoch 275/1199\n",
      "------------------------\n",
      "train Loss: 0.0368 Acc: 99.1553\n",
      "test Loss: 0.1096 Acc: 97.8551\n",
      "\n",
      "Epoch 276/1199\n",
      "------------------------\n",
      "train Loss: 0.0342 Acc: 99.2475\n",
      "test Loss: 0.1031 Acc: 98.0050\n",
      "\n",
      "Epoch 277/1199\n",
      "------------------------\n",
      "train Loss: 0.0485 Acc: 98.8643\n",
      "test Loss: 0.1135 Acc: 97.7472\n",
      "\n",
      "Epoch 278/1199\n",
      "------------------------\n",
      "train Loss: 0.0455 Acc: 98.9208\n",
      "test Loss: 0.1091 Acc: 97.7938\n",
      "\n",
      "Epoch 279/1199\n",
      "------------------------\n",
      "train Loss: 0.0440 Acc: 98.9607\n",
      "test Loss: 0.1131 Acc: 97.7173\n",
      "\n",
      "Epoch 280/1199\n",
      "------------------------\n",
      "train Loss: 0.0412 Acc: 99.0231\n",
      "test Loss: 0.1104 Acc: 97.8116\n",
      "\n",
      "Epoch 281/1199\n",
      "------------------------\n",
      "train Loss: 0.0415 Acc: 99.0221\n",
      "test Loss: 0.1061 Acc: 97.8596\n",
      "\n",
      "Epoch 282/1199\n",
      "------------------------\n",
      "train Loss: 0.0385 Acc: 99.1046\n",
      "test Loss: 0.1047 Acc: 97.9674\n",
      "\n",
      "Epoch 283/1199\n",
      "------------------------\n",
      "train Loss: 0.0348 Acc: 99.2064\n",
      "test Loss: 0.1057 Acc: 97.8797\n",
      "\n",
      "Epoch 284/1199\n",
      "------------------------\n",
      "train Loss: 0.0346 Acc: 99.2198\n",
      "test Loss: 0.1015 Acc: 97.9650\n",
      "\n",
      "Epoch 285/1199\n",
      "------------------------\n",
      "train Loss: 0.0349 Acc: 99.2169\n",
      "test Loss: 0.1052 Acc: 97.9106\n",
      "\n",
      "Epoch 286/1199\n",
      "------------------------\n",
      "train Loss: 0.0337 Acc: 99.2559\n",
      "test Loss: 0.1053 Acc: 97.9786\n",
      "\n",
      "Epoch 287/1199\n",
      "------------------------\n",
      "train Loss: 0.0306 Acc: 99.3244\n",
      "test Loss: 0.1020 Acc: 98.0125\n",
      "\n",
      "Epoch 288/1199\n",
      "------------------------\n",
      "train Loss: 0.0304 Acc: 99.3167\n",
      "test Loss: 0.1049 Acc: 97.9755\n",
      "\n",
      "Epoch 289/1199\n",
      "------------------------\n",
      "train Loss: 0.0319 Acc: 99.3049\n",
      "test Loss: 0.1010 Acc: 98.0748\n",
      "\n",
      "Epoch 290/1199\n",
      "------------------------\n",
      "train Loss: 0.0307 Acc: 99.3189\n",
      "test Loss: 0.1033 Acc: 98.0246\n",
      "\n",
      "Epoch 291/1199\n",
      "------------------------\n",
      "train Loss: 0.0325 Acc: 99.2947\n",
      "test Loss: 0.1025 Acc: 98.0921\n",
      "\n",
      "Epoch 292/1199\n",
      "------------------------\n",
      "train Loss: 0.0306 Acc: 99.3308\n",
      "test Loss: 0.1033 Acc: 98.0152\n",
      "\n",
      "Epoch 293/1199\n",
      "------------------------\n",
      "train Loss: 0.0296 Acc: 99.3581\n",
      "test Loss: 0.1020 Acc: 98.0539\n",
      "\n",
      "Epoch 294/1199\n",
      "------------------------\n",
      "train Loss: 0.0289 Acc: 99.3669\n",
      "test Loss: 0.1003 Acc: 98.1069\n",
      "\n",
      "Epoch 295/1199\n",
      "------------------------\n",
      "train Loss: 0.0318 Acc: 99.3195\n",
      "test Loss: 0.1053 Acc: 98.0436\n",
      "\n",
      "Epoch 296/1199\n",
      "------------------------\n",
      "train Loss: 0.0281 Acc: 99.3777\n",
      "test Loss: 0.1076 Acc: 98.0348\n",
      "\n",
      "Epoch 297/1199\n",
      "------------------------\n",
      "train Loss: 0.0286 Acc: 99.3744\n",
      "test Loss: 0.1042 Acc: 98.0221\n",
      "\n",
      "Epoch 298/1199\n",
      "------------------------\n",
      "train Loss: 0.0345 Acc: 99.2246\n",
      "test Loss: 0.1137 Acc: 97.8525\n",
      "\n",
      "Epoch 299/1199\n",
      "------------------------\n",
      "train Loss: 0.0399 Acc: 99.0885\n",
      "test Loss: 0.1126 Acc: 97.8460\n",
      "\n",
      "Epoch 300/1199\n",
      "------------------------\n",
      "train Loss: 0.0306 Acc: 99.3095\n",
      "test Loss: 0.1013 Acc: 98.0389\n",
      "\n",
      "Epoch 301/1199\n",
      "------------------------\n",
      "train Loss: 0.0283 Acc: 99.3831\n",
      "test Loss: 0.1031 Acc: 98.0747\n",
      "\n",
      "Epoch 302/1199\n",
      "------------------------\n",
      "train Loss: 0.0282 Acc: 99.3868\n",
      "test Loss: 0.1022 Acc: 98.0826\n",
      "\n",
      "Epoch 303/1199\n",
      "------------------------\n",
      "train Loss: 0.0289 Acc: 99.3882\n",
      "test Loss: 0.0999 Acc: 98.0988\n",
      "\n",
      "Epoch 304/1199\n",
      "------------------------\n",
      "train Loss: 0.0274 Acc: 99.4162\n",
      "test Loss: 0.1008 Acc: 98.1076\n",
      "\n",
      "Epoch 305/1199\n",
      "------------------------\n",
      "train Loss: 0.0263 Acc: 99.4512\n",
      "test Loss: 0.1006 Acc: 98.0570\n",
      "\n",
      "Epoch 306/1199\n",
      "------------------------\n",
      "train Loss: 0.0263 Acc: 99.4432\n",
      "test Loss: 0.1004 Acc: 98.1481\n",
      "\n",
      "Epoch 307/1199\n",
      "------------------------\n",
      "train Loss: 0.0291 Acc: 99.3966\n",
      "test Loss: 0.0974 Acc: 98.2037\n",
      "\n",
      "Epoch 308/1199\n",
      "------------------------\n",
      "train Loss: 0.0282 Acc: 99.4081\n",
      "test Loss: 0.0996 Acc: 98.1239\n",
      "\n",
      "Epoch 309/1199\n",
      "------------------------\n",
      "train Loss: 0.0254 Acc: 99.4598\n",
      "test Loss: 0.1024 Acc: 98.1068\n",
      "\n",
      "Epoch 310/1199\n",
      "------------------------\n",
      "train Loss: 0.0265 Acc: 99.4243\n",
      "test Loss: 0.0999 Acc: 98.1370\n",
      "\n",
      "Epoch 311/1199\n",
      "------------------------\n",
      "train Loss: 0.0276 Acc: 99.4313\n",
      "test Loss: 0.0982 Acc: 98.1244\n",
      "\n",
      "Epoch 312/1199\n",
      "------------------------\n",
      "train Loss: 0.0264 Acc: 99.4489\n",
      "test Loss: 0.1005 Acc: 98.1481\n",
      "\n",
      "Epoch 313/1199\n",
      "------------------------\n",
      "train Loss: 0.0261 Acc: 99.4558\n",
      "test Loss: 0.0996 Acc: 98.1797\n",
      "\n",
      "Epoch 314/1199\n",
      "------------------------\n",
      "train Loss: 0.0259 Acc: 99.4497\n",
      "test Loss: 0.0982 Acc: 98.1403\n",
      "\n",
      "Epoch 315/1199\n",
      "------------------------\n",
      "train Loss: 0.0284 Acc: 99.4160\n",
      "test Loss: 0.0987 Acc: 98.1633\n",
      "\n",
      "Epoch 316/1199\n",
      "------------------------\n",
      "train Loss: 0.0263 Acc: 99.4456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1010 Acc: 98.1206\n",
      "\n",
      "Epoch 317/1199\n",
      "------------------------\n",
      "train Loss: 0.0255 Acc: 99.4684\n",
      "test Loss: 0.0976 Acc: 98.1417\n",
      "\n",
      "Epoch 318/1199\n",
      "------------------------\n",
      "train Loss: 0.0256 Acc: 99.4591\n",
      "test Loss: 0.1017 Acc: 98.0861\n",
      "\n",
      "Epoch 319/1199\n",
      "------------------------\n",
      "train Loss: 0.0238 Acc: 99.4974\n",
      "test Loss: 0.1013 Acc: 98.1204\n",
      "\n",
      "Epoch 320/1199\n",
      "------------------------\n",
      "train Loss: 0.0243 Acc: 99.4927\n",
      "test Loss: 0.1005 Acc: 98.1548\n",
      "\n",
      "Epoch 321/1199\n",
      "------------------------\n",
      "train Loss: 0.0258 Acc: 99.4619\n",
      "test Loss: 0.0974 Acc: 98.1878\n",
      "\n",
      "Epoch 322/1199\n",
      "------------------------\n",
      "train Loss: 0.0265 Acc: 99.4477\n",
      "test Loss: 0.1012 Acc: 98.1063\n",
      "\n",
      "Epoch 323/1199\n",
      "------------------------\n",
      "train Loss: 0.0266 Acc: 99.4669\n",
      "test Loss: 0.1007 Acc: 98.2253\n",
      "\n",
      "Epoch 324/1199\n",
      "------------------------\n",
      "train Loss: 0.0243 Acc: 99.4923\n",
      "test Loss: 0.0984 Acc: 98.1645\n",
      "\n",
      "Epoch 325/1199\n",
      "------------------------\n",
      "train Loss: 0.0242 Acc: 99.4894\n",
      "test Loss: 0.1008 Acc: 98.0733\n",
      "\n",
      "Epoch 326/1199\n",
      "------------------------\n",
      "train Loss: 0.0241 Acc: 99.4865\n",
      "test Loss: 0.0993 Acc: 98.2095\n",
      "\n",
      "Epoch 327/1199\n",
      "------------------------\n",
      "train Loss: 0.0246 Acc: 99.4875\n",
      "test Loss: 0.0978 Acc: 98.1577\n",
      "\n",
      "Epoch 328/1199\n",
      "------------------------\n",
      "train Loss: 0.0250 Acc: 99.4650\n",
      "test Loss: 0.0987 Acc: 98.1607\n",
      "\n",
      "Epoch 329/1199\n",
      "------------------------\n",
      "train Loss: 0.0244 Acc: 99.4920\n",
      "test Loss: 0.1017 Acc: 98.1037\n",
      "\n",
      "Epoch 330/1199\n",
      "------------------------\n",
      "train Loss: 0.0247 Acc: 99.4788\n",
      "test Loss: 0.0983 Acc: 98.1800\n",
      "\n",
      "Epoch 331/1199\n",
      "------------------------\n",
      "train Loss: 0.0279 Acc: 99.4244\n",
      "test Loss: 0.0990 Acc: 98.1648\n",
      "\n",
      "Epoch 332/1199\n",
      "------------------------\n",
      "train Loss: 0.0254 Acc: 99.4683\n",
      "test Loss: 0.0992 Acc: 98.1921\n",
      "\n",
      "Epoch 333/1199\n",
      "------------------------\n",
      "train Loss: 0.0270 Acc: 99.4473\n",
      "test Loss: 0.0987 Acc: 98.2296\n",
      "\n",
      "Epoch 334/1199\n",
      "------------------------\n",
      "train Loss: 0.0284 Acc: 99.4229\n",
      "test Loss: 0.0983 Acc: 98.1591\n",
      "\n",
      "Epoch 335/1199\n",
      "------------------------\n",
      "train Loss: 0.0240 Acc: 99.5059\n",
      "test Loss: 0.0978 Acc: 98.1752\n",
      "\n",
      "Epoch 336/1199\n",
      "------------------------\n",
      "train Loss: 0.0254 Acc: 99.4700\n",
      "test Loss: 0.0992 Acc: 98.1755\n",
      "\n",
      "Epoch 337/1199\n",
      "------------------------\n",
      "train Loss: 0.0245 Acc: 99.4890\n",
      "test Loss: 0.1000 Acc: 98.1662\n",
      "\n",
      "Epoch 338/1199\n",
      "------------------------\n",
      "train Loss: 0.0246 Acc: 99.4891\n",
      "test Loss: 0.1034 Acc: 98.1586\n",
      "\n",
      "Epoch 339/1199\n",
      "------------------------\n",
      "train Loss: 0.0271 Acc: 99.4395\n",
      "test Loss: 0.0998 Acc: 98.2168\n",
      "\n",
      "Epoch 340/1199\n",
      "------------------------\n",
      "train Loss: 0.0237 Acc: 99.5142\n",
      "test Loss: 0.1019 Acc: 98.0565\n",
      "\n",
      "Epoch 341/1199\n",
      "------------------------\n",
      "train Loss: 0.0245 Acc: 99.5000\n",
      "test Loss: 0.0967 Acc: 98.1693\n",
      "\n",
      "Epoch 342/1199\n",
      "------------------------\n",
      "train Loss: 0.0239 Acc: 99.5028\n",
      "test Loss: 0.1014 Acc: 98.1985\n",
      "\n",
      "Epoch 343/1199\n",
      "------------------------\n",
      "train Loss: 0.0231 Acc: 99.5184\n",
      "test Loss: 0.0996 Acc: 98.2066\n",
      "\n",
      "Epoch 344/1199\n",
      "------------------------\n",
      "train Loss: 0.0256 Acc: 99.4657\n",
      "test Loss: 0.1025 Acc: 98.1840\n",
      "\n",
      "Epoch 345/1199\n",
      "------------------------\n",
      "train Loss: 0.0264 Acc: 99.4468\n",
      "test Loss: 0.1008 Acc: 98.1591\n",
      "\n",
      "Epoch 346/1199\n",
      "------------------------\n",
      "train Loss: 0.0242 Acc: 99.5013\n",
      "test Loss: 0.1027 Acc: 98.2040\n",
      "\n",
      "Epoch 347/1199\n",
      "------------------------\n",
      "train Loss: 0.0256 Acc: 99.4741\n",
      "test Loss: 0.0990 Acc: 98.2325\n",
      "\n",
      "Epoch 348/1199\n",
      "------------------------\n",
      "train Loss: 0.0236 Acc: 99.5009\n",
      "test Loss: 0.1016 Acc: 98.2130\n",
      "\n",
      "Epoch 349/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.5166\n",
      "test Loss: 0.1012 Acc: 98.1475\n",
      "\n",
      "Epoch 350/1199\n",
      "------------------------\n",
      "train Loss: 0.0238 Acc: 99.5047\n",
      "test Loss: 0.0986 Acc: 98.2190\n",
      "\n",
      "Epoch 351/1199\n",
      "------------------------\n",
      "train Loss: 0.0253 Acc: 99.4893\n",
      "test Loss: 0.1015 Acc: 98.1539\n",
      "\n",
      "Epoch 352/1199\n",
      "------------------------\n",
      "train Loss: 0.0240 Acc: 99.5078\n",
      "test Loss: 0.0970 Acc: 98.2016\n",
      "\n",
      "Epoch 353/1199\n",
      "------------------------\n",
      "train Loss: 0.0251 Acc: 99.4784\n",
      "test Loss: 0.1004 Acc: 98.1857\n",
      "\n",
      "Epoch 354/1199\n",
      "------------------------\n",
      "train Loss: 0.0242 Acc: 99.4939\n",
      "test Loss: 0.0998 Acc: 98.1624\n",
      "\n",
      "Epoch 355/1199\n",
      "------------------------\n",
      "train Loss: 0.0243 Acc: 99.5039\n",
      "test Loss: 0.1020 Acc: 98.1576\n",
      "\n",
      "Epoch 356/1199\n",
      "------------------------\n",
      "train Loss: 0.0244 Acc: 99.4948\n",
      "test Loss: 0.0999 Acc: 98.2080\n",
      "\n",
      "Epoch 357/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.5122\n",
      "test Loss: 0.1024 Acc: 98.1548\n",
      "\n",
      "Epoch 358/1199\n",
      "------------------------\n",
      "train Loss: 0.0240 Acc: 99.5059\n",
      "test Loss: 0.1001 Acc: 98.1976\n",
      "\n",
      "Epoch 359/1199\n",
      "------------------------\n",
      "train Loss: 0.0242 Acc: 99.4894\n",
      "test Loss: 0.1023 Acc: 98.1797\n",
      "\n",
      "Epoch 360/1199\n",
      "------------------------\n",
      "train Loss: 0.0248 Acc: 99.4928\n",
      "test Loss: 0.1012 Acc: 98.1746\n",
      "\n",
      "Epoch 361/1199\n",
      "------------------------\n",
      "train Loss: 0.0247 Acc: 99.4985\n",
      "test Loss: 0.1009 Acc: 98.1582\n",
      "\n",
      "Epoch 362/1199\n",
      "------------------------\n",
      "train Loss: 0.0229 Acc: 99.5227\n",
      "test Loss: 0.0986 Acc: 98.2310\n",
      "\n",
      "Epoch 363/1199\n",
      "------------------------\n",
      "train Loss: 0.0228 Acc: 99.5283\n",
      "test Loss: 0.0994 Acc: 98.1938\n",
      "\n",
      "Epoch 364/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.5230\n",
      "test Loss: 0.1049 Acc: 98.1047\n",
      "\n",
      "Epoch 365/1199\n",
      "------------------------\n",
      "train Loss: 0.0246 Acc: 99.5036\n",
      "test Loss: 0.1003 Acc: 98.1496\n",
      "\n",
      "Epoch 366/1199\n",
      "------------------------\n",
      "train Loss: 0.0232 Acc: 99.5163\n",
      "test Loss: 0.0985 Acc: 98.2080\n",
      "\n",
      "Epoch 367/1199\n",
      "------------------------\n",
      "train Loss: 0.0230 Acc: 99.5271\n",
      "test Loss: 0.1025 Acc: 98.1686\n",
      "\n",
      "Epoch 368/1199\n",
      "------------------------\n",
      "train Loss: 0.0241 Acc: 99.4987\n",
      "test Loss: 0.1011 Acc: 98.2109\n",
      "\n",
      "Epoch 369/1199\n",
      "------------------------\n",
      "train Loss: 0.0245 Acc: 99.5000\n",
      "test Loss: 0.0987 Acc: 98.1914\n",
      "\n",
      "Epoch 370/1199\n",
      "------------------------\n",
      "train Loss: 0.0244 Acc: 99.4975\n",
      "test Loss: 0.1016 Acc: 98.1037\n",
      "\n",
      "Epoch 371/1199\n",
      "------------------------\n",
      "train Loss: 0.0221 Acc: 99.5444\n",
      "test Loss: 0.1000 Acc: 98.1574\n",
      "\n",
      "Epoch 372/1199\n",
      "------------------------\n",
      "train Loss: 0.0228 Acc: 99.5303\n",
      "test Loss: 0.1038 Acc: 98.0997\n",
      "\n",
      "Epoch 373/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.5078\n",
      "test Loss: 0.1033 Acc: 98.1059\n",
      "\n",
      "Epoch 374/1199\n",
      "------------------------\n",
      "train Loss: 0.0229 Acc: 99.5232\n",
      "test Loss: 0.1010 Acc: 98.1824\n",
      "\n",
      "Epoch 375/1199\n",
      "------------------------\n",
      "train Loss: 0.0232 Acc: 99.5189\n",
      "test Loss: 0.1032 Acc: 98.1563\n",
      "\n",
      "Epoch 376/1199\n",
      "------------------------\n",
      "train Loss: 0.0244 Acc: 99.5091\n",
      "test Loss: 0.1019 Acc: 98.1990\n",
      "\n",
      "Epoch 377/1199\n",
      "------------------------\n",
      "train Loss: 0.0224 Acc: 99.5359\n",
      "test Loss: 0.1005 Acc: 98.2249\n",
      "\n",
      "Epoch 378/1199\n",
      "------------------------\n",
      "train Loss: 0.0230 Acc: 99.5216\n",
      "test Loss: 0.1014 Acc: 98.2237\n",
      "\n",
      "Epoch 379/1199\n",
      "------------------------\n",
      "train Loss: 0.0236 Acc: 99.5219\n",
      "test Loss: 0.1029 Acc: 98.1529\n",
      "\n",
      "Epoch 380/1199\n",
      "------------------------\n",
      "train Loss: 0.0242 Acc: 99.5135\n",
      "test Loss: 0.1011 Acc: 98.2323\n",
      "\n",
      "Epoch 381/1199\n",
      "------------------------\n",
      "train Loss: 0.0228 Acc: 99.5289\n",
      "test Loss: 0.1014 Acc: 98.1541\n",
      "\n",
      "Epoch 382/1199\n",
      "------------------------\n",
      "train Loss: 0.0226 Acc: 99.5303\n",
      "test Loss: 0.1010 Acc: 98.2144\n",
      "\n",
      "Epoch 383/1199\n",
      "------------------------\n",
      "train Loss: 0.0245 Acc: 99.4957\n",
      "test Loss: 0.1046 Acc: 98.1463\n",
      "\n",
      "Epoch 384/1199\n",
      "------------------------\n",
      "train Loss: 0.0241 Acc: 99.5087\n",
      "test Loss: 0.0999 Acc: 98.2342\n",
      "\n",
      "Epoch 385/1199\n",
      "------------------------\n",
      "train Loss: 0.0251 Acc: 99.4914\n",
      "test Loss: 0.1012 Acc: 98.2310\n",
      "\n",
      "Epoch 386/1199\n",
      "------------------------\n",
      "train Loss: 0.0219 Acc: 99.5478\n",
      "test Loss: 0.1011 Acc: 98.2368\n",
      "\n",
      "Epoch 387/1199\n",
      "------------------------\n",
      "train Loss: 0.0239 Acc: 99.5133\n",
      "test Loss: 0.1026 Acc: 98.1486\n",
      "\n",
      "Epoch 388/1199\n",
      "------------------------\n",
      "train Loss: 0.0227 Acc: 99.5219\n",
      "test Loss: 0.0987 Acc: 98.2092\n",
      "\n",
      "Epoch 389/1199\n",
      "------------------------\n",
      "train Loss: 0.0232 Acc: 99.5314\n",
      "test Loss: 0.1004 Acc: 98.2194\n",
      "\n",
      "Epoch 390/1199\n",
      "------------------------\n",
      "train Loss: 0.0229 Acc: 99.5323\n",
      "test Loss: 0.0995 Acc: 98.1874\n",
      "\n",
      "Epoch 391/1199\n",
      "------------------------\n",
      "train Loss: 0.0213 Acc: 99.5606\n",
      "test Loss: 0.1020 Acc: 98.1778\n",
      "\n",
      "Epoch 392/1199\n",
      "------------------------\n",
      "train Loss: 0.0227 Acc: 99.5376\n",
      "test Loss: 0.1026 Acc: 98.1852\n",
      "\n",
      "Epoch 393/1199\n",
      "------------------------\n",
      "train Loss: 0.0217 Acc: 99.5499\n",
      "test Loss: 0.1024 Acc: 98.1895\n",
      "\n",
      "Epoch 394/1199\n",
      "------------------------\n",
      "train Loss: 0.0244 Acc: 99.5012\n",
      "test Loss: 0.1021 Acc: 98.2196\n",
      "\n",
      "Epoch 395/1199\n",
      "------------------------\n",
      "train Loss: 0.0228 Acc: 99.5373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1014 Acc: 98.2106\n",
      "\n",
      "Epoch 396/1199\n",
      "------------------------\n",
      "train Loss: 0.0217 Acc: 99.5488\n",
      "test Loss: 0.0988 Acc: 98.1952\n",
      "\n",
      "Epoch 397/1199\n",
      "------------------------\n",
      "train Loss: 0.0220 Acc: 99.5481\n",
      "test Loss: 0.1018 Acc: 98.2277\n",
      "\n",
      "Epoch 398/1199\n",
      "------------------------\n",
      "train Loss: 0.0225 Acc: 99.5271\n",
      "test Loss: 0.1022 Acc: 98.1323\n",
      "\n",
      "Epoch 399/1199\n",
      "------------------------\n",
      "train Loss: 0.0234 Acc: 99.5206\n",
      "test Loss: 0.1003 Acc: 98.1063\n",
      "\n",
      "Epoch 400/1199\n",
      "------------------------\n",
      "train Loss: 0.0221 Acc: 99.5413\n",
      "test Loss: 0.1009 Acc: 98.2323\n",
      "\n",
      "Epoch 401/1199\n",
      "------------------------\n",
      "train Loss: 0.0230 Acc: 99.5338\n",
      "test Loss: 0.1024 Acc: 98.2170\n",
      "\n",
      "Epoch 402/1199\n",
      "------------------------\n",
      "train Loss: 0.0234 Acc: 99.5214\n",
      "test Loss: 0.1039 Acc: 98.2189\n",
      "\n",
      "Epoch 403/1199\n",
      "------------------------\n",
      "train Loss: 0.0215 Acc: 99.5605\n",
      "test Loss: 0.1020 Acc: 98.2220\n",
      "\n",
      "Epoch 404/1199\n",
      "------------------------\n",
      "train Loss: 0.0225 Acc: 99.5339\n",
      "test Loss: 0.1032 Acc: 98.2379\n",
      "\n",
      "Epoch 405/1199\n",
      "------------------------\n",
      "train Loss: 0.0223 Acc: 99.5485\n",
      "test Loss: 0.1012 Acc: 98.1940\n",
      "\n",
      "Epoch 406/1199\n",
      "------------------------\n",
      "train Loss: 0.0218 Acc: 99.5508\n",
      "test Loss: 0.1027 Acc: 98.1864\n",
      "\n",
      "Epoch 407/1199\n",
      "------------------------\n",
      "train Loss: 0.0224 Acc: 99.5406\n",
      "test Loss: 0.1040 Acc: 98.0814\n",
      "\n",
      "Epoch 408/1199\n",
      "------------------------\n",
      "train Loss: 0.0220 Acc: 99.5509\n",
      "test Loss: 0.1012 Acc: 98.1840\n",
      "\n",
      "Epoch 409/1199\n",
      "------------------------\n",
      "train Loss: 0.0207 Acc: 99.5672\n",
      "test Loss: 0.1000 Acc: 98.2377\n",
      "\n",
      "Epoch 410/1199\n",
      "------------------------\n",
      "train Loss: 0.0240 Acc: 99.5172\n",
      "test Loss: 0.1027 Acc: 98.2130\n",
      "\n",
      "Epoch 411/1199\n",
      "------------------------\n",
      "train Loss: 0.0242 Acc: 99.5118\n",
      "test Loss: 0.1023 Acc: 98.2306\n",
      "\n",
      "Epoch 412/1199\n",
      "------------------------\n",
      "train Loss: 0.0203 Acc: 99.5775\n",
      "test Loss: 0.0995 Acc: 98.2085\n",
      "\n",
      "Epoch 413/1199\n",
      "------------------------\n",
      "train Loss: 0.0207 Acc: 99.5697\n",
      "test Loss: 0.1030 Acc: 98.2171\n",
      "\n",
      "Epoch 414/1199\n",
      "------------------------\n",
      "train Loss: 0.0216 Acc: 99.5580\n",
      "test Loss: 0.1029 Acc: 98.2159\n",
      "\n",
      "Epoch 415/1199\n",
      "------------------------\n",
      "train Loss: 0.0230 Acc: 99.5287\n",
      "test Loss: 0.1022 Acc: 98.1840\n",
      "\n",
      "Epoch 416/1199\n",
      "------------------------\n",
      "train Loss: 0.0238 Acc: 99.5181\n",
      "test Loss: 0.1016 Acc: 98.2370\n",
      "\n",
      "Epoch 417/1199\n",
      "------------------------\n",
      "train Loss: 0.0229 Acc: 99.5426\n",
      "test Loss: 0.1003 Acc: 98.1859\n",
      "\n",
      "Epoch 418/1199\n",
      "------------------------\n",
      "train Loss: 0.0228 Acc: 99.5314\n",
      "test Loss: 0.1019 Acc: 98.1562\n",
      "\n",
      "Epoch 419/1199\n",
      "------------------------\n",
      "train Loss: 0.0217 Acc: 99.5564\n",
      "test Loss: 0.0998 Acc: 98.1900\n",
      "\n",
      "Epoch 420/1199\n",
      "------------------------\n",
      "train Loss: 0.0222 Acc: 99.5548\n",
      "test Loss: 0.0999 Acc: 98.1911\n",
      "\n",
      "Epoch 421/1199\n",
      "------------------------\n",
      "train Loss: 0.0215 Acc: 99.5555\n",
      "test Loss: 0.1016 Acc: 98.1974\n",
      "\n",
      "Epoch 422/1199\n",
      "------------------------\n",
      "train Loss: 0.0215 Acc: 99.5485\n",
      "test Loss: 0.1023 Acc: 98.1814\n",
      "\n",
      "Epoch 423/1199\n",
      "------------------------\n",
      "train Loss: 0.0215 Acc: 99.5676\n",
      "test Loss: 0.1002 Acc: 98.1883\n",
      "\n",
      "Epoch 424/1199\n",
      "------------------------\n",
      "train Loss: 0.0228 Acc: 99.5300\n",
      "test Loss: 0.0985 Acc: 98.2045\n",
      "\n",
      "Epoch 425/1199\n",
      "------------------------\n",
      "train Loss: 0.0226 Acc: 99.5412\n",
      "test Loss: 0.0984 Acc: 98.2031\n",
      "\n",
      "Epoch 426/1199\n",
      "------------------------\n",
      "train Loss: 0.0205 Acc: 99.5747\n",
      "test Loss: 0.1059 Acc: 98.1914\n",
      "\n",
      "Epoch 427/1199\n",
      "------------------------\n",
      "train Loss: 0.0213 Acc: 99.5746\n",
      "test Loss: 0.1033 Acc: 98.2137\n",
      "\n",
      "Epoch 428/1199\n",
      "------------------------\n",
      "train Loss: 0.0208 Acc: 99.5683\n",
      "test Loss: 0.1007 Acc: 98.1703\n",
      "\n",
      "Epoch 429/1199\n",
      "------------------------\n",
      "train Loss: 0.0215 Acc: 99.5570\n",
      "test Loss: 0.1033 Acc: 98.1957\n",
      "\n",
      "Epoch 430/1199\n",
      "------------------------\n",
      "train Loss: 0.0222 Acc: 99.5435\n",
      "test Loss: 0.1051 Acc: 98.1881\n",
      "\n",
      "Epoch 431/1199\n",
      "------------------------\n",
      "train Loss: 0.0253 Acc: 99.4837\n",
      "test Loss: 0.1053 Acc: 98.1688\n",
      "\n",
      "Epoch 432/1199\n",
      "------------------------\n",
      "train Loss: 0.0240 Acc: 99.5162\n",
      "test Loss: 0.1024 Acc: 98.1936\n",
      "\n",
      "Epoch 433/1199\n",
      "------------------------\n",
      "train Loss: 0.0223 Acc: 99.5415\n",
      "test Loss: 0.1032 Acc: 98.1506\n",
      "\n",
      "Epoch 434/1199\n",
      "------------------------\n",
      "train Loss: 0.0212 Acc: 99.5738\n",
      "test Loss: 0.1010 Acc: 98.2301\n",
      "\n",
      "Epoch 435/1199\n",
      "------------------------\n",
      "train Loss: 0.0229 Acc: 99.5336\n",
      "test Loss: 0.1014 Acc: 98.2427\n",
      "\n",
      "Epoch 436/1199\n",
      "------------------------\n",
      "train Loss: 0.0225 Acc: 99.5405\n",
      "test Loss: 0.1021 Acc: 98.2189\n",
      "\n",
      "Epoch 437/1199\n",
      "------------------------\n",
      "train Loss: 0.0213 Acc: 99.5749\n",
      "test Loss: 0.1042 Acc: 98.1508\n",
      "\n",
      "Epoch 438/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.5349\n",
      "test Loss: 0.1000 Acc: 98.1964\n",
      "\n",
      "Epoch 439/1199\n",
      "------------------------\n",
      "train Loss: 0.0218 Acc: 99.5635\n",
      "test Loss: 0.1029 Acc: 98.2745\n",
      "\n",
      "Epoch 440/1199\n",
      "------------------------\n",
      "train Loss: 0.0197 Acc: 99.5922\n",
      "test Loss: 0.1043 Acc: 98.2253\n",
      "\n",
      "Epoch 441/1199\n",
      "------------------------\n",
      "train Loss: 0.0208 Acc: 99.5798\n",
      "test Loss: 0.1044 Acc: 98.1881\n",
      "\n",
      "Epoch 442/1199\n",
      "------------------------\n",
      "train Loss: 0.0208 Acc: 99.5746\n",
      "test Loss: 0.1011 Acc: 98.1897\n",
      "\n",
      "Epoch 443/1199\n",
      "------------------------\n",
      "train Loss: 0.0219 Acc: 99.5537\n",
      "test Loss: 0.1012 Acc: 98.2278\n",
      "\n",
      "Epoch 444/1199\n",
      "------------------------\n",
      "train Loss: 0.0222 Acc: 99.5498\n",
      "test Loss: 0.1027 Acc: 98.1923\n",
      "\n",
      "Epoch 445/1199\n",
      "------------------------\n",
      "train Loss: 0.0210 Acc: 99.5708\n",
      "test Loss: 0.1015 Acc: 98.2215\n",
      "\n",
      "Epoch 446/1199\n",
      "------------------------\n",
      "train Loss: 0.0206 Acc: 99.5763\n",
      "test Loss: 0.1029 Acc: 98.1719\n",
      "\n",
      "Epoch 447/1199\n",
      "------------------------\n",
      "train Loss: 0.0226 Acc: 99.5499\n",
      "test Loss: 0.1026 Acc: 98.1862\n",
      "\n",
      "Epoch 448/1199\n",
      "------------------------\n",
      "train Loss: 0.0219 Acc: 99.5565\n",
      "test Loss: 0.1010 Acc: 98.2301\n",
      "\n",
      "Epoch 449/1199\n",
      "------------------------\n",
      "train Loss: 0.0204 Acc: 99.5896\n",
      "test Loss: 0.1043 Acc: 98.1018\n",
      "\n",
      "Epoch 450/1199\n",
      "------------------------\n",
      "train Loss: 0.0218 Acc: 99.5584\n",
      "test Loss: 0.0998 Acc: 98.2396\n",
      "\n",
      "Epoch 451/1199\n",
      "------------------------\n",
      "train Loss: 0.0212 Acc: 99.5756\n",
      "test Loss: 0.1018 Acc: 98.1788\n",
      "\n",
      "Epoch 452/1199\n",
      "------------------------\n",
      "train Loss: 0.0207 Acc: 99.5901\n",
      "test Loss: 0.1028 Acc: 98.1510\n",
      "\n",
      "Epoch 453/1199\n",
      "------------------------\n",
      "train Loss: 0.0208 Acc: 99.5732\n",
      "test Loss: 0.1054 Acc: 98.1258\n",
      "\n",
      "Epoch 454/1199\n",
      "------------------------\n",
      "train Loss: 0.0203 Acc: 99.5870\n",
      "test Loss: 0.1032 Acc: 98.2436\n",
      "\n",
      "Epoch 455/1199\n",
      "------------------------\n",
      "train Loss: 0.0224 Acc: 99.5370\n",
      "test Loss: 0.1052 Acc: 98.1902\n",
      "\n",
      "Epoch 456/1199\n",
      "------------------------\n",
      "train Loss: 0.0205 Acc: 99.5729\n",
      "test Loss: 0.1044 Acc: 98.2247\n",
      "\n",
      "Epoch 457/1199\n",
      "------------------------\n",
      "train Loss: 0.0207 Acc: 99.5798\n",
      "test Loss: 0.1020 Acc: 98.2532\n",
      "\n",
      "Epoch 458/1199\n",
      "------------------------\n",
      "train Loss: 0.0220 Acc: 99.5480\n",
      "test Loss: 0.1035 Acc: 98.1721\n",
      "\n",
      "Epoch 459/1199\n",
      "------------------------\n",
      "train Loss: 0.0203 Acc: 99.5941\n",
      "test Loss: 0.1066 Acc: 98.2311\n",
      "\n",
      "Epoch 460/1199\n",
      "------------------------\n",
      "train Loss: 0.0211 Acc: 99.5799\n",
      "test Loss: 0.1016 Acc: 98.2605\n",
      "\n",
      "Epoch 461/1199\n",
      "------------------------\n",
      "train Loss: 0.0205 Acc: 99.5929\n",
      "test Loss: 0.0996 Acc: 98.2230\n",
      "\n",
      "Epoch 462/1199\n",
      "------------------------\n",
      "train Loss: 0.0221 Acc: 99.5559\n",
      "test Loss: 0.1028 Acc: 98.2223\n",
      "\n",
      "Epoch 463/1199\n",
      "------------------------\n",
      "train Loss: 0.0201 Acc: 99.5986\n",
      "test Loss: 0.1029 Acc: 98.2342\n",
      "\n",
      "Epoch 464/1199\n",
      "------------------------\n",
      "train Loss: 0.0201 Acc: 99.5958\n",
      "test Loss: 0.1017 Acc: 98.2372\n",
      "\n",
      "Epoch 465/1199\n",
      "------------------------\n",
      "train Loss: 0.0218 Acc: 99.5599\n",
      "test Loss: 0.1009 Acc: 98.2581\n",
      "\n",
      "Epoch 466/1199\n",
      "------------------------\n",
      "train Loss: 0.0224 Acc: 99.5583\n",
      "test Loss: 0.1012 Acc: 98.2315\n",
      "\n",
      "Epoch 467/1199\n",
      "------------------------\n",
      "train Loss: 0.0204 Acc: 99.5836\n",
      "test Loss: 0.1034 Acc: 98.2318\n",
      "\n",
      "Epoch 468/1199\n",
      "------------------------\n",
      "train Loss: 0.0205 Acc: 99.5887\n",
      "test Loss: 0.1031 Acc: 98.2023\n",
      "\n",
      "Epoch 469/1199\n",
      "------------------------\n",
      "train Loss: 0.0201 Acc: 99.5887\n",
      "test Loss: 0.1020 Acc: 98.2140\n",
      "\n",
      "Epoch 470/1199\n",
      "------------------------\n",
      "train Loss: 0.0203 Acc: 99.5870\n",
      "test Loss: 0.1001 Acc: 98.2370\n",
      "\n",
      "Epoch 471/1199\n",
      "------------------------\n",
      "train Loss: 0.0206 Acc: 99.5817\n",
      "test Loss: 0.1042 Acc: 98.1824\n",
      "\n",
      "Epoch 472/1199\n",
      "------------------------\n",
      "train Loss: 0.0216 Acc: 99.5611\n",
      "test Loss: 0.1030 Acc: 98.2049\n",
      "\n",
      "Epoch 473/1199\n",
      "------------------------\n",
      "train Loss: 0.0210 Acc: 99.5744\n",
      "test Loss: 0.1029 Acc: 98.2555\n",
      "\n",
      "Epoch 474/1199\n",
      "------------------------\n",
      "train Loss: 0.0205 Acc: 99.5824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1021 Acc: 98.2736\n",
      "\n",
      "Epoch 475/1199\n",
      "------------------------\n",
      "train Loss: 0.0213 Acc: 99.5689\n",
      "test Loss: 0.1035 Acc: 98.2322\n",
      "\n",
      "Epoch 476/1199\n",
      "------------------------\n",
      "train Loss: 0.0213 Acc: 99.5669\n",
      "test Loss: 0.1024 Acc: 98.2370\n",
      "\n",
      "Epoch 477/1199\n",
      "------------------------\n",
      "train Loss: 0.0218 Acc: 99.5532\n",
      "test Loss: 0.1022 Acc: 98.2277\n",
      "\n",
      "Epoch 478/1199\n",
      "------------------------\n",
      "train Loss: 0.0200 Acc: 99.5850\n",
      "test Loss: 0.1038 Acc: 98.2493\n",
      "\n",
      "Epoch 479/1199\n",
      "------------------------\n",
      "train Loss: 0.0200 Acc: 99.6037\n",
      "test Loss: 0.1015 Acc: 98.1911\n",
      "\n",
      "Epoch 480/1199\n",
      "------------------------\n",
      "train Loss: 0.0208 Acc: 99.5836\n",
      "test Loss: 0.1002 Acc: 98.2251\n",
      "\n",
      "Epoch 481/1199\n",
      "------------------------\n",
      "train Loss: 0.0204 Acc: 99.5833\n",
      "test Loss: 0.1020 Acc: 98.2151\n",
      "\n",
      "Epoch 482/1199\n",
      "------------------------\n",
      "train Loss: 0.0207 Acc: 99.5900\n",
      "test Loss: 0.1069 Acc: 98.1643\n",
      "\n",
      "Epoch 483/1199\n",
      "------------------------\n",
      "train Loss: 0.0232 Acc: 99.5118\n",
      "test Loss: 0.1060 Acc: 98.1420\n",
      "\n",
      "Epoch 484/1199\n",
      "------------------------\n",
      "train Loss: 0.0239 Acc: 99.5016\n",
      "test Loss: 0.1052 Acc: 98.2354\n",
      "\n",
      "Epoch 485/1199\n",
      "------------------------\n",
      "train Loss: 0.0212 Acc: 99.5671\n",
      "test Loss: 0.1046 Acc: 98.1855\n",
      "\n",
      "Epoch 486/1199\n",
      "------------------------\n",
      "train Loss: 0.0212 Acc: 99.5624\n",
      "test Loss: 0.1026 Acc: 98.2137\n",
      "\n",
      "Epoch 487/1199\n",
      "------------------------\n",
      "train Loss: 0.0209 Acc: 99.5709\n",
      "test Loss: 0.1038 Acc: 98.2477\n",
      "\n",
      "Epoch 488/1199\n",
      "------------------------\n",
      "train Loss: 0.0198 Acc: 99.5953\n",
      "test Loss: 0.1047 Acc: 98.1914\n",
      "\n",
      "Epoch 489/1199\n",
      "------------------------\n",
      "train Loss: 0.0215 Acc: 99.5695\n",
      "test Loss: 0.0984 Acc: 98.2601\n",
      "\n",
      "Epoch 490/1199\n",
      "------------------------\n",
      "train Loss: 0.0201 Acc: 99.5896\n",
      "test Loss: 0.0997 Acc: 98.2202\n",
      "\n",
      "Epoch 491/1199\n",
      "------------------------\n",
      "train Loss: 0.0196 Acc: 99.6029\n",
      "test Loss: 0.0988 Acc: 98.2460\n",
      "\n",
      "Epoch 492/1199\n",
      "------------------------\n",
      "train Loss: 0.0196 Acc: 99.5916\n",
      "test Loss: 0.1028 Acc: 98.2398\n",
      "\n",
      "Epoch 493/1199\n",
      "------------------------\n",
      "train Loss: 0.0200 Acc: 99.5858\n",
      "test Loss: 0.0995 Acc: 98.2741\n",
      "\n",
      "Epoch 494/1199\n",
      "------------------------\n",
      "train Loss: 0.0203 Acc: 99.5835\n",
      "test Loss: 0.1002 Acc: 98.2544\n",
      "\n",
      "Epoch 495/1199\n",
      "------------------------\n",
      "train Loss: 0.0188 Acc: 99.6046\n",
      "test Loss: 0.1016 Acc: 98.2425\n",
      "\n",
      "Epoch 496/1199\n",
      "------------------------\n",
      "train Loss: 0.0205 Acc: 99.5820\n",
      "test Loss: 0.1021 Acc: 98.1988\n",
      "\n",
      "Epoch 497/1199\n",
      "------------------------\n",
      "train Loss: 0.0192 Acc: 99.6086\n",
      "test Loss: 0.1019 Acc: 98.2451\n",
      "\n",
      "Epoch 498/1199\n",
      "------------------------\n",
      "train Loss: 0.0196 Acc: 99.6090\n",
      "test Loss: 0.1036 Acc: 98.2443\n",
      "\n",
      "Epoch 499/1199\n",
      "------------------------\n",
      "train Loss: 0.0210 Acc: 99.5787\n",
      "test Loss: 0.1015 Acc: 98.2650\n",
      "\n",
      "Epoch 500/1199\n",
      "------------------------\n",
      "train Loss: 0.0192 Acc: 99.6105\n",
      "test Loss: 0.1043 Acc: 98.2465\n",
      "\n",
      "Epoch 501/1199\n",
      "------------------------\n",
      "train Loss: 0.0207 Acc: 99.5817\n",
      "test Loss: 0.1025 Acc: 98.2078\n",
      "\n",
      "Epoch 502/1199\n",
      "------------------------\n",
      "train Loss: 0.0184 Acc: 99.6278\n",
      "test Loss: 0.1001 Acc: 98.2774\n",
      "\n",
      "Epoch 503/1199\n",
      "------------------------\n",
      "train Loss: 0.0212 Acc: 99.5811\n",
      "test Loss: 0.1035 Acc: 98.2746\n",
      "\n",
      "Epoch 504/1199\n",
      "------------------------\n",
      "train Loss: 0.0209 Acc: 99.5793\n",
      "test Loss: 0.1028 Acc: 98.1603\n",
      "\n",
      "Epoch 505/1199\n",
      "------------------------\n",
      "train Loss: 0.0191 Acc: 99.6205\n",
      "test Loss: 0.1080 Acc: 98.0752\n",
      "\n",
      "Epoch 506/1199\n",
      "------------------------\n",
      "train Loss: 0.0210 Acc: 99.5876\n",
      "test Loss: 0.1043 Acc: 98.1759\n",
      "\n",
      "Epoch 507/1199\n",
      "------------------------\n",
      "train Loss: 0.0194 Acc: 99.6114\n",
      "test Loss: 0.1035 Acc: 98.2194\n",
      "\n",
      "Epoch 508/1199\n",
      "------------------------\n",
      "train Loss: 0.0195 Acc: 99.6069\n",
      "test Loss: 0.1031 Acc: 98.2106\n",
      "\n",
      "Epoch 509/1199\n",
      "------------------------\n",
      "train Loss: 0.0196 Acc: 99.6061\n",
      "test Loss: 0.1024 Acc: 98.1762\n",
      "\n",
      "Epoch 510/1199\n",
      "------------------------\n",
      "train Loss: 0.0198 Acc: 99.6006\n",
      "test Loss: 0.1019 Acc: 98.2318\n",
      "\n",
      "Epoch 511/1199\n",
      "------------------------\n",
      "train Loss: 0.0214 Acc: 99.5696\n",
      "test Loss: 0.1029 Acc: 98.2465\n",
      "\n",
      "Epoch 512/1199\n",
      "------------------------\n",
      "train Loss: 0.0219 Acc: 99.5714\n",
      "test Loss: 0.1013 Acc: 98.2287\n",
      "\n",
      "Epoch 513/1199\n",
      "------------------------\n",
      "train Loss: 0.0193 Acc: 99.6218\n",
      "test Loss: 0.1047 Acc: 98.2177\n",
      "\n",
      "Epoch 514/1199\n",
      "------------------------\n",
      "train Loss: 0.0205 Acc: 99.5852\n",
      "test Loss: 0.1024 Acc: 98.2154\n",
      "\n",
      "Epoch 515/1199\n",
      "------------------------\n",
      "train Loss: 0.0181 Acc: 99.6286\n",
      "test Loss: 0.1022 Acc: 98.2453\n",
      "\n",
      "Epoch 516/1199\n",
      "------------------------\n",
      "train Loss: 0.0205 Acc: 99.5866\n",
      "test Loss: 0.0998 Acc: 98.2605\n",
      "\n",
      "Epoch 517/1199\n",
      "------------------------\n",
      "train Loss: 0.0205 Acc: 99.5890\n",
      "test Loss: 0.1059 Acc: 98.2391\n",
      "\n",
      "Epoch 518/1199\n",
      "------------------------\n",
      "train Loss: 0.0190 Acc: 99.6143\n",
      "test Loss: 0.1043 Acc: 98.2565\n",
      "\n",
      "Epoch 519/1199\n",
      "------------------------\n",
      "train Loss: 0.0197 Acc: 99.6001\n",
      "test Loss: 0.1054 Acc: 98.2277\n",
      "\n",
      "Epoch 520/1199\n",
      "------------------------\n",
      "train Loss: 0.0188 Acc: 99.6173\n",
      "test Loss: 0.1053 Acc: 98.1935\n",
      "\n",
      "Epoch 521/1199\n",
      "------------------------\n",
      "train Loss: 0.0198 Acc: 99.6051\n",
      "test Loss: 0.1035 Acc: 98.2320\n",
      "\n",
      "Epoch 522/1199\n",
      "------------------------\n",
      "train Loss: 0.0194 Acc: 99.6079\n",
      "test Loss: 0.1073 Acc: 98.2142\n",
      "\n",
      "Epoch 523/1199\n",
      "------------------------\n",
      "train Loss: 0.0200 Acc: 99.5845\n",
      "test Loss: 0.1027 Acc: 98.2349\n",
      "\n",
      "Epoch 524/1199\n",
      "------------------------\n",
      "train Loss: 0.0194 Acc: 99.6008\n",
      "test Loss: 0.1027 Acc: 98.1779\n",
      "\n",
      "Epoch 525/1199\n",
      "------------------------\n",
      "train Loss: 0.0197 Acc: 99.6107\n",
      "test Loss: 0.1009 Acc: 98.2375\n",
      "\n",
      "Epoch 526/1199\n",
      "------------------------\n",
      "train Loss: 0.0185 Acc: 99.6217\n",
      "test Loss: 0.1021 Acc: 98.2418\n",
      "\n",
      "Epoch 527/1199\n",
      "------------------------\n",
      "train Loss: 0.0211 Acc: 99.5840\n",
      "test Loss: 0.1046 Acc: 98.2539\n",
      "\n",
      "Epoch 528/1199\n",
      "------------------------\n",
      "train Loss: 0.0175 Acc: 99.6407\n",
      "test Loss: 0.1031 Acc: 98.2330\n",
      "\n",
      "Epoch 529/1199\n",
      "------------------------\n",
      "train Loss: 0.0187 Acc: 99.6257\n",
      "test Loss: 0.1016 Acc: 98.2482\n",
      "\n",
      "Epoch 530/1199\n",
      "------------------------\n",
      "train Loss: 0.0198 Acc: 99.6082\n",
      "test Loss: 0.1055 Acc: 98.1342\n",
      "\n",
      "Epoch 531/1199\n",
      "------------------------\n",
      "train Loss: 0.0189 Acc: 99.6216\n",
      "test Loss: 0.1020 Acc: 98.2503\n",
      "\n",
      "Epoch 532/1199\n",
      "------------------------\n",
      "train Loss: 0.0190 Acc: 99.6148\n",
      "test Loss: 0.1045 Acc: 98.2121\n",
      "\n",
      "Epoch 533/1199\n",
      "------------------------\n",
      "train Loss: 0.0201 Acc: 99.6039\n",
      "test Loss: 0.1023 Acc: 98.2344\n",
      "\n",
      "Epoch 534/1199\n",
      "------------------------\n",
      "train Loss: 0.0199 Acc: 99.5996\n",
      "test Loss: 0.1033 Acc: 98.2594\n",
      "\n",
      "Epoch 535/1199\n",
      "------------------------\n",
      "train Loss: 0.0194 Acc: 99.6107\n",
      "test Loss: 0.1021 Acc: 98.2303\n",
      "\n",
      "Epoch 536/1199\n",
      "------------------------\n",
      "train Loss: 0.0180 Acc: 99.6374\n",
      "test Loss: 0.1012 Acc: 98.1987\n",
      "\n",
      "Epoch 537/1199\n",
      "------------------------\n",
      "train Loss: 0.0201 Acc: 99.6001\n",
      "test Loss: 0.1037 Acc: 98.2536\n",
      "\n",
      "Epoch 538/1199\n",
      "------------------------\n",
      "train Loss: 0.0177 Acc: 99.6398\n",
      "test Loss: 0.0997 Acc: 98.2729\n",
      "\n",
      "Epoch 539/1199\n",
      "------------------------\n",
      "train Loss: 0.0174 Acc: 99.6538\n",
      "test Loss: 0.1050 Acc: 98.1006\n",
      "\n",
      "Epoch 540/1199\n",
      "------------------------\n",
      "train Loss: 0.0184 Acc: 99.6250\n",
      "test Loss: 0.1073 Acc: 98.1933\n",
      "\n",
      "Epoch 541/1199\n",
      "------------------------\n",
      "train Loss: 0.0196 Acc: 99.6059\n",
      "test Loss: 0.1036 Acc: 98.2460\n",
      "\n",
      "Epoch 542/1199\n",
      "------------------------\n",
      "train Loss: 0.0190 Acc: 99.6188\n",
      "test Loss: 0.1046 Acc: 98.2049\n",
      "\n",
      "Epoch 543/1199\n",
      "------------------------\n",
      "train Loss: 0.0189 Acc: 99.6197\n",
      "test Loss: 0.1025 Acc: 98.2059\n",
      "\n",
      "Epoch 544/1199\n",
      "------------------------\n",
      "train Loss: 0.0187 Acc: 99.6343\n",
      "test Loss: 0.1010 Acc: 98.2159\n",
      "\n",
      "Epoch 545/1199\n",
      "------------------------\n",
      "train Loss: 0.0186 Acc: 99.6311\n",
      "test Loss: 0.1022 Acc: 98.2460\n",
      "\n",
      "Epoch 546/1199\n",
      "------------------------\n",
      "train Loss: 0.0194 Acc: 99.6162\n",
      "test Loss: 0.1060 Acc: 98.2158\n",
      "\n",
      "Epoch 547/1199\n",
      "------------------------\n",
      "train Loss: 0.0198 Acc: 99.6106\n",
      "test Loss: 0.1035 Acc: 98.2864\n",
      "\n",
      "Epoch 548/1199\n",
      "------------------------\n",
      "train Loss: 0.0184 Acc: 99.6365\n",
      "test Loss: 0.1016 Acc: 98.2173\n",
      "\n",
      "Epoch 549/1199\n",
      "------------------------\n",
      "train Loss: 0.0181 Acc: 99.6434\n",
      "test Loss: 0.1027 Acc: 98.2710\n",
      "\n",
      "Epoch 550/1199\n",
      "------------------------\n",
      "train Loss: 0.0190 Acc: 99.6195\n",
      "test Loss: 0.1050 Acc: 98.2056\n",
      "\n",
      "Epoch 551/1199\n",
      "------------------------\n",
      "train Loss: 0.0202 Acc: 99.6019\n",
      "test Loss: 0.1034 Acc: 98.2145\n",
      "\n",
      "Epoch 552/1199\n",
      "------------------------\n",
      "train Loss: 0.0181 Acc: 99.6401\n",
      "test Loss: 0.1005 Acc: 98.2859\n",
      "\n",
      "Epoch 553/1199\n",
      "------------------------\n",
      "train Loss: 0.0194 Acc: 99.6093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1034 Acc: 98.2434\n",
      "\n",
      "Epoch 554/1199\n",
      "------------------------\n",
      "train Loss: 0.0186 Acc: 99.6330\n",
      "test Loss: 0.1025 Acc: 98.1900\n",
      "\n",
      "Epoch 555/1199\n",
      "------------------------\n",
      "train Loss: 0.0175 Acc: 99.6488\n",
      "test Loss: 0.1027 Acc: 98.1957\n",
      "\n",
      "Epoch 556/1199\n",
      "------------------------\n",
      "train Loss: 0.0186 Acc: 99.6288\n",
      "test Loss: 0.1024 Acc: 98.2715\n",
      "\n",
      "Epoch 557/1199\n",
      "------------------------\n",
      "train Loss: 0.0180 Acc: 99.6422\n",
      "test Loss: 0.1039 Acc: 98.2313\n",
      "\n",
      "Epoch 558/1199\n",
      "------------------------\n",
      "train Loss: 0.0191 Acc: 99.6228\n",
      "test Loss: 0.1032 Acc: 98.2297\n",
      "\n",
      "Epoch 559/1199\n",
      "------------------------\n",
      "train Loss: 0.0180 Acc: 99.6338\n",
      "test Loss: 0.1067 Acc: 98.1886\n",
      "\n",
      "Epoch 560/1199\n",
      "------------------------\n",
      "train Loss: 0.0186 Acc: 99.6239\n",
      "test Loss: 0.1042 Acc: 98.2513\n",
      "\n",
      "Epoch 561/1199\n",
      "------------------------\n",
      "train Loss: 0.0175 Acc: 99.6418\n",
      "test Loss: 0.1018 Acc: 98.2627\n",
      "\n",
      "Epoch 562/1199\n",
      "------------------------\n",
      "train Loss: 0.0194 Acc: 99.6103\n",
      "test Loss: 0.1012 Acc: 98.2710\n",
      "\n",
      "Epoch 563/1199\n",
      "------------------------\n",
      "train Loss: 0.0191 Acc: 99.6239\n",
      "test Loss: 0.1063 Acc: 98.2740\n",
      "\n",
      "Epoch 564/1199\n",
      "------------------------\n",
      "train Loss: 0.0187 Acc: 99.6271\n",
      "test Loss: 0.1038 Acc: 98.2373\n",
      "\n",
      "Epoch 565/1199\n",
      "------------------------\n",
      "train Loss: 0.0173 Acc: 99.6573\n",
      "test Loss: 0.1045 Acc: 98.2916\n",
      "\n",
      "Epoch 566/1199\n",
      "------------------------\n",
      "train Loss: 0.0179 Acc: 99.6323\n",
      "test Loss: 0.1008 Acc: 98.2759\n",
      "\n",
      "Epoch 567/1199\n",
      "------------------------\n",
      "train Loss: 0.0183 Acc: 99.6380\n",
      "test Loss: 0.1035 Acc: 98.2297\n",
      "\n",
      "Epoch 568/1199\n",
      "------------------------\n",
      "train Loss: 0.0198 Acc: 99.6051\n",
      "test Loss: 0.1037 Acc: 98.2477\n",
      "\n",
      "Epoch 569/1199\n",
      "------------------------\n",
      "train Loss: 0.0171 Acc: 99.6529\n",
      "test Loss: 0.1001 Acc: 98.3399\n",
      "\n",
      "Epoch 570/1199\n",
      "------------------------\n",
      "train Loss: 0.0182 Acc: 99.6313\n",
      "test Loss: 0.1098 Acc: 98.1225\n",
      "\n",
      "Epoch 571/1199\n",
      "------------------------\n",
      "train Loss: 0.0191 Acc: 99.6151\n",
      "test Loss: 0.1012 Acc: 98.2943\n",
      "\n",
      "Epoch 572/1199\n",
      "------------------------\n",
      "train Loss: 0.0183 Acc: 99.6394\n",
      "test Loss: 0.1042 Acc: 98.2601\n",
      "\n",
      "Epoch 573/1199\n",
      "------------------------\n",
      "train Loss: 0.0188 Acc: 99.6212\n",
      "test Loss: 0.1032 Acc: 98.2242\n",
      "\n",
      "Epoch 574/1199\n",
      "------------------------\n",
      "train Loss: 0.0185 Acc: 99.6352\n",
      "test Loss: 0.1054 Acc: 98.1883\n",
      "\n",
      "Epoch 575/1199\n",
      "------------------------\n",
      "train Loss: 0.0201 Acc: 99.5943\n",
      "test Loss: 0.1051 Acc: 98.1569\n",
      "\n",
      "Epoch 576/1199\n",
      "------------------------\n",
      "train Loss: 0.0202 Acc: 99.5900\n",
      "test Loss: 0.1005 Acc: 98.2273\n",
      "\n",
      "Epoch 577/1199\n",
      "------------------------\n",
      "train Loss: 0.0238 Acc: 99.5025\n",
      "test Loss: 0.1035 Acc: 98.1686\n",
      "\n",
      "Epoch 578/1199\n",
      "------------------------\n",
      "train Loss: 0.0214 Acc: 99.5483\n",
      "test Loss: 0.1009 Acc: 98.1814\n",
      "\n",
      "Epoch 579/1199\n",
      "------------------------\n",
      "train Loss: 0.0228 Acc: 99.5267\n",
      "test Loss: 0.1068 Acc: 98.0833\n",
      "\n",
      "Epoch 580/1199\n",
      "------------------------\n",
      "train Loss: 0.0241 Acc: 99.4803\n",
      "test Loss: 0.1025 Acc: 98.1807\n",
      "\n",
      "Epoch 581/1199\n",
      "------------------------\n",
      "train Loss: 0.0201 Acc: 99.5599\n",
      "test Loss: 0.1019 Acc: 98.2006\n",
      "\n",
      "Epoch 582/1199\n",
      "------------------------\n",
      "train Loss: 0.0197 Acc: 99.5953\n",
      "test Loss: 0.1004 Acc: 98.2259\n",
      "\n",
      "Epoch 583/1199\n",
      "------------------------\n",
      "train Loss: 0.0188 Acc: 99.6090\n",
      "test Loss: 0.1030 Acc: 98.1574\n",
      "\n",
      "Epoch 584/1199\n",
      "------------------------\n",
      "train Loss: 0.0188 Acc: 99.6188\n",
      "test Loss: 0.1020 Acc: 98.2613\n",
      "\n",
      "Epoch 585/1199\n",
      "------------------------\n",
      "train Loss: 0.0181 Acc: 99.6308\n",
      "test Loss: 0.1006 Acc: 98.1945\n",
      "\n",
      "Epoch 586/1199\n",
      "------------------------\n",
      "train Loss: 0.0192 Acc: 99.6118\n",
      "test Loss: 0.0995 Acc: 98.2588\n",
      "\n",
      "Epoch 587/1199\n",
      "------------------------\n",
      "train Loss: 0.0190 Acc: 99.6149\n",
      "test Loss: 0.1007 Acc: 98.2427\n",
      "\n",
      "Epoch 588/1199\n",
      "------------------------\n",
      "train Loss: 0.0177 Acc: 99.6491\n",
      "test Loss: 0.1042 Acc: 98.1745\n",
      "\n",
      "Epoch 589/1199\n",
      "------------------------\n",
      "train Loss: 0.0182 Acc: 99.6412\n",
      "test Loss: 0.1037 Acc: 98.2021\n",
      "\n",
      "Epoch 590/1199\n",
      "------------------------\n",
      "train Loss: 0.0197 Acc: 99.6156\n",
      "test Loss: 0.1019 Acc: 98.2821\n",
      "\n",
      "Epoch 591/1199\n",
      "------------------------\n",
      "train Loss: 0.0178 Acc: 99.6463\n",
      "test Loss: 0.1034 Acc: 98.2434\n",
      "\n",
      "Epoch 592/1199\n",
      "------------------------\n",
      "train Loss: 0.0185 Acc: 99.6265\n",
      "test Loss: 0.1044 Acc: 98.2204\n",
      "\n",
      "Epoch 593/1199\n",
      "------------------------\n",
      "train Loss: 0.0190 Acc: 99.6175\n",
      "test Loss: 0.1025 Acc: 98.2094\n",
      "\n",
      "Epoch 594/1199\n",
      "------------------------\n",
      "train Loss: 0.0171 Acc: 99.6593\n",
      "test Loss: 0.1004 Acc: 98.2315\n",
      "\n",
      "Epoch 595/1199\n",
      "------------------------\n",
      "train Loss: 0.0181 Acc: 99.6339\n",
      "test Loss: 0.1043 Acc: 98.2455\n",
      "\n",
      "Epoch 596/1199\n",
      "------------------------\n",
      "train Loss: 0.0188 Acc: 99.6311\n",
      "test Loss: 0.1039 Acc: 98.2373\n",
      "\n",
      "Epoch 597/1199\n",
      "------------------------\n",
      "train Loss: 0.0173 Acc: 99.6599\n",
      "test Loss: 0.1016 Acc: 98.2132\n",
      "\n",
      "Epoch 598/1199\n",
      "------------------------\n",
      "train Loss: 0.0185 Acc: 99.6368\n",
      "test Loss: 0.1050 Acc: 98.1843\n",
      "\n",
      "Epoch 599/1199\n",
      "------------------------\n",
      "train Loss: 0.0180 Acc: 99.6318\n",
      "test Loss: 0.1040 Acc: 98.2277\n",
      "\n",
      "Epoch 600/1199\n",
      "------------------------\n",
      "train Loss: 0.0186 Acc: 99.6327\n",
      "test Loss: 0.1021 Acc: 98.2828\n",
      "\n",
      "Epoch 601/1199\n",
      "------------------------\n",
      "train Loss: 0.0189 Acc: 99.6325\n",
      "test Loss: 0.1047 Acc: 98.1961\n",
      "\n",
      "Epoch 602/1199\n",
      "------------------------\n",
      "train Loss: 0.0175 Acc: 99.6483\n",
      "test Loss: 0.1047 Acc: 98.1581\n",
      "\n",
      "Epoch 603/1199\n",
      "------------------------\n",
      "train Loss: 0.0170 Acc: 99.6694\n",
      "test Loss: 0.1015 Acc: 98.2453\n",
      "\n",
      "Epoch 604/1199\n",
      "------------------------\n",
      "train Loss: 0.0171 Acc: 99.6488\n",
      "test Loss: 0.1065 Acc: 98.2365\n",
      "\n",
      "Epoch 605/1199\n",
      "------------------------\n",
      "train Loss: 0.0171 Acc: 99.6725\n",
      "test Loss: 0.1033 Acc: 98.1760\n",
      "\n",
      "Epoch 606/1199\n",
      "------------------------\n",
      "train Loss: 0.0180 Acc: 99.6496\n",
      "test Loss: 0.1064 Acc: 98.2443\n",
      "\n",
      "Epoch 607/1199\n",
      "------------------------\n",
      "train Loss: 0.0173 Acc: 99.6680\n",
      "test Loss: 0.1011 Acc: 98.2897\n",
      "\n",
      "Epoch 608/1199\n",
      "------------------------\n",
      "train Loss: 0.0188 Acc: 99.6255\n",
      "test Loss: 0.1019 Acc: 98.2819\n",
      "\n",
      "Epoch 609/1199\n",
      "------------------------\n",
      "train Loss: 0.0182 Acc: 99.6380\n",
      "test Loss: 0.1027 Acc: 98.2268\n",
      "\n",
      "Epoch 610/1199\n",
      "------------------------\n",
      "train Loss: 0.0179 Acc: 99.6516\n",
      "test Loss: 0.1056 Acc: 98.2883\n",
      "\n",
      "Epoch 611/1199\n",
      "------------------------\n",
      "train Loss: 0.0190 Acc: 99.6289\n",
      "test Loss: 0.1032 Acc: 98.2684\n",
      "\n",
      "Epoch 612/1199\n",
      "------------------------\n",
      "train Loss: 0.0165 Acc: 99.6749\n",
      "test Loss: 0.0998 Acc: 98.2627\n",
      "\n",
      "Epoch 613/1199\n",
      "------------------------\n",
      "train Loss: 0.0178 Acc: 99.6524\n",
      "test Loss: 0.1004 Acc: 98.2930\n",
      "\n",
      "Epoch 614/1199\n",
      "------------------------\n",
      "train Loss: 0.0167 Acc: 99.6742\n",
      "test Loss: 0.1014 Acc: 98.2746\n",
      "\n",
      "Epoch 615/1199\n",
      "------------------------\n",
      "train Loss: 0.0186 Acc: 99.6396\n",
      "test Loss: 0.1002 Acc: 98.2474\n",
      "\n",
      "Epoch 616/1199\n",
      "------------------------\n",
      "train Loss: 0.0173 Acc: 99.6704\n",
      "test Loss: 0.1032 Acc: 98.2446\n",
      "\n",
      "Epoch 617/1199\n",
      "------------------------\n",
      "train Loss: 0.0175 Acc: 99.6588\n",
      "test Loss: 0.1027 Acc: 98.2836\n",
      "\n",
      "Epoch 618/1199\n",
      "------------------------\n",
      "train Loss: 0.0160 Acc: 99.6820\n",
      "test Loss: 0.1058 Acc: 98.2496\n",
      "\n",
      "Epoch 619/1199\n",
      "------------------------\n",
      "train Loss: 0.0177 Acc: 99.6580\n",
      "test Loss: 0.1016 Acc: 98.2584\n",
      "\n",
      "Epoch 620/1199\n",
      "------------------------\n",
      "train Loss: 0.0164 Acc: 99.6795\n",
      "test Loss: 0.1046 Acc: 98.2574\n",
      "\n",
      "Epoch 621/1199\n",
      "------------------------\n",
      "train Loss: 0.0184 Acc: 99.6424\n",
      "test Loss: 0.1020 Acc: 98.2907\n",
      "\n",
      "Epoch 622/1199\n",
      "------------------------\n",
      "train Loss: 0.0166 Acc: 99.6831\n",
      "test Loss: 0.1023 Acc: 98.2474\n",
      "\n",
      "Epoch 623/1199\n",
      "------------------------\n",
      "train Loss: 0.0166 Acc: 99.6715\n",
      "test Loss: 0.1020 Acc: 98.2993\n",
      "\n",
      "Epoch 624/1199\n",
      "------------------------\n",
      "train Loss: 0.0159 Acc: 99.6882\n",
      "test Loss: 0.1061 Acc: 98.2256\n",
      "\n",
      "Epoch 625/1199\n",
      "------------------------\n",
      "train Loss: 0.0178 Acc: 99.6525\n",
      "test Loss: 0.1021 Acc: 98.3075\n",
      "\n",
      "Epoch 626/1199\n",
      "------------------------\n",
      "train Loss: 0.0173 Acc: 99.6644\n",
      "test Loss: 0.1051 Acc: 98.1652\n",
      "\n",
      "Epoch 627/1199\n",
      "------------------------\n",
      "train Loss: 0.0171 Acc: 99.6561\n",
      "test Loss: 0.1010 Acc: 98.2532\n",
      "\n",
      "Epoch 628/1199\n",
      "------------------------\n",
      "train Loss: 0.0180 Acc: 99.6453\n",
      "test Loss: 0.1050 Acc: 98.2923\n",
      "\n",
      "Epoch 629/1199\n",
      "------------------------\n",
      "train Loss: 0.0173 Acc: 99.6644\n",
      "test Loss: 0.1019 Acc: 98.2931\n",
      "\n",
      "Epoch 630/1199\n",
      "------------------------\n",
      "train Loss: 0.0157 Acc: 99.6945\n",
      "test Loss: 0.1047 Acc: 98.2520\n",
      "\n",
      "Epoch 631/1199\n",
      "------------------------\n",
      "train Loss: 0.0167 Acc: 99.6641\n",
      "test Loss: 0.1027 Acc: 98.3235\n",
      "\n",
      "Epoch 632/1199\n",
      "------------------------\n",
      "train Loss: 0.0169 Acc: 99.6686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1040 Acc: 98.2919\n",
      "\n",
      "Epoch 633/1199\n",
      "------------------------\n",
      "train Loss: 0.0167 Acc: 99.6695\n",
      "test Loss: 0.1048 Acc: 98.2824\n",
      "\n",
      "Epoch 634/1199\n",
      "------------------------\n",
      "train Loss: 0.0171 Acc: 99.6678\n",
      "test Loss: 0.1040 Acc: 98.2529\n",
      "\n",
      "Epoch 635/1199\n",
      "------------------------\n",
      "train Loss: 0.0171 Acc: 99.6624\n",
      "test Loss: 0.1063 Acc: 98.1980\n",
      "\n",
      "Epoch 636/1199\n",
      "------------------------\n",
      "train Loss: 0.0182 Acc: 99.6391\n",
      "test Loss: 0.1004 Acc: 98.2645\n",
      "\n",
      "Epoch 637/1199\n",
      "------------------------\n",
      "train Loss: 0.0166 Acc: 99.6776\n",
      "test Loss: 0.1021 Acc: 98.2555\n",
      "\n",
      "Epoch 638/1199\n",
      "------------------------\n",
      "train Loss: 0.0176 Acc: 99.6554\n",
      "test Loss: 0.1019 Acc: 98.3094\n",
      "\n",
      "Epoch 639/1199\n",
      "------------------------\n",
      "train Loss: 0.0163 Acc: 99.6824\n",
      "test Loss: 0.1005 Acc: 98.2703\n",
      "\n",
      "Epoch 640/1199\n",
      "------------------------\n",
      "train Loss: 0.0184 Acc: 99.6453\n",
      "test Loss: 0.1031 Acc: 98.3076\n",
      "\n",
      "Epoch 641/1199\n",
      "------------------------\n",
      "train Loss: 0.0160 Acc: 99.6899\n",
      "test Loss: 0.1038 Acc: 98.2536\n",
      "\n",
      "Epoch 642/1199\n",
      "------------------------\n",
      "train Loss: 0.0153 Acc: 99.7005\n",
      "test Loss: 0.1034 Acc: 98.2434\n",
      "\n",
      "Epoch 643/1199\n",
      "------------------------\n",
      "train Loss: 0.0158 Acc: 99.6942\n",
      "test Loss: 0.1037 Acc: 98.2973\n",
      "\n",
      "Epoch 644/1199\n",
      "------------------------\n",
      "train Loss: 0.0173 Acc: 99.6609\n",
      "test Loss: 0.1044 Acc: 98.2337\n",
      "\n",
      "Epoch 645/1199\n",
      "------------------------\n",
      "train Loss: 0.0171 Acc: 99.6720\n",
      "test Loss: 0.1079 Acc: 98.2643\n",
      "\n",
      "Epoch 646/1199\n",
      "------------------------\n",
      "train Loss: 0.0166 Acc: 99.6756\n",
      "test Loss: 0.1041 Acc: 98.2919\n",
      "\n",
      "Epoch 647/1199\n",
      "------------------------\n",
      "train Loss: 0.0173 Acc: 99.6600\n",
      "test Loss: 0.1018 Acc: 98.2467\n",
      "\n",
      "Epoch 648/1199\n",
      "------------------------\n",
      "train Loss: 0.0170 Acc: 99.6649\n",
      "test Loss: 0.1029 Acc: 98.2506\n",
      "\n",
      "Epoch 649/1199\n",
      "------------------------\n",
      "train Loss: 0.0157 Acc: 99.6921\n",
      "test Loss: 0.1029 Acc: 98.2753\n",
      "\n",
      "Epoch 650/1199\n",
      "------------------------\n",
      "train Loss: 0.0163 Acc: 99.6792\n",
      "test Loss: 0.1036 Acc: 98.2358\n",
      "\n",
      "Epoch 651/1199\n",
      "------------------------\n",
      "train Loss: 0.0170 Acc: 99.6637\n",
      "test Loss: 0.1072 Acc: 98.2132\n",
      "\n",
      "Epoch 652/1199\n",
      "------------------------\n",
      "train Loss: 0.0175 Acc: 99.6579\n",
      "test Loss: 0.1055 Acc: 98.2334\n",
      "\n",
      "Epoch 653/1199\n",
      "------------------------\n",
      "train Loss: 0.0170 Acc: 99.6703\n",
      "test Loss: 0.1047 Acc: 98.3139\n",
      "\n",
      "Epoch 654/1199\n",
      "------------------------\n",
      "train Loss: 0.0165 Acc: 99.6770\n",
      "test Loss: 0.1033 Acc: 98.1874\n",
      "\n",
      "Epoch 655/1199\n",
      "------------------------\n",
      "train Loss: 0.0173 Acc: 99.6724\n",
      "test Loss: 0.1032 Acc: 98.3125\n",
      "\n",
      "Epoch 656/1199\n",
      "------------------------\n",
      "train Loss: 0.0164 Acc: 99.6872\n",
      "test Loss: 0.1042 Acc: 98.2743\n",
      "\n",
      "Epoch 657/1199\n",
      "------------------------\n",
      "train Loss: 0.0178 Acc: 99.6526\n",
      "test Loss: 0.1021 Acc: 98.2969\n",
      "\n",
      "Epoch 658/1199\n",
      "------------------------\n",
      "train Loss: 0.0171 Acc: 99.6705\n",
      "test Loss: 0.1053 Acc: 98.2242\n",
      "\n",
      "Epoch 659/1199\n",
      "------------------------\n",
      "train Loss: 0.0169 Acc: 99.6760\n",
      "test Loss: 0.1045 Acc: 98.2667\n",
      "\n",
      "Epoch 660/1199\n",
      "------------------------\n",
      "train Loss: 0.0161 Acc: 99.6819\n",
      "test Loss: 0.1059 Acc: 98.1793\n",
      "\n",
      "Epoch 661/1199\n",
      "------------------------\n",
      "train Loss: 0.0184 Acc: 99.6496\n",
      "test Loss: 0.1032 Acc: 98.2923\n",
      "\n",
      "Epoch 662/1199\n",
      "------------------------\n",
      "train Loss: 0.0157 Acc: 99.6981\n",
      "test Loss: 0.1024 Acc: 98.2864\n",
      "\n",
      "Epoch 663/1199\n",
      "------------------------\n",
      "train Loss: 0.0166 Acc: 99.6737\n",
      "test Loss: 0.1039 Acc: 98.2620\n",
      "\n",
      "Epoch 664/1199\n",
      "------------------------\n",
      "train Loss: 0.0158 Acc: 99.6877\n",
      "test Loss: 0.1011 Acc: 98.2702\n",
      "\n",
      "Epoch 665/1199\n",
      "------------------------\n",
      "train Loss: 0.0173 Acc: 99.6752\n",
      "test Loss: 0.1079 Acc: 98.2724\n",
      "\n",
      "Epoch 666/1199\n",
      "------------------------\n",
      "train Loss: 0.0157 Acc: 99.6932\n",
      "test Loss: 0.1049 Acc: 98.2335\n",
      "\n",
      "Epoch 667/1199\n",
      "------------------------\n",
      "train Loss: 0.0175 Acc: 99.6532\n",
      "test Loss: 0.1038 Acc: 98.2342\n",
      "\n",
      "Epoch 668/1199\n",
      "------------------------\n",
      "train Loss: 0.0165 Acc: 99.6855\n",
      "test Loss: 0.1027 Acc: 98.2721\n",
      "\n",
      "Epoch 669/1199\n",
      "------------------------\n",
      "train Loss: 0.0166 Acc: 99.6803\n",
      "test Loss: 0.1025 Acc: 98.2890\n",
      "\n",
      "Epoch 670/1199\n",
      "------------------------\n",
      "train Loss: 0.0153 Acc: 99.6957\n",
      "test Loss: 0.1019 Acc: 98.3035\n",
      "\n",
      "Epoch 671/1199\n",
      "------------------------\n",
      "train Loss: 0.0165 Acc: 99.6840\n",
      "test Loss: 0.1061 Acc: 98.2569\n",
      "\n",
      "Epoch 672/1199\n",
      "------------------------\n",
      "train Loss: 0.0166 Acc: 99.6745\n",
      "test Loss: 0.1021 Acc: 98.2822\n",
      "\n",
      "Epoch 673/1199\n",
      "------------------------\n",
      "train Loss: 0.0164 Acc: 99.6845\n",
      "test Loss: 0.1054 Acc: 98.2954\n",
      "\n",
      "Epoch 674/1199\n",
      "------------------------\n",
      "train Loss: 0.0152 Acc: 99.7058\n",
      "test Loss: 0.1052 Acc: 98.3063\n",
      "\n",
      "Epoch 675/1199\n",
      "------------------------\n",
      "train Loss: 0.0186 Acc: 99.6357\n",
      "test Loss: 0.1035 Acc: 98.2826\n",
      "\n",
      "Epoch 676/1199\n",
      "------------------------\n",
      "train Loss: 0.0166 Acc: 99.6809\n",
      "test Loss: 0.1045 Acc: 98.2272\n",
      "\n",
      "Epoch 677/1199\n",
      "------------------------\n",
      "train Loss: 0.0160 Acc: 99.6919\n",
      "test Loss: 0.1038 Acc: 98.3004\n",
      "\n",
      "Epoch 678/1199\n",
      "------------------------\n",
      "train Loss: 0.0168 Acc: 99.6766\n",
      "test Loss: 0.0994 Acc: 98.3189\n",
      "\n",
      "Epoch 679/1199\n",
      "------------------------\n",
      "train Loss: 0.0170 Acc: 99.6749\n",
      "test Loss: 0.1036 Acc: 98.2553\n",
      "\n",
      "Epoch 680/1199\n",
      "------------------------\n",
      "train Loss: 0.0157 Acc: 99.6906\n",
      "test Loss: 0.1034 Acc: 98.2474\n",
      "\n",
      "Epoch 681/1199\n",
      "------------------------\n",
      "train Loss: 0.0155 Acc: 99.6997\n",
      "test Loss: 0.1014 Acc: 98.2372\n",
      "\n",
      "Epoch 682/1199\n",
      "------------------------\n",
      "train Loss: 0.0159 Acc: 99.6894\n",
      "test Loss: 0.1029 Acc: 98.2463\n",
      "\n",
      "Epoch 683/1199\n",
      "------------------------\n",
      "train Loss: 0.0151 Acc: 99.7084\n",
      "test Loss: 0.1069 Acc: 98.1949\n",
      "\n",
      "Epoch 684/1199\n",
      "------------------------\n",
      "train Loss: 0.0166 Acc: 99.6787\n",
      "test Loss: 0.1039 Acc: 98.3021\n",
      "\n",
      "Epoch 685/1199\n",
      "------------------------\n",
      "train Loss: 0.0157 Acc: 99.7031\n",
      "test Loss: 0.1037 Acc: 98.3080\n",
      "\n",
      "Epoch 686/1199\n",
      "------------------------\n",
      "train Loss: 0.0162 Acc: 99.6860\n",
      "test Loss: 0.1048 Acc: 98.3014\n",
      "\n",
      "Epoch 687/1199\n",
      "------------------------\n",
      "train Loss: 0.0148 Acc: 99.7070\n",
      "test Loss: 0.1029 Acc: 98.2676\n",
      "\n",
      "Epoch 688/1199\n",
      "------------------------\n",
      "train Loss: 0.0169 Acc: 99.6671\n",
      "test Loss: 0.1061 Acc: 98.2303\n",
      "\n",
      "Epoch 689/1199\n",
      "------------------------\n",
      "train Loss: 0.0173 Acc: 99.6625\n",
      "test Loss: 0.1043 Acc: 98.2536\n",
      "\n",
      "Epoch 690/1199\n",
      "------------------------\n",
      "train Loss: 0.0165 Acc: 99.6739\n",
      "test Loss: 0.1046 Acc: 98.2318\n",
      "\n",
      "Epoch 691/1199\n",
      "------------------------\n",
      "train Loss: 0.0163 Acc: 99.6891\n",
      "test Loss: 0.1054 Acc: 98.2683\n",
      "\n",
      "Epoch 692/1199\n",
      "------------------------\n",
      "train Loss: 0.0171 Acc: 99.6760\n",
      "test Loss: 0.1045 Acc: 98.2669\n",
      "\n",
      "Epoch 693/1199\n",
      "------------------------\n",
      "train Loss: 0.0152 Acc: 99.7000\n",
      "test Loss: 0.1068 Acc: 98.2389\n",
      "\n",
      "Epoch 694/1199\n",
      "------------------------\n",
      "train Loss: 0.0154 Acc: 99.7005\n",
      "test Loss: 0.1015 Acc: 98.2962\n",
      "\n",
      "Epoch 695/1199\n",
      "------------------------\n",
      "train Loss: 0.0162 Acc: 99.6762\n",
      "test Loss: 0.1063 Acc: 98.2398\n",
      "\n",
      "Epoch 696/1199\n",
      "------------------------\n",
      "train Loss: 0.0157 Acc: 99.6885\n",
      "test Loss: 0.1049 Acc: 98.2204\n",
      "\n",
      "Epoch 697/1199\n",
      "------------------------\n",
      "train Loss: 0.0174 Acc: 99.6476\n",
      "test Loss: 0.1035 Acc: 98.2914\n",
      "\n",
      "Epoch 698/1199\n",
      "------------------------\n",
      "train Loss: 0.0176 Acc: 99.6526\n",
      "test Loss: 0.1084 Acc: 98.2256\n",
      "\n",
      "Epoch 699/1199\n",
      "------------------------\n",
      "train Loss: 0.0159 Acc: 99.6902\n",
      "test Loss: 0.1040 Acc: 98.2608\n",
      "\n",
      "Epoch 700/1199\n",
      "------------------------\n",
      "train Loss: 0.0170 Acc: 99.6678\n",
      "test Loss: 0.1050 Acc: 98.3004\n",
      "\n",
      "Epoch 701/1199\n",
      "------------------------\n",
      "train Loss: 0.0165 Acc: 99.6847\n",
      "test Loss: 0.1044 Acc: 98.2556\n",
      "\n",
      "Epoch 702/1199\n",
      "------------------------\n",
      "train Loss: 0.0180 Acc: 99.6455\n",
      "test Loss: 0.1040 Acc: 98.2512\n",
      "\n",
      "Epoch 703/1199\n",
      "------------------------\n",
      "train Loss: 0.0156 Acc: 99.6870\n",
      "test Loss: 0.1019 Acc: 98.2995\n",
      "\n",
      "Epoch 704/1199\n",
      "------------------------\n",
      "train Loss: 0.0155 Acc: 99.6947\n",
      "test Loss: 0.1032 Acc: 98.3071\n",
      "\n",
      "Epoch 705/1199\n",
      "------------------------\n",
      "train Loss: 0.0170 Acc: 99.6773\n",
      "test Loss: 0.1069 Acc: 98.2924\n",
      "\n",
      "Epoch 706/1199\n",
      "------------------------\n",
      "train Loss: 0.0165 Acc: 99.6856\n",
      "test Loss: 0.1026 Acc: 98.2187\n",
      "\n",
      "Epoch 707/1199\n",
      "------------------------\n",
      "train Loss: 0.0171 Acc: 99.6755\n",
      "test Loss: 0.1033 Acc: 98.2874\n",
      "\n",
      "Epoch 708/1199\n",
      "------------------------\n",
      "train Loss: 0.0159 Acc: 99.7022\n",
      "test Loss: 0.1027 Acc: 98.3071\n",
      "\n",
      "Epoch 709/1199\n",
      "------------------------\n",
      "train Loss: 0.0153 Acc: 99.7133\n",
      "test Loss: 0.1018 Acc: 98.2708\n",
      "\n",
      "Epoch 710/1199\n",
      "------------------------\n",
      "train Loss: 0.0169 Acc: 99.6619\n",
      "test Loss: 0.1069 Acc: 98.2432\n",
      "\n",
      "Epoch 711/1199\n",
      "------------------------\n",
      "train Loss: 0.0168 Acc: 99.6810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1023 Acc: 98.2234\n",
      "\n",
      "Epoch 712/1199\n",
      "------------------------\n",
      "train Loss: 0.0153 Acc: 99.7017\n",
      "test Loss: 0.1037 Acc: 98.2962\n",
      "\n",
      "Epoch 713/1199\n",
      "------------------------\n",
      "train Loss: 0.0159 Acc: 99.7000\n",
      "test Loss: 0.1025 Acc: 98.3094\n",
      "\n",
      "Epoch 714/1199\n",
      "------------------------\n",
      "train Loss: 0.0167 Acc: 99.6624\n",
      "test Loss: 0.1047 Acc: 98.2772\n",
      "\n",
      "Epoch 715/1199\n",
      "------------------------\n",
      "train Loss: 0.0167 Acc: 99.6743\n",
      "test Loss: 0.1053 Acc: 98.2874\n",
      "\n",
      "Epoch 716/1199\n",
      "------------------------\n",
      "train Loss: 0.0154 Acc: 99.6964\n",
      "test Loss: 0.1009 Acc: 98.3002\n",
      "\n",
      "Epoch 717/1199\n",
      "------------------------\n",
      "train Loss: 0.0163 Acc: 99.6884\n",
      "test Loss: 0.1078 Acc: 98.3080\n",
      "\n",
      "Epoch 718/1199\n",
      "------------------------\n",
      "train Loss: 0.0154 Acc: 99.6980\n",
      "test Loss: 0.1067 Acc: 98.2377\n",
      "\n",
      "Epoch 719/1199\n",
      "------------------------\n",
      "train Loss: 0.0160 Acc: 99.6906\n",
      "test Loss: 0.1062 Acc: 98.2959\n",
      "\n",
      "Epoch 720/1199\n",
      "------------------------\n",
      "train Loss: 0.0146 Acc: 99.7235\n",
      "test Loss: 0.1061 Acc: 98.2120\n",
      "\n",
      "Epoch 721/1199\n",
      "------------------------\n",
      "train Loss: 0.0156 Acc: 99.6923\n",
      "test Loss: 0.1060 Acc: 98.2881\n",
      "\n",
      "Epoch 722/1199\n",
      "------------------------\n",
      "train Loss: 0.0157 Acc: 99.6881\n",
      "test Loss: 0.1023 Acc: 98.2950\n",
      "\n",
      "Epoch 723/1199\n",
      "------------------------\n",
      "train Loss: 0.0168 Acc: 99.6770\n",
      "test Loss: 0.1049 Acc: 98.3170\n",
      "\n",
      "Epoch 724/1199\n",
      "------------------------\n",
      "train Loss: 0.0161 Acc: 99.6930\n",
      "test Loss: 0.1033 Acc: 98.2848\n",
      "\n",
      "Epoch 725/1199\n",
      "------------------------\n",
      "train Loss: 0.0170 Acc: 99.6808\n",
      "test Loss: 0.1022 Acc: 98.3085\n",
      "\n",
      "Epoch 726/1199\n",
      "------------------------\n",
      "train Loss: 0.0158 Acc: 99.6906\n",
      "test Loss: 0.1057 Acc: 98.2764\n",
      "\n",
      "Epoch 727/1199\n",
      "------------------------\n",
      "train Loss: 0.0160 Acc: 99.6877\n",
      "test Loss: 0.1038 Acc: 98.2617\n",
      "\n",
      "Epoch 728/1199\n",
      "------------------------\n",
      "train Loss: 0.0166 Acc: 99.6820\n",
      "test Loss: 0.1070 Acc: 98.2904\n",
      "\n",
      "Epoch 729/1199\n",
      "------------------------\n",
      "train Loss: 0.0154 Acc: 99.7037\n",
      "test Loss: 0.1063 Acc: 98.2107\n",
      "\n",
      "Epoch 730/1199\n",
      "------------------------\n",
      "train Loss: 0.0158 Acc: 99.6814\n",
      "test Loss: 0.1074 Acc: 98.3135\n",
      "\n",
      "Epoch 731/1199\n",
      "------------------------\n",
      "train Loss: 0.0158 Acc: 99.6959\n",
      "test Loss: 0.1057 Acc: 98.3164\n",
      "\n",
      "Epoch 732/1199\n",
      "------------------------\n",
      "train Loss: 0.0169 Acc: 99.6730\n",
      "test Loss: 0.1058 Acc: 98.3289\n",
      "\n",
      "Epoch 733/1199\n",
      "------------------------\n",
      "train Loss: 0.0148 Acc: 99.7119\n",
      "test Loss: 0.1020 Acc: 98.2422\n",
      "\n",
      "Epoch 734/1199\n",
      "------------------------\n",
      "train Loss: 0.0152 Acc: 99.7079\n",
      "test Loss: 0.1052 Acc: 98.2715\n",
      "\n",
      "Epoch 735/1199\n",
      "------------------------\n",
      "train Loss: 0.0150 Acc: 99.7166\n",
      "test Loss: 0.1056 Acc: 98.2615\n",
      "\n",
      "Epoch 736/1199\n",
      "------------------------\n",
      "train Loss: 0.0165 Acc: 99.6756\n",
      "test Loss: 0.1042 Acc: 98.2708\n",
      "\n",
      "Epoch 737/1199\n",
      "------------------------\n",
      "train Loss: 0.0162 Acc: 99.6901\n",
      "test Loss: 0.1047 Acc: 98.3019\n",
      "\n",
      "Epoch 738/1199\n",
      "------------------------\n",
      "train Loss: 0.0156 Acc: 99.7058\n",
      "test Loss: 0.1046 Acc: 98.2387\n",
      "\n",
      "Epoch 739/1199\n",
      "------------------------\n",
      "train Loss: 0.0164 Acc: 99.6825\n",
      "test Loss: 0.1041 Acc: 98.2665\n",
      "\n",
      "Epoch 740/1199\n",
      "------------------------\n",
      "train Loss: 0.0157 Acc: 99.6928\n",
      "test Loss: 0.1039 Acc: 98.2517\n",
      "\n",
      "Epoch 741/1199\n",
      "------------------------\n",
      "train Loss: 0.0155 Acc: 99.6977\n",
      "test Loss: 0.1023 Acc: 98.2816\n",
      "\n",
      "Epoch 742/1199\n",
      "------------------------\n",
      "train Loss: 0.0173 Acc: 99.6662\n",
      "test Loss: 0.1020 Acc: 98.2330\n",
      "\n",
      "Epoch 743/1199\n",
      "------------------------\n",
      "train Loss: 0.0151 Acc: 99.7078\n",
      "test Loss: 0.1021 Acc: 98.2626\n",
      "\n",
      "Epoch 744/1199\n",
      "------------------------\n",
      "train Loss: 0.0150 Acc: 99.7117\n",
      "test Loss: 0.1071 Acc: 98.2534\n",
      "\n",
      "Epoch 745/1199\n",
      "------------------------\n",
      "train Loss: 0.0149 Acc: 99.7103\n",
      "test Loss: 0.1044 Acc: 98.2802\n",
      "\n",
      "Epoch 746/1199\n",
      "------------------------\n",
      "train Loss: 0.0161 Acc: 99.6937\n",
      "test Loss: 0.1043 Acc: 98.2898\n",
      "\n",
      "Epoch 747/1199\n",
      "------------------------\n",
      "train Loss: 0.0148 Acc: 99.7111\n",
      "test Loss: 0.1046 Acc: 98.3111\n",
      "\n",
      "Epoch 748/1199\n",
      "------------------------\n",
      "train Loss: 0.0163 Acc: 99.6826\n",
      "test Loss: 0.1054 Acc: 98.2930\n",
      "\n",
      "Epoch 749/1199\n",
      "------------------------\n",
      "train Loss: 0.0165 Acc: 99.6853\n",
      "test Loss: 0.1058 Acc: 98.2833\n",
      "\n",
      "Epoch 750/1199\n",
      "------------------------\n",
      "train Loss: 0.0150 Acc: 99.7039\n",
      "test Loss: 0.1047 Acc: 98.3130\n",
      "\n",
      "Epoch 751/1199\n",
      "------------------------\n",
      "train Loss: 0.0146 Acc: 99.7207\n",
      "test Loss: 0.1032 Acc: 98.3349\n",
      "\n",
      "Epoch 752/1199\n",
      "------------------------\n",
      "train Loss: 0.0155 Acc: 99.6975\n",
      "test Loss: 0.1040 Acc: 98.3365\n",
      "\n",
      "Epoch 753/1199\n",
      "------------------------\n",
      "train Loss: 0.0150 Acc: 99.7099\n",
      "test Loss: 0.1061 Acc: 98.2688\n",
      "\n",
      "Epoch 754/1199\n",
      "------------------------\n",
      "train Loss: 0.0147 Acc: 99.7167\n",
      "test Loss: 0.1063 Acc: 98.2752\n",
      "\n",
      "Epoch 755/1199\n",
      "------------------------\n",
      "train Loss: 0.0156 Acc: 99.6978\n",
      "test Loss: 0.1037 Acc: 98.2871\n",
      "\n",
      "Epoch 756/1199\n",
      "------------------------\n",
      "train Loss: 0.0153 Acc: 99.7087\n",
      "test Loss: 0.1058 Acc: 98.2983\n",
      "\n",
      "Epoch 757/1199\n",
      "------------------------\n",
      "train Loss: 0.0161 Acc: 99.6915\n",
      "test Loss: 0.1037 Acc: 98.2729\n",
      "\n",
      "Epoch 758/1199\n",
      "------------------------\n",
      "train Loss: 0.0141 Acc: 99.7336\n",
      "test Loss: 0.1052 Acc: 98.2755\n",
      "\n",
      "Epoch 759/1199\n",
      "------------------------\n",
      "train Loss: 0.0143 Acc: 99.7273\n",
      "test Loss: 0.1081 Acc: 98.2325\n",
      "\n",
      "Epoch 760/1199\n",
      "------------------------\n",
      "train Loss: 0.0147 Acc: 99.7139\n",
      "test Loss: 0.1069 Acc: 98.3038\n",
      "\n",
      "Epoch 761/1199\n",
      "------------------------\n",
      "train Loss: 0.0163 Acc: 99.6957\n",
      "test Loss: 0.1054 Acc: 98.2386\n",
      "\n",
      "Epoch 762/1199\n",
      "------------------------\n",
      "train Loss: 0.0162 Acc: 99.6908\n",
      "test Loss: 0.1037 Acc: 98.3202\n",
      "\n",
      "Epoch 763/1199\n",
      "------------------------\n",
      "train Loss: 0.0156 Acc: 99.7049\n",
      "test Loss: 0.1061 Acc: 98.3035\n",
      "\n",
      "Epoch 764/1199\n",
      "------------------------\n",
      "train Loss: 0.0147 Acc: 99.7154\n",
      "test Loss: 0.1068 Acc: 98.2743\n",
      "\n",
      "Epoch 765/1199\n",
      "------------------------\n",
      "train Loss: 0.0153 Acc: 99.7112\n",
      "test Loss: 0.1066 Acc: 98.3097\n",
      "\n",
      "Epoch 766/1199\n",
      "------------------------\n",
      "train Loss: 0.0152 Acc: 99.7044\n",
      "test Loss: 0.1071 Acc: 98.2907\n",
      "\n",
      "Epoch 767/1199\n",
      "------------------------\n",
      "train Loss: 0.0147 Acc: 99.7208\n",
      "test Loss: 0.1052 Acc: 98.3246\n",
      "\n",
      "Epoch 768/1199\n",
      "------------------------\n",
      "train Loss: 0.0149 Acc: 99.7164\n",
      "test Loss: 0.1017 Acc: 98.3489\n",
      "\n",
      "Epoch 769/1199\n",
      "------------------------\n",
      "train Loss: 0.0156 Acc: 99.6978\n",
      "test Loss: 0.1050 Acc: 98.2990\n",
      "\n",
      "Epoch 770/1199\n",
      "------------------------\n",
      "train Loss: 0.0143 Acc: 99.7223\n",
      "test Loss: 0.1023 Acc: 98.3038\n",
      "\n",
      "Epoch 771/1199\n",
      "------------------------\n",
      "train Loss: 0.0159 Acc: 99.6975\n",
      "test Loss: 0.1039 Acc: 98.3266\n",
      "\n",
      "Epoch 772/1199\n",
      "------------------------\n",
      "train Loss: 0.0139 Acc: 99.7278\n",
      "test Loss: 0.1049 Acc: 98.3515\n",
      "\n",
      "Epoch 773/1199\n",
      "------------------------\n",
      "train Loss: 0.0167 Acc: 99.6793\n",
      "test Loss: 0.1059 Acc: 98.3206\n",
      "\n",
      "Epoch 774/1199\n",
      "------------------------\n",
      "train Loss: 0.0155 Acc: 99.7054\n",
      "test Loss: 0.1081 Acc: 98.2558\n",
      "\n",
      "Epoch 775/1199\n",
      "------------------------\n",
      "train Loss: 0.0150 Acc: 99.7085\n",
      "test Loss: 0.1045 Acc: 98.2999\n",
      "\n",
      "Epoch 776/1199\n",
      "------------------------\n",
      "train Loss: 0.0156 Acc: 99.6956\n",
      "test Loss: 0.1033 Acc: 98.3303\n",
      "\n",
      "Epoch 777/1199\n",
      "------------------------\n",
      "train Loss: 0.0162 Acc: 99.6796\n",
      "test Loss: 0.1050 Acc: 98.3185\n",
      "\n",
      "Epoch 778/1199\n",
      "------------------------\n",
      "train Loss: 0.0148 Acc: 99.7128\n",
      "test Loss: 0.1075 Acc: 98.2429\n",
      "\n",
      "Epoch 779/1199\n",
      "------------------------\n",
      "train Loss: 0.0155 Acc: 99.7021\n",
      "test Loss: 0.1050 Acc: 98.2821\n",
      "\n",
      "Epoch 780/1199\n",
      "------------------------\n",
      "train Loss: 0.0153 Acc: 99.7076\n",
      "test Loss: 0.1052 Acc: 98.2608\n",
      "\n",
      "Epoch 781/1199\n",
      "------------------------\n",
      "train Loss: 0.0188 Acc: 99.6295\n",
      "test Loss: 0.1075 Acc: 98.2356\n",
      "\n",
      "Epoch 782/1199\n",
      "------------------------\n",
      "train Loss: 0.0168 Acc: 99.6682\n",
      "test Loss: 0.1050 Acc: 98.2741\n",
      "\n",
      "Epoch 783/1199\n",
      "------------------------\n",
      "train Loss: 0.0163 Acc: 99.6871\n",
      "test Loss: 0.1036 Acc: 98.3197\n",
      "\n",
      "Epoch 784/1199\n",
      "------------------------\n",
      "train Loss: 0.0156 Acc: 99.7008\n",
      "test Loss: 0.1071 Acc: 98.2449\n",
      "\n",
      "Epoch 785/1199\n",
      "------------------------\n",
      "train Loss: 0.0149 Acc: 99.7074\n",
      "test Loss: 0.1057 Acc: 98.2814\n",
      "\n",
      "Epoch 786/1199\n",
      "------------------------\n",
      "train Loss: 0.0146 Acc: 99.7216\n",
      "test Loss: 0.1053 Acc: 98.2993\n",
      "\n",
      "Epoch 787/1199\n",
      "------------------------\n",
      "train Loss: 0.0150 Acc: 99.7027\n",
      "test Loss: 0.1037 Acc: 98.3154\n",
      "\n",
      "Epoch 788/1199\n",
      "------------------------\n",
      "train Loss: 0.0154 Acc: 99.6965\n",
      "test Loss: 0.1052 Acc: 98.2945\n",
      "\n",
      "Epoch 789/1199\n",
      "------------------------\n",
      "train Loss: 0.0137 Acc: 99.7359\n",
      "test Loss: 0.1064 Acc: 98.3085\n",
      "\n",
      "Epoch 790/1199\n",
      "------------------------\n",
      "train Loss: 0.0166 Acc: 99.6855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1036 Acc: 98.2949\n",
      "\n",
      "Epoch 791/1199\n",
      "------------------------\n",
      "train Loss: 0.0154 Acc: 99.7010\n",
      "test Loss: 0.1044 Acc: 98.3164\n",
      "\n",
      "Epoch 792/1199\n",
      "------------------------\n",
      "train Loss: 0.0152 Acc: 99.7113\n",
      "test Loss: 0.1045 Acc: 98.2919\n",
      "\n",
      "Epoch 793/1199\n",
      "------------------------\n",
      "train Loss: 0.0164 Acc: 99.6913\n",
      "test Loss: 0.1051 Acc: 98.2812\n",
      "\n",
      "Epoch 794/1199\n",
      "------------------------\n",
      "train Loss: 0.0149 Acc: 99.7152\n",
      "test Loss: 0.1044 Acc: 98.2778\n",
      "\n",
      "Epoch 795/1199\n",
      "------------------------\n",
      "train Loss: 0.0144 Acc: 99.7193\n",
      "test Loss: 0.1054 Acc: 98.2588\n",
      "\n",
      "Epoch 796/1199\n",
      "------------------------\n",
      "train Loss: 0.0140 Acc: 99.7303\n",
      "test Loss: 0.1061 Acc: 98.2936\n",
      "\n",
      "Epoch 797/1199\n",
      "------------------------\n",
      "train Loss: 0.0160 Acc: 99.6987\n",
      "test Loss: 0.1073 Acc: 98.2529\n",
      "\n",
      "Epoch 798/1199\n",
      "------------------------\n",
      "train Loss: 0.0158 Acc: 99.6918\n",
      "test Loss: 0.1100 Acc: 98.2831\n",
      "\n",
      "Epoch 799/1199\n",
      "------------------------\n",
      "train Loss: 0.0153 Acc: 99.7028\n",
      "test Loss: 0.1097 Acc: 98.2695\n",
      "\n",
      "Epoch 800/1199\n",
      "------------------------\n",
      "train Loss: 0.0166 Acc: 99.6715\n",
      "test Loss: 0.1089 Acc: 98.2624\n",
      "\n",
      "Epoch 801/1199\n",
      "------------------------\n",
      "train Loss: 0.0185 Acc: 99.6416\n",
      "test Loss: 0.1048 Acc: 98.2280\n",
      "\n",
      "Epoch 802/1199\n",
      "------------------------\n",
      "train Loss: 0.0193 Acc: 99.6045\n",
      "test Loss: 0.1136 Acc: 98.1980\n",
      "\n",
      "Epoch 803/1199\n",
      "------------------------\n",
      "train Loss: 0.0205 Acc: 99.5709\n",
      "test Loss: 0.1029 Acc: 98.2130\n",
      "\n",
      "Epoch 804/1199\n",
      "------------------------\n",
      "train Loss: 0.0216 Acc: 99.5453\n",
      "test Loss: 0.1047 Acc: 98.2757\n",
      "\n",
      "Epoch 805/1199\n",
      "------------------------\n",
      "train Loss: 0.0162 Acc: 99.6691\n",
      "test Loss: 0.1019 Acc: 98.2330\n",
      "\n",
      "Epoch 806/1199\n",
      "------------------------\n",
      "train Loss: 0.0160 Acc: 99.6721\n",
      "test Loss: 0.1062 Acc: 98.2033\n",
      "\n",
      "Epoch 807/1199\n",
      "------------------------\n",
      "train Loss: 0.0154 Acc: 99.6950\n",
      "test Loss: 0.1049 Acc: 98.2253\n",
      "\n",
      "Epoch 808/1199\n",
      "------------------------\n",
      "train Loss: 0.0165 Acc: 99.6700\n",
      "test Loss: 0.1062 Acc: 98.3097\n",
      "\n",
      "Epoch 809/1199\n",
      "------------------------\n",
      "train Loss: 0.0155 Acc: 99.6971\n",
      "test Loss: 0.1057 Acc: 98.2892\n",
      "\n",
      "Epoch 810/1199\n",
      "------------------------\n",
      "train Loss: 0.0146 Acc: 99.7133\n",
      "test Loss: 0.1018 Acc: 98.3399\n",
      "\n",
      "Epoch 811/1199\n",
      "------------------------\n",
      "train Loss: 0.0153 Acc: 99.7042\n",
      "test Loss: 0.1052 Acc: 98.2917\n",
      "\n",
      "Epoch 812/1199\n",
      "------------------------\n",
      "train Loss: 0.0145 Acc: 99.7218\n",
      "test Loss: 0.1063 Acc: 98.2980\n",
      "\n",
      "Epoch 813/1199\n",
      "------------------------\n",
      "train Loss: 0.0140 Acc: 99.7247\n",
      "test Loss: 0.1058 Acc: 98.2938\n",
      "\n",
      "Epoch 814/1199\n",
      "------------------------\n",
      "train Loss: 0.0156 Acc: 99.6977\n",
      "test Loss: 0.1053 Acc: 98.2983\n",
      "\n",
      "Epoch 815/1199\n",
      "------------------------\n",
      "train Loss: 0.0145 Acc: 99.7195\n",
      "test Loss: 0.1022 Acc: 98.3102\n",
      "\n",
      "Epoch 816/1199\n",
      "------------------------\n",
      "train Loss: 0.0148 Acc: 99.7061\n",
      "test Loss: 0.0999 Acc: 98.3328\n",
      "\n",
      "Epoch 817/1199\n",
      "------------------------\n",
      "train Loss: 0.0158 Acc: 99.7024\n",
      "test Loss: 0.1009 Acc: 98.2852\n",
      "\n",
      "Epoch 818/1199\n",
      "------------------------\n",
      "train Loss: 0.0141 Acc: 99.7248\n",
      "test Loss: 0.1048 Acc: 98.3021\n",
      "\n",
      "Epoch 819/1199\n",
      "------------------------\n",
      "train Loss: 0.0152 Acc: 99.7147\n",
      "test Loss: 0.1018 Acc: 98.2886\n",
      "\n",
      "Epoch 820/1199\n",
      "------------------------\n",
      "train Loss: 0.0155 Acc: 99.6993\n",
      "test Loss: 0.1010 Acc: 98.3220\n",
      "\n",
      "Epoch 821/1199\n",
      "------------------------\n",
      "train Loss: 0.0154 Acc: 99.7033\n",
      "test Loss: 0.1051 Acc: 98.3253\n",
      "\n",
      "Epoch 822/1199\n",
      "------------------------\n",
      "train Loss: 0.0146 Acc: 99.7171\n",
      "test Loss: 0.1034 Acc: 98.3073\n",
      "\n",
      "Epoch 823/1199\n",
      "------------------------\n",
      "train Loss: 0.0148 Acc: 99.7178\n",
      "test Loss: 0.1053 Acc: 98.3171\n",
      "\n",
      "Epoch 824/1199\n",
      "------------------------\n",
      "train Loss: 0.0146 Acc: 99.7205\n",
      "test Loss: 0.1026 Acc: 98.2973\n",
      "\n",
      "Epoch 825/1199\n",
      "------------------------\n",
      "train Loss: 0.0146 Acc: 99.7253\n",
      "test Loss: 0.1044 Acc: 98.3259\n",
      "\n",
      "Epoch 826/1199\n",
      "------------------------\n",
      "train Loss: 0.0149 Acc: 99.7235\n",
      "test Loss: 0.1037 Acc: 98.3031\n",
      "\n",
      "Epoch 827/1199\n",
      "------------------------\n",
      "train Loss: 0.0149 Acc: 99.7130\n",
      "test Loss: 0.1038 Acc: 98.3196\n",
      "\n",
      "Epoch 828/1199\n",
      "------------------------\n",
      "train Loss: 0.0138 Acc: 99.7364\n",
      "test Loss: 0.1048 Acc: 98.3215\n",
      "\n",
      "Epoch 829/1199\n",
      "------------------------\n",
      "train Loss: 0.0142 Acc: 99.7301\n",
      "test Loss: 0.1072 Acc: 98.2613\n",
      "\n",
      "Epoch 830/1199\n",
      "------------------------\n",
      "train Loss: 0.0149 Acc: 99.7121\n",
      "test Loss: 0.1057 Acc: 98.3059\n",
      "\n",
      "Epoch 831/1199\n",
      "------------------------\n",
      "train Loss: 0.0163 Acc: 99.6969\n",
      "test Loss: 0.1079 Acc: 98.2836\n",
      "\n",
      "Epoch 832/1199\n",
      "------------------------\n",
      "train Loss: 0.0147 Acc: 99.7261\n",
      "test Loss: 0.1023 Acc: 98.3009\n",
      "\n",
      "Epoch 833/1199\n",
      "------------------------\n",
      "train Loss: 0.0144 Acc: 99.7259\n",
      "test Loss: 0.1050 Acc: 98.2805\n",
      "\n",
      "Epoch 834/1199\n",
      "------------------------\n",
      "train Loss: 0.0165 Acc: 99.7016\n",
      "test Loss: 0.1051 Acc: 98.3392\n",
      "\n",
      "Epoch 835/1199\n",
      "------------------------\n",
      "train Loss: 0.0149 Acc: 99.7209\n",
      "test Loss: 0.1043 Acc: 98.2879\n",
      "\n",
      "Epoch 836/1199\n",
      "------------------------\n",
      "train Loss: 0.0149 Acc: 99.7137\n",
      "test Loss: 0.1042 Acc: 98.3009\n",
      "\n",
      "Epoch 837/1199\n",
      "------------------------\n",
      "train Loss: 0.0137 Acc: 99.7355\n",
      "test Loss: 0.1065 Acc: 98.2575\n",
      "\n",
      "Epoch 838/1199\n",
      "------------------------\n",
      "train Loss: 0.0138 Acc: 99.7424\n",
      "test Loss: 0.1033 Acc: 98.2917\n",
      "\n",
      "Epoch 839/1199\n",
      "------------------------\n",
      "train Loss: 0.0160 Acc: 99.7035\n",
      "test Loss: 0.1067 Acc: 98.3463\n",
      "\n",
      "Epoch 840/1199\n",
      "------------------------\n",
      "train Loss: 0.0152 Acc: 99.7095\n",
      "test Loss: 0.1046 Acc: 98.2971\n",
      "\n",
      "Epoch 841/1199\n",
      "------------------------\n",
      "train Loss: 0.0161 Acc: 99.6860\n",
      "test Loss: 0.1036 Acc: 98.3294\n",
      "\n",
      "Epoch 842/1199\n",
      "------------------------\n",
      "train Loss: 0.0156 Acc: 99.7092\n",
      "test Loss: 0.1048 Acc: 98.2968\n",
      "\n",
      "Epoch 843/1199\n",
      "------------------------\n",
      "train Loss: 0.0148 Acc: 99.7111\n",
      "test Loss: 0.1061 Acc: 98.3171\n",
      "\n",
      "Epoch 844/1199\n",
      "------------------------\n",
      "train Loss: 0.0139 Acc: 99.7294\n",
      "test Loss: 0.1038 Acc: 98.3142\n",
      "\n",
      "Epoch 845/1199\n",
      "------------------------\n",
      "train Loss: 0.0158 Acc: 99.7019\n",
      "test Loss: 0.1073 Acc: 98.3266\n",
      "\n",
      "Epoch 846/1199\n",
      "------------------------\n",
      "train Loss: 0.0143 Acc: 99.7231\n",
      "test Loss: 0.1054 Acc: 98.3270\n",
      "\n",
      "Epoch 847/1199\n",
      "------------------------\n",
      "train Loss: 0.0152 Acc: 99.7159\n",
      "test Loss: 0.1064 Acc: 98.3025\n",
      "\n",
      "Epoch 848/1199\n",
      "------------------------\n",
      "train Loss: 0.0150 Acc: 99.7192\n",
      "test Loss: 0.1075 Acc: 98.3147\n",
      "\n",
      "Epoch 849/1199\n",
      "------------------------\n",
      "train Loss: 0.0145 Acc: 99.7309\n",
      "test Loss: 0.1048 Acc: 98.3113\n",
      "\n",
      "Epoch 850/1199\n",
      "------------------------\n",
      "train Loss: 0.0145 Acc: 99.7269\n",
      "test Loss: 0.1059 Acc: 98.3354\n",
      "\n",
      "Epoch 851/1199\n",
      "------------------------\n",
      "train Loss: 0.0153 Acc: 99.7080\n",
      "test Loss: 0.1057 Acc: 98.3246\n",
      "\n",
      "Epoch 852/1199\n",
      "------------------------\n",
      "train Loss: 0.0142 Acc: 99.7280\n",
      "test Loss: 0.1071 Acc: 98.3249\n",
      "\n",
      "Epoch 853/1199\n",
      "------------------------\n",
      "train Loss: 0.0149 Acc: 99.7184\n",
      "test Loss: 0.1059 Acc: 98.2712\n",
      "\n",
      "Epoch 854/1199\n",
      "------------------------\n",
      "train Loss: 0.0145 Acc: 99.7236\n",
      "test Loss: 0.1058 Acc: 98.2985\n",
      "\n",
      "Epoch 855/1199\n",
      "------------------------\n",
      "train Loss: 0.0143 Acc: 99.7239\n",
      "test Loss: 0.1094 Acc: 98.2962\n",
      "\n",
      "Epoch 856/1199\n",
      "------------------------\n",
      "train Loss: 0.0156 Acc: 99.7031\n",
      "test Loss: 0.1046 Acc: 98.3228\n",
      "\n",
      "Epoch 857/1199\n",
      "------------------------\n",
      "train Loss: 0.0166 Acc: 99.6857\n",
      "test Loss: 0.1020 Acc: 98.3339\n",
      "\n",
      "Epoch 858/1199\n",
      "------------------------\n",
      "train Loss: 0.0143 Acc: 99.7139\n",
      "test Loss: 0.1064 Acc: 98.2873\n",
      "\n",
      "Epoch 859/1199\n",
      "------------------------\n",
      "train Loss: 0.0149 Acc: 99.7189\n",
      "test Loss: 0.1053 Acc: 98.3215\n",
      "\n",
      "Epoch 860/1199\n",
      "------------------------\n",
      "train Loss: 0.0161 Acc: 99.6891\n",
      "test Loss: 0.1017 Acc: 98.3339\n",
      "\n",
      "Epoch 861/1199\n",
      "------------------------\n",
      "train Loss: 0.0132 Acc: 99.7583\n",
      "test Loss: 0.1021 Acc: 98.3361\n",
      "\n",
      "Epoch 862/1199\n",
      "------------------------\n",
      "train Loss: 0.0146 Acc: 99.7244\n",
      "test Loss: 0.1072 Acc: 98.3522\n",
      "\n",
      "Epoch 863/1199\n",
      "------------------------\n",
      "train Loss: 0.0155 Acc: 99.7164\n",
      "test Loss: 0.1044 Acc: 98.2935\n",
      "\n",
      "Epoch 864/1199\n",
      "------------------------\n",
      "train Loss: 0.0143 Acc: 99.7158\n",
      "test Loss: 0.1027 Acc: 98.3061\n",
      "\n",
      "Epoch 865/1199\n",
      "------------------------\n",
      "train Loss: 0.0145 Acc: 99.7285\n",
      "test Loss: 0.1027 Acc: 98.2947\n",
      "\n",
      "Epoch 866/1199\n",
      "------------------------\n",
      "train Loss: 0.0144 Acc: 99.7198\n",
      "test Loss: 0.1045 Acc: 98.3366\n",
      "\n",
      "Epoch 867/1199\n",
      "------------------------\n",
      "train Loss: 0.0149 Acc: 99.7190\n",
      "test Loss: 0.1068 Acc: 98.3142\n",
      "\n",
      "Epoch 868/1199\n",
      "------------------------\n",
      "train Loss: 0.0155 Acc: 99.7022\n",
      "test Loss: 0.1055 Acc: 98.2962\n",
      "\n",
      "Epoch 869/1199\n",
      "------------------------\n",
      "train Loss: 0.0157 Acc: 99.7120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1047 Acc: 98.3030\n",
      "\n",
      "Epoch 870/1199\n",
      "------------------------\n",
      "train Loss: 0.0156 Acc: 99.7040\n",
      "test Loss: 0.1033 Acc: 98.3171\n",
      "\n",
      "Epoch 871/1199\n",
      "------------------------\n",
      "train Loss: 0.0144 Acc: 99.7323\n",
      "test Loss: 0.1056 Acc: 98.3410\n",
      "\n",
      "Epoch 872/1199\n",
      "------------------------\n",
      "train Loss: 0.0146 Acc: 99.7264\n",
      "test Loss: 0.1049 Acc: 98.3765\n",
      "\n",
      "Epoch 873/1199\n",
      "------------------------\n",
      "train Loss: 0.0146 Acc: 99.7255\n",
      "test Loss: 0.1037 Acc: 98.2876\n",
      "\n",
      "Epoch 874/1199\n",
      "------------------------\n",
      "train Loss: 0.0141 Acc: 99.7299\n",
      "test Loss: 0.1048 Acc: 98.3299\n",
      "\n",
      "Epoch 875/1199\n",
      "------------------------\n",
      "train Loss: 0.0155 Acc: 99.7118\n",
      "test Loss: 0.1058 Acc: 98.3272\n",
      "\n",
      "Epoch 876/1199\n",
      "------------------------\n",
      "train Loss: 0.0148 Acc: 99.7228\n",
      "test Loss: 0.1065 Acc: 98.3437\n",
      "\n",
      "Epoch 877/1199\n",
      "------------------------\n",
      "train Loss: 0.0142 Acc: 99.7328\n",
      "test Loss: 0.1073 Acc: 98.2968\n",
      "\n",
      "Epoch 878/1199\n",
      "------------------------\n",
      "train Loss: 0.0141 Acc: 99.7310\n",
      "test Loss: 0.1055 Acc: 98.2969\n",
      "\n",
      "Epoch 879/1199\n",
      "------------------------\n",
      "train Loss: 0.0139 Acc: 99.7343\n",
      "test Loss: 0.1069 Acc: 98.3182\n",
      "\n",
      "Epoch 880/1199\n",
      "------------------------\n",
      "train Loss: 0.0148 Acc: 99.7215\n",
      "test Loss: 0.1061 Acc: 98.3384\n",
      "\n",
      "Epoch 881/1199\n",
      "------------------------\n",
      "train Loss: 0.0140 Acc: 99.7286\n",
      "test Loss: 0.1007 Acc: 98.3613\n",
      "\n",
      "Epoch 882/1199\n",
      "------------------------\n",
      "train Loss: 0.0136 Acc: 99.7398\n",
      "test Loss: 0.1062 Acc: 98.2748\n",
      "\n",
      "Epoch 883/1199\n",
      "------------------------\n",
      "train Loss: 0.0137 Acc: 99.7463\n",
      "test Loss: 0.1031 Acc: 98.3007\n",
      "\n",
      "Epoch 884/1199\n",
      "------------------------\n",
      "train Loss: 0.0139 Acc: 99.7419\n",
      "test Loss: 0.1067 Acc: 98.3465\n",
      "\n",
      "Epoch 885/1199\n",
      "------------------------\n",
      "train Loss: 0.0151 Acc: 99.7176\n",
      "test Loss: 0.1037 Acc: 98.3006\n",
      "\n",
      "Epoch 886/1199\n",
      "------------------------\n",
      "train Loss: 0.0135 Acc: 99.7440\n",
      "test Loss: 0.1043 Acc: 98.2935\n",
      "\n",
      "Epoch 887/1199\n",
      "------------------------\n",
      "train Loss: 0.0141 Acc: 99.7341\n",
      "test Loss: 0.1063 Acc: 98.3019\n",
      "\n",
      "Epoch 888/1199\n",
      "------------------------\n",
      "train Loss: 0.0138 Acc: 99.7263\n",
      "test Loss: 0.1048 Acc: 98.3372\n",
      "\n",
      "Epoch 889/1199\n",
      "------------------------\n",
      "train Loss: 0.0143 Acc: 99.7231\n",
      "test Loss: 0.1038 Acc: 98.3436\n",
      "\n",
      "Epoch 890/1199\n",
      "------------------------\n",
      "train Loss: 0.0145 Acc: 99.7238\n",
      "test Loss: 0.1058 Acc: 98.3272\n",
      "\n",
      "Epoch 891/1199\n",
      "------------------------\n",
      "train Loss: 0.0149 Acc: 99.7145\n",
      "test Loss: 0.1060 Acc: 98.3285\n",
      "\n",
      "Epoch 892/1199\n",
      "------------------------\n",
      "train Loss: 0.0153 Acc: 99.7096\n",
      "test Loss: 0.1061 Acc: 98.3107\n",
      "\n",
      "Epoch 893/1199\n",
      "------------------------\n",
      "train Loss: 0.0167 Acc: 99.6620\n",
      "test Loss: 0.1082 Acc: 98.3052\n",
      "\n",
      "Epoch 894/1199\n",
      "------------------------\n",
      "train Loss: 0.0188 Acc: 99.6303\n",
      "test Loss: 0.1076 Acc: 98.2175\n",
      "\n",
      "Epoch 895/1199\n",
      "------------------------\n",
      "train Loss: 0.0209 Acc: 99.5873\n",
      "test Loss: 0.1065 Acc: 98.1955\n",
      "\n",
      "Epoch 896/1199\n",
      "------------------------\n",
      "train Loss: 0.0181 Acc: 99.6228\n",
      "test Loss: 0.1082 Acc: 98.1987\n",
      "\n",
      "Epoch 897/1199\n",
      "------------------------\n",
      "train Loss: 0.0176 Acc: 99.6486\n",
      "test Loss: 0.1090 Acc: 98.1698\n",
      "\n",
      "Epoch 898/1199\n",
      "------------------------\n",
      "train Loss: 0.0153 Acc: 99.6904\n",
      "test Loss: 0.1098 Acc: 98.2258\n",
      "\n",
      "Epoch 899/1199\n",
      "------------------------\n",
      "train Loss: 0.0169 Acc: 99.6684\n",
      "test Loss: 0.1082 Acc: 98.2073\n",
      "\n",
      "Epoch 900/1199\n",
      "------------------------\n",
      "train Loss: 0.0155 Acc: 99.6988\n",
      "test Loss: 0.1101 Acc: 98.2349\n",
      "\n",
      "Epoch 901/1199\n",
      "------------------------\n",
      "train Loss: 0.0140 Acc: 99.7356\n",
      "test Loss: 0.1043 Acc: 98.2968\n",
      "\n",
      "Epoch 902/1199\n",
      "------------------------\n",
      "train Loss: 0.0149 Acc: 99.7219\n",
      "test Loss: 0.1061 Acc: 98.2101\n",
      "\n",
      "Epoch 903/1199\n",
      "------------------------\n",
      "train Loss: 0.0140 Acc: 99.7321\n",
      "test Loss: 0.1059 Acc: 98.3095\n",
      "\n",
      "Epoch 904/1199\n",
      "------------------------\n",
      "train Loss: 0.0137 Acc: 99.7471\n",
      "test Loss: 0.1062 Acc: 98.2886\n",
      "\n",
      "Epoch 905/1199\n",
      "------------------------\n",
      "train Loss: 0.0147 Acc: 99.7226\n",
      "test Loss: 0.1036 Acc: 98.3482\n",
      "\n",
      "Epoch 906/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7552\n",
      "test Loss: 0.1090 Acc: 98.2897\n",
      "\n",
      "Epoch 907/1199\n",
      "------------------------\n",
      "train Loss: 0.0141 Acc: 99.7390\n",
      "test Loss: 0.1033 Acc: 98.3164\n",
      "\n",
      "Epoch 908/1199\n",
      "------------------------\n",
      "train Loss: 0.0139 Acc: 99.7432\n",
      "test Loss: 0.1084 Acc: 98.2631\n",
      "\n",
      "Epoch 909/1199\n",
      "------------------------\n",
      "train Loss: 0.0139 Acc: 99.7323\n",
      "test Loss: 0.1048 Acc: 98.2684\n",
      "\n",
      "Epoch 910/1199\n",
      "------------------------\n",
      "train Loss: 0.0135 Acc: 99.7479\n",
      "test Loss: 0.1037 Acc: 98.3745\n",
      "\n",
      "Epoch 911/1199\n",
      "------------------------\n",
      "train Loss: 0.0144 Acc: 99.7342\n",
      "test Loss: 0.1057 Acc: 98.3375\n",
      "\n",
      "Epoch 912/1199\n",
      "------------------------\n",
      "train Loss: 0.0141 Acc: 99.7341\n",
      "test Loss: 0.1044 Acc: 98.3297\n",
      "\n",
      "Epoch 913/1199\n",
      "------------------------\n",
      "train Loss: 0.0142 Acc: 99.7263\n",
      "test Loss: 0.1030 Acc: 98.3683\n",
      "\n",
      "Epoch 914/1199\n",
      "------------------------\n",
      "train Loss: 0.0125 Acc: 99.7600\n",
      "test Loss: 0.1035 Acc: 98.2981\n",
      "\n",
      "Epoch 915/1199\n",
      "------------------------\n",
      "train Loss: 0.0133 Acc: 99.7479\n",
      "test Loss: 0.1053 Acc: 98.3472\n",
      "\n",
      "Epoch 916/1199\n",
      "------------------------\n",
      "train Loss: 0.0146 Acc: 99.7242\n",
      "test Loss: 0.1058 Acc: 98.3202\n",
      "\n",
      "Epoch 917/1199\n",
      "------------------------\n",
      "train Loss: 0.0147 Acc: 99.7244\n",
      "test Loss: 0.1037 Acc: 98.3693\n",
      "\n",
      "Epoch 918/1199\n",
      "------------------------\n",
      "train Loss: 0.0145 Acc: 99.7288\n",
      "test Loss: 0.1068 Acc: 98.2912\n",
      "\n",
      "Epoch 919/1199\n",
      "------------------------\n",
      "train Loss: 0.0146 Acc: 99.7363\n",
      "test Loss: 0.1050 Acc: 98.3202\n",
      "\n",
      "Epoch 920/1199\n",
      "------------------------\n",
      "train Loss: 0.0137 Acc: 99.7456\n",
      "test Loss: 0.1078 Acc: 98.3192\n",
      "\n",
      "Epoch 921/1199\n",
      "------------------------\n",
      "train Loss: 0.0138 Acc: 99.7398\n",
      "test Loss: 0.1058 Acc: 98.2971\n",
      "\n",
      "Epoch 922/1199\n",
      "------------------------\n",
      "train Loss: 0.0139 Acc: 99.7370\n",
      "test Loss: 0.1058 Acc: 98.3332\n",
      "\n",
      "Epoch 923/1199\n",
      "------------------------\n",
      "train Loss: 0.0132 Acc: 99.7524\n",
      "test Loss: 0.1025 Acc: 98.3183\n",
      "\n",
      "Epoch 924/1199\n",
      "------------------------\n",
      "train Loss: 0.0142 Acc: 99.7360\n",
      "test Loss: 0.1041 Acc: 98.3626\n",
      "\n",
      "Epoch 925/1199\n",
      "------------------------\n",
      "train Loss: 0.0143 Acc: 99.7331\n",
      "test Loss: 0.1080 Acc: 98.3489\n",
      "\n",
      "Epoch 926/1199\n",
      "------------------------\n",
      "train Loss: 0.0141 Acc: 99.7383\n",
      "test Loss: 0.1059 Acc: 98.3144\n",
      "\n",
      "Epoch 927/1199\n",
      "------------------------\n",
      "train Loss: 0.0130 Acc: 99.7522\n",
      "test Loss: 0.1021 Acc: 98.3358\n",
      "\n",
      "Epoch 928/1199\n",
      "------------------------\n",
      "train Loss: 0.0137 Acc: 99.7423\n",
      "test Loss: 0.1050 Acc: 98.3225\n",
      "\n",
      "Epoch 929/1199\n",
      "------------------------\n",
      "train Loss: 0.0137 Acc: 99.7484\n",
      "test Loss: 0.1024 Acc: 98.3211\n",
      "\n",
      "Epoch 930/1199\n",
      "------------------------\n",
      "train Loss: 0.0141 Acc: 99.7390\n",
      "test Loss: 0.1087 Acc: 98.3328\n",
      "\n",
      "Epoch 931/1199\n",
      "------------------------\n",
      "train Loss: 0.0146 Acc: 99.7300\n",
      "test Loss: 0.1027 Acc: 98.3387\n",
      "\n",
      "Epoch 932/1199\n",
      "------------------------\n",
      "train Loss: 0.0143 Acc: 99.7331\n",
      "test Loss: 0.1067 Acc: 98.3190\n",
      "\n",
      "Epoch 933/1199\n",
      "------------------------\n",
      "train Loss: 0.0129 Acc: 99.7597\n",
      "test Loss: 0.1041 Acc: 98.3266\n",
      "\n",
      "Epoch 934/1199\n",
      "------------------------\n",
      "train Loss: 0.0141 Acc: 99.7418\n",
      "test Loss: 0.1079 Acc: 98.2938\n",
      "\n",
      "Epoch 935/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7486\n",
      "test Loss: 0.1056 Acc: 98.3253\n",
      "\n",
      "Epoch 936/1199\n",
      "------------------------\n",
      "train Loss: 0.0133 Acc: 99.7588\n",
      "test Loss: 0.1056 Acc: 98.3499\n",
      "\n",
      "Epoch 937/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7511\n",
      "test Loss: 0.1039 Acc: 98.3470\n",
      "\n",
      "Epoch 938/1199\n",
      "------------------------\n",
      "train Loss: 0.0132 Acc: 99.7546\n",
      "test Loss: 0.1050 Acc: 98.2933\n",
      "\n",
      "Epoch 939/1199\n",
      "------------------------\n",
      "train Loss: 0.0133 Acc: 99.7528\n",
      "test Loss: 0.1057 Acc: 98.3114\n",
      "\n",
      "Epoch 940/1199\n",
      "------------------------\n",
      "train Loss: 0.0130 Acc: 99.7561\n",
      "test Loss: 0.1064 Acc: 98.3161\n",
      "\n",
      "Epoch 941/1199\n",
      "------------------------\n",
      "train Loss: 0.0131 Acc: 99.7622\n",
      "test Loss: 0.1029 Acc: 98.3132\n",
      "\n",
      "Epoch 942/1199\n",
      "------------------------\n",
      "train Loss: 0.0137 Acc: 99.7458\n",
      "test Loss: 0.1066 Acc: 98.3023\n",
      "\n",
      "Epoch 943/1199\n",
      "------------------------\n",
      "train Loss: 0.0136 Acc: 99.7465\n",
      "test Loss: 0.1039 Acc: 98.3147\n",
      "\n",
      "Epoch 944/1199\n",
      "------------------------\n",
      "train Loss: 0.0124 Acc: 99.7711\n",
      "test Loss: 0.1056 Acc: 98.3373\n",
      "\n",
      "Epoch 945/1199\n",
      "------------------------\n",
      "train Loss: 0.0136 Acc: 99.7473\n",
      "test Loss: 0.1077 Acc: 98.3107\n",
      "\n",
      "Epoch 946/1199\n",
      "------------------------\n",
      "train Loss: 0.0136 Acc: 99.7510\n",
      "test Loss: 0.1061 Acc: 98.3192\n",
      "\n",
      "Epoch 947/1199\n",
      "------------------------\n",
      "train Loss: 0.0137 Acc: 99.7450\n",
      "test Loss: 0.1082 Acc: 98.2847\n",
      "\n",
      "Epoch 948/1199\n",
      "------------------------\n",
      "train Loss: 0.0144 Acc: 99.7336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1060 Acc: 98.3385\n",
      "\n",
      "Epoch 949/1199\n",
      "------------------------\n",
      "train Loss: 0.0141 Acc: 99.7313\n",
      "test Loss: 0.1063 Acc: 98.3296\n",
      "\n",
      "Epoch 950/1199\n",
      "------------------------\n",
      "train Loss: 0.0140 Acc: 99.7434\n",
      "test Loss: 0.1074 Acc: 98.3332\n",
      "\n",
      "Epoch 951/1199\n",
      "------------------------\n",
      "train Loss: 0.0137 Acc: 99.7419\n",
      "test Loss: 0.1053 Acc: 98.3396\n",
      "\n",
      "Epoch 952/1199\n",
      "------------------------\n",
      "train Loss: 0.0150 Acc: 99.7224\n",
      "test Loss: 0.1042 Acc: 98.3183\n",
      "\n",
      "Epoch 953/1199\n",
      "------------------------\n",
      "train Loss: 0.0131 Acc: 99.7602\n",
      "test Loss: 0.1108 Acc: 98.3285\n",
      "\n",
      "Epoch 954/1199\n",
      "------------------------\n",
      "train Loss: 0.0137 Acc: 99.7447\n",
      "test Loss: 0.1063 Acc: 98.2876\n",
      "\n",
      "Epoch 955/1199\n",
      "------------------------\n",
      "train Loss: 0.0140 Acc: 99.7399\n",
      "test Loss: 0.1051 Acc: 98.3550\n",
      "\n",
      "Epoch 956/1199\n",
      "------------------------\n",
      "train Loss: 0.0133 Acc: 99.7519\n",
      "test Loss: 0.1036 Acc: 98.3278\n",
      "\n",
      "Epoch 957/1199\n",
      "------------------------\n",
      "train Loss: 0.0136 Acc: 99.7486\n",
      "test Loss: 0.1044 Acc: 98.3493\n",
      "\n",
      "Epoch 958/1199\n",
      "------------------------\n",
      "train Loss: 0.0132 Acc: 99.7565\n",
      "test Loss: 0.1047 Acc: 98.3588\n",
      "\n",
      "Epoch 959/1199\n",
      "------------------------\n",
      "train Loss: 0.0141 Acc: 99.7474\n",
      "test Loss: 0.1061 Acc: 98.3315\n",
      "\n",
      "Epoch 960/1199\n",
      "------------------------\n",
      "train Loss: 0.0146 Acc: 99.7398\n",
      "test Loss: 0.1070 Acc: 98.3285\n",
      "\n",
      "Epoch 961/1199\n",
      "------------------------\n",
      "train Loss: 0.0132 Acc: 99.7580\n",
      "test Loss: 0.1047 Acc: 98.3287\n",
      "\n",
      "Epoch 962/1199\n",
      "------------------------\n",
      "train Loss: 0.0135 Acc: 99.7497\n",
      "test Loss: 0.1053 Acc: 98.3196\n",
      "\n",
      "Epoch 963/1199\n",
      "------------------------\n",
      "train Loss: 0.0131 Acc: 99.7580\n",
      "test Loss: 0.1075 Acc: 98.2919\n",
      "\n",
      "Epoch 964/1199\n",
      "------------------------\n",
      "train Loss: 0.0149 Acc: 99.7192\n",
      "test Loss: 0.1064 Acc: 98.3316\n",
      "\n",
      "Epoch 965/1199\n",
      "------------------------\n",
      "train Loss: 0.0133 Acc: 99.7445\n",
      "test Loss: 0.1077 Acc: 98.3154\n",
      "\n",
      "Epoch 966/1199\n",
      "------------------------\n",
      "train Loss: 0.0145 Acc: 99.7235\n",
      "test Loss: 0.1063 Acc: 98.3057\n",
      "\n",
      "Epoch 967/1199\n",
      "------------------------\n",
      "train Loss: 0.0132 Acc: 99.7513\n",
      "test Loss: 0.1073 Acc: 98.3339\n",
      "\n",
      "Epoch 968/1199\n",
      "------------------------\n",
      "train Loss: 0.0142 Acc: 99.7399\n",
      "test Loss: 0.1054 Acc: 98.2883\n",
      "\n",
      "Epoch 969/1199\n",
      "------------------------\n",
      "train Loss: 0.0122 Acc: 99.7727\n",
      "test Loss: 0.1084 Acc: 98.3727\n",
      "\n",
      "Epoch 970/1199\n",
      "------------------------\n",
      "train Loss: 0.0133 Acc: 99.7525\n",
      "test Loss: 0.1070 Acc: 98.2966\n",
      "\n",
      "Epoch 971/1199\n",
      "------------------------\n",
      "train Loss: 0.0147 Acc: 99.7267\n",
      "test Loss: 0.1031 Acc: 98.3677\n",
      "\n",
      "Epoch 972/1199\n",
      "------------------------\n",
      "train Loss: 0.0131 Acc: 99.7630\n",
      "test Loss: 0.1051 Acc: 98.3458\n",
      "\n",
      "Epoch 973/1199\n",
      "------------------------\n",
      "train Loss: 0.0147 Acc: 99.7256\n",
      "test Loss: 0.1061 Acc: 98.3417\n",
      "\n",
      "Epoch 974/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7565\n",
      "test Loss: 0.1072 Acc: 98.3423\n",
      "\n",
      "Epoch 975/1199\n",
      "------------------------\n",
      "train Loss: 0.0142 Acc: 99.7410\n",
      "test Loss: 0.1057 Acc: 98.3209\n",
      "\n",
      "Epoch 976/1199\n",
      "------------------------\n",
      "train Loss: 0.0133 Acc: 99.7520\n",
      "test Loss: 0.1058 Acc: 98.2917\n",
      "\n",
      "Epoch 977/1199\n",
      "------------------------\n",
      "train Loss: 0.0137 Acc: 99.7430\n",
      "test Loss: 0.1046 Acc: 98.3299\n",
      "\n",
      "Epoch 978/1199\n",
      "------------------------\n",
      "train Loss: 0.0136 Acc: 99.7479\n",
      "test Loss: 0.1046 Acc: 98.3439\n",
      "\n",
      "Epoch 979/1199\n",
      "------------------------\n",
      "train Loss: 0.0138 Acc: 99.7472\n",
      "test Loss: 0.1055 Acc: 98.3006\n",
      "\n",
      "Epoch 980/1199\n",
      "------------------------\n",
      "train Loss: 0.0148 Acc: 99.7377\n",
      "test Loss: 0.1056 Acc: 98.2570\n",
      "\n",
      "Epoch 981/1199\n",
      "------------------------\n",
      "train Loss: 0.0128 Acc: 99.7637\n",
      "test Loss: 0.1076 Acc: 98.3235\n",
      "\n",
      "Epoch 982/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7563\n",
      "test Loss: 0.1065 Acc: 98.3703\n",
      "\n",
      "Epoch 983/1199\n",
      "------------------------\n",
      "train Loss: 0.0148 Acc: 99.7263\n",
      "test Loss: 0.1058 Acc: 98.3052\n",
      "\n",
      "Epoch 984/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7609\n",
      "test Loss: 0.1058 Acc: 98.2708\n",
      "\n",
      "Epoch 985/1199\n",
      "------------------------\n",
      "train Loss: 0.0123 Acc: 99.7709\n",
      "test Loss: 0.1052 Acc: 98.3363\n",
      "\n",
      "Epoch 986/1199\n",
      "------------------------\n",
      "train Loss: 0.0127 Acc: 99.7639\n",
      "test Loss: 0.1034 Acc: 98.3603\n",
      "\n",
      "Epoch 987/1199\n",
      "------------------------\n",
      "train Loss: 0.0121 Acc: 99.7754\n",
      "test Loss: 0.1064 Acc: 98.3484\n",
      "\n",
      "Epoch 988/1199\n",
      "------------------------\n",
      "train Loss: 0.0142 Acc: 99.7392\n",
      "test Loss: 0.1040 Acc: 98.2985\n",
      "\n",
      "Epoch 989/1199\n",
      "------------------------\n",
      "train Loss: 0.0141 Acc: 99.7411\n",
      "test Loss: 0.1047 Acc: 98.3337\n",
      "\n",
      "Epoch 990/1199\n",
      "------------------------\n",
      "train Loss: 0.0132 Acc: 99.7514\n",
      "test Loss: 0.1047 Acc: 98.3057\n",
      "\n",
      "Epoch 991/1199\n",
      "------------------------\n",
      "train Loss: 0.0130 Acc: 99.7642\n",
      "test Loss: 0.1049 Acc: 98.3660\n",
      "\n",
      "Epoch 992/1199\n",
      "------------------------\n",
      "train Loss: 0.0138 Acc: 99.7510\n",
      "test Loss: 0.1051 Acc: 98.3170\n",
      "\n",
      "Epoch 993/1199\n",
      "------------------------\n",
      "train Loss: 0.0131 Acc: 99.7606\n",
      "test Loss: 0.1077 Acc: 98.3019\n",
      "\n",
      "Epoch 994/1199\n",
      "------------------------\n",
      "train Loss: 0.0126 Acc: 99.7689\n",
      "test Loss: 0.1096 Acc: 98.2814\n",
      "\n",
      "Epoch 995/1199\n",
      "------------------------\n",
      "train Loss: 0.0137 Acc: 99.7527\n",
      "test Loss: 0.1066 Acc: 98.3460\n",
      "\n",
      "Epoch 996/1199\n",
      "------------------------\n",
      "train Loss: 0.0138 Acc: 99.7373\n",
      "test Loss: 0.1037 Acc: 98.3743\n",
      "\n",
      "Epoch 997/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7507\n",
      "test Loss: 0.1070 Acc: 98.3049\n",
      "\n",
      "Epoch 998/1199\n",
      "------------------------\n",
      "train Loss: 0.0130 Acc: 99.7590\n",
      "test Loss: 0.1081 Acc: 98.3209\n",
      "\n",
      "Epoch 999/1199\n",
      "------------------------\n",
      "train Loss: 0.0144 Acc: 99.7352\n",
      "test Loss: 0.1066 Acc: 98.2759\n",
      "\n",
      "Epoch 1000/1199\n",
      "------------------------\n",
      "train Loss: 0.0120 Acc: 99.7821\n",
      "test Loss: 0.1096 Acc: 98.3617\n",
      "\n",
      "Epoch 1001/1199\n",
      "------------------------\n",
      "train Loss: 0.0126 Acc: 99.7741\n",
      "test Loss: 0.1057 Acc: 98.2990\n",
      "\n",
      "Epoch 1002/1199\n",
      "------------------------\n",
      "train Loss: 0.0132 Acc: 99.7540\n",
      "test Loss: 0.1067 Acc: 98.3237\n",
      "\n",
      "Epoch 1003/1199\n",
      "------------------------\n",
      "train Loss: 0.0133 Acc: 99.7519\n",
      "test Loss: 0.1051 Acc: 98.3375\n",
      "\n",
      "Epoch 1004/1199\n",
      "------------------------\n",
      "train Loss: 0.0131 Acc: 99.7541\n",
      "test Loss: 0.1064 Acc: 98.3624\n",
      "\n",
      "Epoch 1005/1199\n",
      "------------------------\n",
      "train Loss: 0.0131 Acc: 99.7506\n",
      "test Loss: 0.1071 Acc: 98.2653\n",
      "\n",
      "Epoch 1006/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7571\n",
      "test Loss: 0.1046 Acc: 98.2890\n",
      "\n",
      "Epoch 1007/1199\n",
      "------------------------\n",
      "train Loss: 0.0148 Acc: 99.7235\n",
      "test Loss: 0.1026 Acc: 98.3389\n",
      "\n",
      "Epoch 1008/1199\n",
      "------------------------\n",
      "train Loss: 0.0142 Acc: 99.7350\n",
      "test Loss: 0.1092 Acc: 98.3838\n",
      "\n",
      "Epoch 1009/1199\n",
      "------------------------\n",
      "train Loss: 0.0127 Acc: 99.7655\n",
      "test Loss: 0.1113 Acc: 98.2949\n",
      "\n",
      "Epoch 1010/1199\n",
      "------------------------\n",
      "train Loss: 0.0144 Acc: 99.7414\n",
      "test Loss: 0.1088 Acc: 98.3315\n",
      "\n",
      "Epoch 1011/1199\n",
      "------------------------\n",
      "train Loss: 0.0127 Acc: 99.7652\n",
      "test Loss: 0.1042 Acc: 98.3256\n",
      "\n",
      "Epoch 1012/1199\n",
      "------------------------\n",
      "train Loss: 0.0132 Acc: 99.7553\n",
      "test Loss: 0.1089 Acc: 98.3444\n",
      "\n",
      "Epoch 1013/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7450\n",
      "test Loss: 0.1034 Acc: 98.3349\n",
      "\n",
      "Epoch 1014/1199\n",
      "------------------------\n",
      "train Loss: 0.0129 Acc: 99.7591\n",
      "test Loss: 0.1061 Acc: 98.3265\n",
      "\n",
      "Epoch 1015/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7556\n",
      "test Loss: 0.1078 Acc: 98.2765\n",
      "\n",
      "Epoch 1016/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7481\n",
      "test Loss: 0.1056 Acc: 98.3009\n",
      "\n",
      "Epoch 1017/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7544\n",
      "test Loss: 0.1059 Acc: 98.3555\n",
      "\n",
      "Epoch 1018/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7472\n",
      "test Loss: 0.1085 Acc: 98.2852\n",
      "\n",
      "Epoch 1019/1199\n",
      "------------------------\n",
      "train Loss: 0.0147 Acc: 99.7327\n",
      "test Loss: 0.1068 Acc: 98.3736\n",
      "\n",
      "Epoch 1020/1199\n",
      "------------------------\n",
      "train Loss: 0.0129 Acc: 99.7671\n",
      "test Loss: 0.1080 Acc: 98.3612\n",
      "\n",
      "Epoch 1021/1199\n",
      "------------------------\n",
      "train Loss: 0.0138 Acc: 99.7460\n",
      "test Loss: 0.1072 Acc: 98.3144\n",
      "\n",
      "Epoch 1022/1199\n",
      "------------------------\n",
      "train Loss: 0.0141 Acc: 99.7386\n",
      "test Loss: 0.1072 Acc: 98.3731\n",
      "\n",
      "Epoch 1023/1199\n",
      "------------------------\n",
      "train Loss: 0.0120 Acc: 99.7841\n",
      "test Loss: 0.1057 Acc: 98.3515\n",
      "\n",
      "Epoch 1024/1199\n",
      "------------------------\n",
      "train Loss: 0.0128 Acc: 99.7633\n",
      "test Loss: 0.1057 Acc: 98.3213\n",
      "\n",
      "Epoch 1025/1199\n",
      "------------------------\n",
      "train Loss: 0.0142 Acc: 99.7447\n",
      "test Loss: 0.1028 Acc: 98.3551\n",
      "\n",
      "Epoch 1026/1199\n",
      "------------------------\n",
      "train Loss: 0.0146 Acc: 99.7273\n",
      "test Loss: 0.1082 Acc: 98.3328\n",
      "\n",
      "Epoch 1027/1199\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0136 Acc: 99.7544\n",
      "test Loss: 0.1099 Acc: 98.3800\n",
      "\n",
      "Epoch 1028/1199\n",
      "------------------------\n",
      "train Loss: 0.0127 Acc: 99.7675\n",
      "test Loss: 0.1070 Acc: 98.3377\n",
      "\n",
      "Epoch 1029/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7569\n",
      "test Loss: 0.1096 Acc: 98.3413\n",
      "\n",
      "Epoch 1030/1199\n",
      "------------------------\n",
      "train Loss: 0.0135 Acc: 99.7519\n",
      "test Loss: 0.1075 Acc: 98.3467\n",
      "\n",
      "Epoch 1031/1199\n",
      "------------------------\n",
      "train Loss: 0.0147 Acc: 99.7326\n",
      "test Loss: 0.1074 Acc: 98.2897\n",
      "\n",
      "Epoch 1032/1199\n",
      "------------------------\n",
      "train Loss: 0.0128 Acc: 99.7606\n",
      "test Loss: 0.1090 Acc: 98.3436\n",
      "\n",
      "Epoch 1033/1199\n",
      "------------------------\n",
      "train Loss: 0.0129 Acc: 99.7566\n",
      "test Loss: 0.1082 Acc: 98.2802\n",
      "\n",
      "Epoch 1034/1199\n",
      "------------------------\n",
      "train Loss: 0.0139 Acc: 99.7459\n",
      "test Loss: 0.1061 Acc: 98.3366\n",
      "\n",
      "Epoch 1035/1199\n",
      "------------------------\n",
      "train Loss: 0.0139 Acc: 99.7436\n",
      "test Loss: 0.1054 Acc: 98.3408\n",
      "\n",
      "Epoch 1036/1199\n",
      "------------------------\n",
      "train Loss: 0.0142 Acc: 99.7383\n",
      "test Loss: 0.1069 Acc: 98.3256\n",
      "\n",
      "Epoch 1037/1199\n",
      "------------------------\n",
      "train Loss: 0.0135 Acc: 99.7513\n",
      "test Loss: 0.1059 Acc: 98.3726\n",
      "\n",
      "Epoch 1038/1199\n",
      "------------------------\n",
      "train Loss: 0.0139 Acc: 99.7476\n",
      "test Loss: 0.1052 Acc: 98.3259\n",
      "\n",
      "Epoch 1039/1199\n",
      "------------------------\n",
      "train Loss: 0.0133 Acc: 99.7628\n",
      "test Loss: 0.1084 Acc: 98.2928\n",
      "\n",
      "Epoch 1040/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7565\n",
      "test Loss: 0.1060 Acc: 98.3080\n",
      "\n",
      "Epoch 1041/1199\n",
      "------------------------\n",
      "train Loss: 0.0132 Acc: 99.7563\n",
      "test Loss: 0.1089 Acc: 98.3638\n",
      "\n",
      "Epoch 1042/1199\n",
      "------------------------\n",
      "train Loss: 0.0138 Acc: 99.7492\n",
      "test Loss: 0.1092 Acc: 98.3045\n",
      "\n",
      "Epoch 1043/1199\n",
      "------------------------\n",
      "train Loss: 0.0139 Acc: 99.7426\n",
      "test Loss: 0.1047 Acc: 98.3556\n",
      "\n",
      "Epoch 1044/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7528\n",
      "test Loss: 0.1068 Acc: 98.3087\n",
      "\n",
      "Epoch 1045/1199\n",
      "------------------------\n",
      "train Loss: 0.0136 Acc: 99.7509\n",
      "test Loss: 0.1046 Acc: 98.3019\n",
      "\n",
      "Epoch 1046/1199\n",
      "------------------------\n",
      "train Loss: 0.0132 Acc: 99.7629\n",
      "test Loss: 0.1100 Acc: 98.3266\n",
      "\n",
      "Epoch 1047/1199\n",
      "------------------------\n",
      "train Loss: 0.0132 Acc: 99.7649\n",
      "test Loss: 0.1054 Acc: 98.3322\n",
      "\n",
      "Epoch 1048/1199\n",
      "------------------------\n",
      "train Loss: 0.0137 Acc: 99.7509\n",
      "test Loss: 0.1045 Acc: 98.3344\n",
      "\n",
      "Epoch 1049/1199\n",
      "------------------------\n",
      "train Loss: 0.0142 Acc: 99.7357\n",
      "test Loss: 0.1057 Acc: 98.3189\n",
      "\n",
      "Epoch 1050/1199\n",
      "------------------------\n",
      "train Loss: 0.0125 Acc: 99.7701\n",
      "test Loss: 0.1077 Acc: 98.3239\n",
      "\n",
      "Epoch 1051/1199\n",
      "------------------------\n",
      "train Loss: 0.0131 Acc: 99.7610\n",
      "test Loss: 0.1070 Acc: 98.3665\n",
      "\n",
      "Epoch 1052/1199\n",
      "------------------------\n",
      "train Loss: 0.0125 Acc: 99.7717\n",
      "test Loss: 0.1091 Acc: 98.2724\n",
      "\n",
      "Epoch 1053/1199\n",
      "------------------------\n",
      "train Loss: 0.0138 Acc: 99.7415\n",
      "test Loss: 0.1080 Acc: 98.3356\n",
      "\n",
      "Epoch 1054/1199\n",
      "------------------------\n",
      "train Loss: 0.0136 Acc: 99.7452\n",
      "test Loss: 0.1069 Acc: 98.3227\n",
      "\n",
      "Epoch 1055/1199\n",
      "------------------------\n",
      "train Loss: 0.0129 Acc: 99.7666\n",
      "test Loss: 0.1064 Acc: 98.3522\n",
      "\n",
      "Epoch 1056/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7518\n",
      "test Loss: 0.1083 Acc: 98.2907\n",
      "\n",
      "Epoch 1057/1199\n",
      "------------------------\n",
      "train Loss: 0.0132 Acc: 99.7576\n",
      "test Loss: 0.1080 Acc: 98.3037\n",
      "\n",
      "Epoch 1058/1199\n",
      "------------------------\n",
      "train Loss: 0.0136 Acc: 99.7450\n",
      "test Loss: 0.1068 Acc: 98.3619\n",
      "\n",
      "Epoch 1059/1199\n",
      "------------------------\n",
      "train Loss: 0.0133 Acc: 99.7493\n",
      "test Loss: 0.1046 Acc: 98.3225\n",
      "\n",
      "Epoch 1060/1199\n",
      "------------------------\n",
      "train Loss: 0.0141 Acc: 99.7311\n",
      "test Loss: 0.1034 Acc: 98.3106\n",
      "\n",
      "Epoch 1061/1199\n",
      "------------------------\n",
      "train Loss: 0.0128 Acc: 99.7673\n",
      "test Loss: 0.1047 Acc: 98.2997\n",
      "\n",
      "Epoch 1062/1199\n",
      "------------------------\n",
      "train Loss: 0.0125 Acc: 99.7738\n",
      "test Loss: 0.1079 Acc: 98.3403\n",
      "\n",
      "Epoch 1063/1199\n",
      "------------------------\n",
      "train Loss: 0.0137 Acc: 99.7528\n",
      "test Loss: 0.1066 Acc: 98.3075\n",
      "\n",
      "Epoch 1064/1199\n",
      "------------------------\n",
      "train Loss: 0.0127 Acc: 99.7736\n",
      "test Loss: 0.1073 Acc: 98.3296\n",
      "\n",
      "Epoch 1065/1199\n",
      "------------------------\n",
      "train Loss: 0.0127 Acc: 99.7676\n",
      "test Loss: 0.1071 Acc: 98.3608\n",
      "\n",
      "Epoch 1066/1199\n",
      "------------------------\n",
      "train Loss: 0.0155 Acc: 99.7210\n",
      "test Loss: 0.1065 Acc: 98.3593\n",
      "\n",
      "Epoch 1067/1199\n",
      "------------------------\n",
      "train Loss: 0.0131 Acc: 99.7587\n",
      "test Loss: 0.1068 Acc: 98.3664\n",
      "\n",
      "Epoch 1068/1199\n",
      "------------------------\n",
      "train Loss: 0.0122 Acc: 99.7816\n",
      "test Loss: 0.1075 Acc: 98.3398\n",
      "\n",
      "Epoch 1069/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7558\n",
      "test Loss: 0.1081 Acc: 98.3344\n",
      "\n",
      "Epoch 1070/1199\n",
      "------------------------\n",
      "train Loss: 0.0131 Acc: 99.7598\n",
      "test Loss: 0.1048 Acc: 98.3123\n",
      "\n",
      "Epoch 1071/1199\n",
      "------------------------\n",
      "train Loss: 0.0138 Acc: 99.7498\n",
      "test Loss: 0.1075 Acc: 98.3090\n",
      "\n",
      "Epoch 1072/1199\n",
      "------------------------\n",
      "train Loss: 0.0131 Acc: 99.7659\n",
      "test Loss: 0.1078 Acc: 98.3361\n",
      "\n",
      "Epoch 1073/1199\n",
      "------------------------\n",
      "train Loss: 0.0129 Acc: 99.7564\n",
      "test Loss: 0.1046 Acc: 98.3672\n",
      "\n",
      "Epoch 1074/1199\n",
      "------------------------\n",
      "train Loss: 0.0124 Acc: 99.7726\n",
      "test Loss: 0.1071 Acc: 98.3525\n",
      "\n",
      "Epoch 1075/1199\n",
      "------------------------\n",
      "train Loss: 0.0131 Acc: 99.7632\n",
      "test Loss: 0.1068 Acc: 98.3432\n",
      "\n",
      "Epoch 1076/1199\n",
      "------------------------\n",
      "train Loss: 0.0145 Acc: 99.7374\n",
      "test Loss: 0.1072 Acc: 98.3686\n",
      "\n",
      "Epoch 1077/1199\n",
      "------------------------\n",
      "train Loss: 0.0122 Acc: 99.7800\n",
      "test Loss: 0.1056 Acc: 98.3575\n",
      "\n",
      "Epoch 1078/1199\n",
      "------------------------\n",
      "train Loss: 0.0122 Acc: 99.7781\n",
      "test Loss: 0.1082 Acc: 98.3253\n",
      "\n",
      "Epoch 1079/1199\n",
      "------------------------\n",
      "train Loss: 0.0125 Acc: 99.7658\n",
      "test Loss: 0.1043 Acc: 98.3370\n",
      "\n",
      "Epoch 1080/1199\n",
      "------------------------\n",
      "train Loss: 0.0131 Acc: 99.7683\n",
      "test Loss: 0.1045 Acc: 98.3372\n",
      "\n",
      "Epoch 1081/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7536\n",
      "test Loss: 0.1079 Acc: 98.2980\n",
      "\n",
      "Epoch 1082/1199\n",
      "------------------------\n",
      "train Loss: 0.0127 Acc: 99.7658\n",
      "test Loss: 0.1086 Acc: 98.3425\n",
      "\n",
      "Epoch 1083/1199\n",
      "------------------------\n",
      "train Loss: 0.0136 Acc: 99.7537\n",
      "test Loss: 0.1067 Acc: 98.2855\n",
      "\n",
      "Epoch 1084/1199\n",
      "------------------------\n",
      "train Loss: 0.0124 Acc: 99.7706\n",
      "test Loss: 0.1062 Acc: 98.3154\n",
      "\n",
      "Epoch 1085/1199\n",
      "------------------------\n",
      "train Loss: 0.0125 Acc: 99.7685\n",
      "test Loss: 0.1072 Acc: 98.3088\n",
      "\n",
      "Epoch 1086/1199\n",
      "------------------------\n",
      "train Loss: 0.0131 Acc: 99.7529\n",
      "test Loss: 0.1065 Acc: 98.3420\n",
      "\n",
      "Epoch 1087/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7545\n",
      "test Loss: 0.1064 Acc: 98.2905\n",
      "\n",
      "Epoch 1088/1199\n",
      "------------------------\n",
      "train Loss: 0.0130 Acc: 99.7646\n",
      "test Loss: 0.1091 Acc: 98.3802\n",
      "\n",
      "Epoch 1089/1199\n",
      "------------------------\n",
      "train Loss: 0.0147 Acc: 99.7380\n",
      "test Loss: 0.1044 Acc: 98.3014\n",
      "\n",
      "Epoch 1090/1199\n",
      "------------------------\n",
      "train Loss: 0.0121 Acc: 99.7817\n",
      "test Loss: 0.1085 Acc: 98.3610\n",
      "\n",
      "Epoch 1091/1199\n",
      "------------------------\n",
      "train Loss: 0.0135 Acc: 99.7506\n",
      "test Loss: 0.1071 Acc: 98.2791\n",
      "\n",
      "Epoch 1092/1199\n",
      "------------------------\n",
      "train Loss: 0.0135 Acc: 99.7587\n",
      "test Loss: 0.1068 Acc: 98.3518\n",
      "\n",
      "Epoch 1093/1199\n",
      "------------------------\n",
      "train Loss: 0.0136 Acc: 99.7536\n",
      "test Loss: 0.1062 Acc: 98.3489\n",
      "\n",
      "Epoch 1094/1199\n",
      "------------------------\n",
      "train Loss: 0.0129 Acc: 99.7592\n",
      "test Loss: 0.1048 Acc: 98.3349\n",
      "\n",
      "Epoch 1095/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7581\n",
      "test Loss: 0.1083 Acc: 98.3116\n",
      "\n",
      "Epoch 1096/1199\n",
      "------------------------\n",
      "train Loss: 0.0124 Acc: 99.7757\n",
      "test Loss: 0.1054 Acc: 98.3463\n",
      "\n",
      "Epoch 1097/1199\n",
      "------------------------\n",
      "train Loss: 0.0136 Acc: 99.7479\n",
      "test Loss: 0.1067 Acc: 98.3142\n",
      "\n",
      "Epoch 1098/1199\n",
      "------------------------\n",
      "train Loss: 0.0140 Acc: 99.7494\n",
      "test Loss: 0.1081 Acc: 98.3513\n",
      "\n",
      "Epoch 1099/1199\n",
      "------------------------\n",
      "train Loss: 0.0139 Acc: 99.7334\n",
      "test Loss: 0.1070 Acc: 98.3489\n",
      "\n",
      "Epoch 1100/1199\n",
      "------------------------\n",
      "train Loss: 0.0139 Acc: 99.7390\n",
      "test Loss: 0.1097 Acc: 98.3508\n",
      "\n",
      "Epoch 1101/1199\n",
      "------------------------\n",
      "train Loss: 0.0123 Acc: 99.7816\n",
      "test Loss: 0.1059 Acc: 98.3240\n",
      "\n",
      "Epoch 1102/1199\n",
      "------------------------\n",
      "train Loss: 0.0124 Acc: 99.7745\n",
      "test Loss: 0.1086 Acc: 98.3370\n",
      "\n",
      "Epoch 1103/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7522\n",
      "test Loss: 0.1082 Acc: 98.3223\n",
      "\n",
      "Epoch 1104/1199\n",
      "------------------------\n",
      "train Loss: 0.0132 Acc: 99.7607\n",
      "test Loss: 0.1042 Acc: 98.3273\n",
      "\n",
      "Epoch 1105/1199\n",
      "------------------------\n",
      "train Loss: 0.0124 Acc: 99.7745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1075 Acc: 98.2921\n",
      "\n",
      "Epoch 1106/1199\n",
      "------------------------\n",
      "train Loss: 0.0128 Acc: 99.7742\n",
      "test Loss: 0.1067 Acc: 98.3582\n",
      "\n",
      "Epoch 1107/1199\n",
      "------------------------\n",
      "train Loss: 0.0126 Acc: 99.7708\n",
      "test Loss: 0.1049 Acc: 98.3477\n",
      "\n",
      "Epoch 1108/1199\n",
      "------------------------\n",
      "train Loss: 0.0119 Acc: 99.7831\n",
      "test Loss: 0.1066 Acc: 98.3040\n",
      "\n",
      "Epoch 1109/1199\n",
      "------------------------\n",
      "train Loss: 0.0129 Acc: 99.7593\n",
      "test Loss: 0.1059 Acc: 98.2645\n",
      "\n",
      "Epoch 1110/1199\n",
      "------------------------\n",
      "train Loss: 0.0125 Acc: 99.7723\n",
      "test Loss: 0.1044 Acc: 98.3873\n",
      "\n",
      "Epoch 1111/1199\n",
      "------------------------\n",
      "train Loss: 0.0133 Acc: 99.7609\n",
      "test Loss: 0.1045 Acc: 98.3486\n",
      "\n",
      "Epoch 1112/1199\n",
      "------------------------\n",
      "train Loss: 0.0120 Acc: 99.7795\n",
      "test Loss: 0.1054 Acc: 98.2862\n",
      "\n",
      "Epoch 1113/1199\n",
      "------------------------\n",
      "train Loss: 0.0124 Acc: 99.7725\n",
      "test Loss: 0.1069 Acc: 98.3164\n",
      "\n",
      "Epoch 1114/1199\n",
      "------------------------\n",
      "train Loss: 0.0117 Acc: 99.7874\n",
      "test Loss: 0.1077 Acc: 98.3660\n",
      "\n",
      "Epoch 1115/1199\n",
      "------------------------\n",
      "train Loss: 0.0131 Acc: 99.7579\n",
      "test Loss: 0.1048 Acc: 98.3080\n",
      "\n",
      "Epoch 1116/1199\n",
      "------------------------\n",
      "train Loss: 0.0132 Acc: 99.7612\n",
      "test Loss: 0.1093 Acc: 98.3779\n",
      "\n",
      "Epoch 1117/1199\n",
      "------------------------\n",
      "train Loss: 0.0127 Acc: 99.7657\n",
      "test Loss: 0.1056 Acc: 98.2807\n",
      "\n",
      "Epoch 1118/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7488\n",
      "test Loss: 0.1066 Acc: 98.3353\n",
      "\n",
      "Epoch 1119/1199\n",
      "------------------------\n",
      "train Loss: 0.0121 Acc: 99.7779\n",
      "test Loss: 0.1059 Acc: 98.3429\n",
      "\n",
      "Epoch 1120/1199\n",
      "------------------------\n",
      "train Loss: 0.0129 Acc: 99.7607\n",
      "test Loss: 0.1056 Acc: 98.3657\n",
      "\n",
      "Epoch 1121/1199\n",
      "------------------------\n",
      "train Loss: 0.0119 Acc: 99.7859\n",
      "test Loss: 0.1082 Acc: 98.3361\n",
      "\n",
      "Epoch 1122/1199\n",
      "------------------------\n",
      "train Loss: 0.0122 Acc: 99.7828\n",
      "test Loss: 0.1067 Acc: 98.3558\n",
      "\n",
      "Epoch 1123/1199\n",
      "------------------------\n",
      "train Loss: 0.0122 Acc: 99.7657\n",
      "test Loss: 0.1085 Acc: 98.2966\n",
      "\n",
      "Epoch 1124/1199\n",
      "------------------------\n",
      "train Loss: 0.0133 Acc: 99.7604\n",
      "test Loss: 0.1049 Acc: 98.3309\n",
      "\n",
      "Epoch 1125/1199\n",
      "------------------------\n",
      "train Loss: 0.0129 Acc: 99.7633\n",
      "test Loss: 0.1052 Acc: 98.2667\n",
      "\n",
      "Epoch 1126/1199\n",
      "------------------------\n",
      "train Loss: 0.0130 Acc: 99.7655\n",
      "test Loss: 0.1065 Acc: 98.3871\n",
      "\n",
      "Epoch 1127/1199\n",
      "------------------------\n",
      "train Loss: 0.0128 Acc: 99.7700\n",
      "test Loss: 0.1061 Acc: 98.3164\n",
      "\n",
      "Epoch 1128/1199\n",
      "------------------------\n",
      "train Loss: 0.0130 Acc: 99.7606\n",
      "test Loss: 0.1067 Acc: 98.2717\n",
      "\n",
      "Epoch 1129/1199\n",
      "------------------------\n",
      "train Loss: 0.0127 Acc: 99.7656\n",
      "test Loss: 0.1058 Acc: 98.3151\n",
      "\n",
      "Epoch 1130/1199\n",
      "------------------------\n",
      "train Loss: 0.0143 Acc: 99.7235\n",
      "test Loss: 0.1070 Acc: 98.3221\n",
      "\n",
      "Epoch 1131/1199\n",
      "------------------------\n",
      "train Loss: 0.0137 Acc: 99.7504\n",
      "test Loss: 0.1091 Acc: 98.3558\n",
      "\n",
      "Epoch 1132/1199\n",
      "------------------------\n",
      "train Loss: 0.0137 Acc: 99.7500\n",
      "test Loss: 0.1082 Acc: 98.3489\n",
      "\n",
      "Epoch 1133/1199\n",
      "------------------------\n",
      "train Loss: 0.0121 Acc: 99.7803\n",
      "test Loss: 0.1077 Acc: 98.3976\n",
      "\n",
      "Epoch 1134/1199\n",
      "------------------------\n",
      "train Loss: 0.0138 Acc: 99.7471\n",
      "test Loss: 0.1031 Acc: 98.3384\n",
      "\n",
      "Epoch 1135/1199\n",
      "------------------------\n",
      "train Loss: 0.0128 Acc: 99.7729\n",
      "test Loss: 0.1074 Acc: 98.3550\n",
      "\n",
      "Epoch 1136/1199\n",
      "------------------------\n",
      "train Loss: 0.0128 Acc: 99.7640\n",
      "test Loss: 0.1091 Acc: 98.3477\n",
      "\n",
      "Epoch 1137/1199\n",
      "------------------------\n",
      "train Loss: 0.0133 Acc: 99.7561\n",
      "test Loss: 0.1062 Acc: 98.3613\n",
      "\n",
      "Epoch 1138/1199\n",
      "------------------------\n",
      "train Loss: 0.0135 Acc: 99.7579\n",
      "test Loss: 0.1043 Acc: 98.3021\n",
      "\n",
      "Epoch 1139/1199\n",
      "------------------------\n",
      "train Loss: 0.0137 Acc: 99.7538\n",
      "test Loss: 0.1044 Acc: 98.3679\n",
      "\n",
      "Epoch 1140/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7570\n",
      "test Loss: 0.1056 Acc: 98.3149\n",
      "\n",
      "Epoch 1141/1199\n",
      "------------------------\n",
      "train Loss: 0.0122 Acc: 99.7717\n",
      "test Loss: 0.1040 Acc: 98.3598\n",
      "\n",
      "Epoch 1142/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7563\n",
      "test Loss: 0.1087 Acc: 98.2755\n",
      "\n",
      "Epoch 1143/1199\n",
      "------------------------\n",
      "train Loss: 0.0122 Acc: 99.7696\n",
      "test Loss: 0.1070 Acc: 98.3339\n",
      "\n",
      "Epoch 1144/1199\n",
      "------------------------\n",
      "train Loss: 0.0137 Acc: 99.7516\n",
      "test Loss: 0.1041 Acc: 98.3366\n",
      "\n",
      "Epoch 1145/1199\n",
      "------------------------\n",
      "train Loss: 0.0124 Acc: 99.7735\n",
      "test Loss: 0.1065 Acc: 98.3228\n",
      "\n",
      "Epoch 1146/1199\n",
      "------------------------\n",
      "train Loss: 0.0132 Acc: 99.7536\n",
      "test Loss: 0.1050 Acc: 98.3486\n",
      "\n",
      "Epoch 1147/1199\n",
      "------------------------\n",
      "train Loss: 0.0127 Acc: 99.7636\n",
      "test Loss: 0.1073 Acc: 98.3574\n",
      "\n",
      "Epoch 1148/1199\n",
      "------------------------\n",
      "train Loss: 0.0123 Acc: 99.7697\n",
      "test Loss: 0.1075 Acc: 98.3344\n",
      "\n",
      "Epoch 1149/1199\n",
      "------------------------\n",
      "train Loss: 0.0118 Acc: 99.7749\n",
      "test Loss: 0.1082 Acc: 98.3537\n",
      "\n",
      "Epoch 1150/1199\n",
      "------------------------\n",
      "train Loss: 0.0126 Acc: 99.7709\n",
      "test Loss: 0.1061 Acc: 98.3784\n",
      "\n",
      "Epoch 1151/1199\n",
      "------------------------\n",
      "train Loss: 0.0129 Acc: 99.7616\n",
      "test Loss: 0.1079 Acc: 98.3313\n",
      "\n",
      "Epoch 1152/1199\n",
      "------------------------\n",
      "train Loss: 0.0133 Acc: 99.7538\n",
      "test Loss: 0.1058 Acc: 98.3508\n",
      "\n",
      "Epoch 1153/1199\n",
      "------------------------\n",
      "train Loss: 0.0129 Acc: 99.7582\n",
      "test Loss: 0.1088 Acc: 98.3572\n",
      "\n",
      "Epoch 1154/1199\n",
      "------------------------\n",
      "train Loss: 0.0120 Acc: 99.7845\n",
      "test Loss: 0.1065 Acc: 98.3389\n",
      "\n",
      "Epoch 1155/1199\n",
      "------------------------\n",
      "train Loss: 0.0131 Acc: 99.7618\n",
      "test Loss: 0.1107 Acc: 98.3225\n",
      "\n",
      "Epoch 1156/1199\n",
      "------------------------\n",
      "train Loss: 0.0129 Acc: 99.7605\n",
      "test Loss: 0.1067 Acc: 98.3553\n",
      "\n",
      "Epoch 1157/1199\n",
      "------------------------\n",
      "train Loss: 0.0123 Acc: 99.7770\n",
      "test Loss: 0.1074 Acc: 98.3370\n",
      "\n",
      "Epoch 1158/1199\n",
      "------------------------\n",
      "train Loss: 0.0122 Acc: 99.7756\n",
      "test Loss: 0.1073 Acc: 98.3363\n",
      "\n",
      "Epoch 1159/1199\n",
      "------------------------\n",
      "train Loss: 0.0131 Acc: 99.7603\n",
      "test Loss: 0.1067 Acc: 98.3513\n",
      "\n",
      "Epoch 1160/1199\n",
      "------------------------\n",
      "train Loss: 0.0135 Acc: 99.7576\n",
      "test Loss: 0.1082 Acc: 98.3451\n",
      "\n",
      "Epoch 1161/1199\n",
      "------------------------\n",
      "train Loss: 0.0123 Acc: 99.7755\n",
      "test Loss: 0.1050 Acc: 98.3012\n",
      "\n",
      "Epoch 1162/1199\n",
      "------------------------\n",
      "train Loss: 0.0122 Acc: 99.7754\n",
      "test Loss: 0.1093 Acc: 98.3251\n",
      "\n",
      "Epoch 1163/1199\n",
      "------------------------\n",
      "train Loss: 0.0127 Acc: 99.7631\n",
      "test Loss: 0.1061 Acc: 98.3517\n",
      "\n",
      "Epoch 1164/1199\n",
      "------------------------\n",
      "train Loss: 0.0129 Acc: 99.7654\n",
      "test Loss: 0.1071 Acc: 98.3662\n",
      "\n",
      "Epoch 1165/1199\n",
      "------------------------\n",
      "train Loss: 0.0125 Acc: 99.7632\n",
      "test Loss: 0.1071 Acc: 98.3897\n",
      "\n",
      "Epoch 1166/1199\n",
      "------------------------\n",
      "train Loss: 0.0137 Acc: 99.7519\n",
      "test Loss: 0.1052 Acc: 98.3346\n",
      "\n",
      "Epoch 1167/1199\n",
      "------------------------\n",
      "train Loss: 0.0121 Acc: 99.7797\n",
      "test Loss: 0.1078 Acc: 98.3626\n",
      "\n",
      "Epoch 1168/1199\n",
      "------------------------\n",
      "train Loss: 0.0145 Acc: 99.7471\n",
      "test Loss: 0.1071 Acc: 98.3676\n",
      "\n",
      "Epoch 1169/1199\n",
      "------------------------\n",
      "train Loss: 0.0130 Acc: 99.7557\n",
      "test Loss: 0.1066 Acc: 98.3909\n",
      "\n",
      "Epoch 1170/1199\n",
      "------------------------\n",
      "train Loss: 0.0122 Acc: 99.7858\n",
      "test Loss: 0.1056 Acc: 98.3175\n",
      "\n",
      "Epoch 1171/1199\n",
      "------------------------\n",
      "train Loss: 0.0136 Acc: 99.7440\n",
      "test Loss: 0.1068 Acc: 98.3752\n",
      "\n",
      "Epoch 1172/1199\n",
      "------------------------\n",
      "train Loss: 0.0136 Acc: 99.7517\n",
      "test Loss: 0.1080 Acc: 98.2981\n",
      "\n",
      "Epoch 1173/1199\n",
      "------------------------\n",
      "train Loss: 0.0123 Acc: 99.7735\n",
      "test Loss: 0.1067 Acc: 98.3232\n",
      "\n",
      "Epoch 1174/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7571\n",
      "test Loss: 0.1074 Acc: 98.3299\n",
      "\n",
      "Epoch 1175/1199\n",
      "------------------------\n",
      "train Loss: 0.0128 Acc: 99.7729\n",
      "test Loss: 0.1081 Acc: 98.3216\n",
      "\n",
      "Epoch 1176/1199\n",
      "------------------------\n",
      "train Loss: 0.0132 Acc: 99.7632\n",
      "test Loss: 0.1099 Acc: 98.3234\n",
      "\n",
      "Epoch 1177/1199\n",
      "------------------------\n",
      "train Loss: 0.0134 Acc: 99.7529\n",
      "test Loss: 0.1071 Acc: 98.3323\n",
      "\n",
      "Epoch 1178/1199\n",
      "------------------------\n",
      "train Loss: 0.0132 Acc: 99.7533\n",
      "test Loss: 0.1061 Acc: 98.3740\n",
      "\n",
      "Epoch 1179/1199\n",
      "------------------------\n",
      "train Loss: 0.0126 Acc: 99.7717\n",
      "test Loss: 0.1095 Acc: 98.2923\n",
      "\n",
      "Epoch 1180/1199\n",
      "------------------------\n",
      "train Loss: 0.0131 Acc: 99.7594\n",
      "test Loss: 0.1114 Acc: 98.3631\n",
      "\n",
      "Epoch 1181/1199\n",
      "------------------------\n",
      "train Loss: 0.0127 Acc: 99.7719\n",
      "test Loss: 0.1061 Acc: 98.3527\n",
      "\n",
      "Epoch 1182/1199\n",
      "------------------------\n",
      "train Loss: 0.0129 Acc: 99.7588\n",
      "test Loss: 0.1049 Acc: 98.3304\n",
      "\n",
      "Epoch 1183/1199\n",
      "------------------------\n",
      "train Loss: 0.0127 Acc: 99.7659\n",
      "test Loss: 0.1066 Acc: 98.3225\n",
      "\n",
      "Epoch 1184/1199\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0124 Acc: 99.7702\n",
      "test Loss: 0.1100 Acc: 98.3463\n",
      "\n",
      "Epoch 1185/1199\n",
      "------------------------\n",
      "train Loss: 0.0130 Acc: 99.7646\n",
      "test Loss: 0.1073 Acc: 98.3069\n",
      "\n",
      "Epoch 1186/1199\n",
      "------------------------\n",
      "train Loss: 0.0128 Acc: 99.7693\n",
      "test Loss: 0.1073 Acc: 98.3608\n",
      "\n",
      "Epoch 1187/1199\n",
      "------------------------\n",
      "train Loss: 0.0120 Acc: 99.7864\n",
      "test Loss: 0.1067 Acc: 98.3575\n",
      "\n",
      "Epoch 1188/1199\n",
      "------------------------\n",
      "train Loss: 0.0130 Acc: 99.7578\n",
      "test Loss: 0.1057 Acc: 98.3185\n",
      "\n",
      "Epoch 1189/1199\n",
      "------------------------\n",
      "train Loss: 0.0132 Acc: 99.7592\n",
      "test Loss: 0.1128 Acc: 98.3056\n",
      "\n",
      "Epoch 1190/1199\n",
      "------------------------\n",
      "train Loss: 0.0136 Acc: 99.7523\n",
      "test Loss: 0.1048 Acc: 98.3297\n",
      "\n",
      "Epoch 1191/1199\n",
      "------------------------\n",
      "train Loss: 0.0118 Acc: 99.7786\n",
      "test Loss: 0.1076 Acc: 98.3836\n",
      "\n",
      "Epoch 1192/1199\n",
      "------------------------\n",
      "train Loss: 0.0128 Acc: 99.7683\n",
      "test Loss: 0.1087 Acc: 98.3551\n",
      "\n",
      "Epoch 1193/1199\n",
      "------------------------\n",
      "train Loss: 0.0129 Acc: 99.7594\n",
      "test Loss: 0.1084 Acc: 98.3719\n",
      "\n",
      "Epoch 1194/1199\n",
      "------------------------\n",
      "train Loss: 0.0125 Acc: 99.7704\n",
      "test Loss: 0.1063 Acc: 98.3453\n",
      "\n",
      "Epoch 1195/1199\n",
      "------------------------\n",
      "train Loss: 0.0118 Acc: 99.7895\n",
      "test Loss: 0.1067 Acc: 98.3622\n",
      "\n",
      "Epoch 1196/1199\n",
      "------------------------\n",
      "train Loss: 0.0117 Acc: 99.7847\n",
      "test Loss: 0.1057 Acc: 98.3586\n",
      "\n",
      "Epoch 1197/1199\n",
      "------------------------\n",
      "train Loss: 0.0125 Acc: 99.7691\n",
      "test Loss: 0.1107 Acc: 98.3242\n",
      "\n",
      "Epoch 1198/1199\n",
      "------------------------\n",
      "train Loss: 0.0136 Acc: 99.7459\n",
      "test Loss: 0.1081 Acc: 98.3664\n",
      "\n",
      "Epoch 1199/1199\n",
      "------------------------\n",
      "train Loss: 0.0131 Acc: 99.7593\n",
      "test Loss: 0.1060 Acc: 98.3278\n",
      "\n",
      "Training complete in 187m 11s\n",
      "Best val Acc: 98.397615\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa74c0eaeb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type Model1. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "model_adam = mac_train_model(model=model_adam, criterion=criterion, optimizer=optimizer_adam, \n",
    "                          scheduler=exp_lr_scheduler, num_epochs=1200)\n",
    "torch.save(model_adam, './data/model_adam_lstm.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train with Adam with GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1199\n",
      "------------------------\n",
      "train Loss: 3.3892 Acc: 42.2490\n",
      "test Loss: 2.2591 Acc: 55.5413\n",
      "\n",
      "Epoch 1/1199\n",
      "------------------------\n",
      "train Loss: 2.0709 Acc: 57.7959\n",
      "test Loss: 1.8925 Acc: 59.8556\n",
      "\n",
      "Epoch 2/1199\n",
      "------------------------\n",
      "train Loss: 1.7759 Acc: 61.4404\n",
      "test Loss: 1.6381 Acc: 63.6080\n",
      "\n",
      "Epoch 3/1199\n",
      "------------------------\n",
      "train Loss: 1.5620 Acc: 64.6518\n",
      "test Loss: 1.5139 Acc: 65.6121\n",
      "\n",
      "Epoch 4/1199\n",
      "------------------------\n",
      "train Loss: 1.4017 Acc: 67.3450\n",
      "test Loss: 1.3413 Acc: 68.1970\n",
      "\n",
      "Epoch 5/1199\n",
      "------------------------\n",
      "train Loss: 1.2966 Acc: 69.5813\n",
      "test Loss: 1.2185 Acc: 70.8312\n",
      "\n",
      "Epoch 6/1199\n",
      "------------------------\n",
      "train Loss: 1.1737 Acc: 71.8680\n",
      "test Loss: 1.1412 Acc: 72.6961\n",
      "\n",
      "Epoch 7/1199\n",
      "------------------------\n",
      "train Loss: 1.0999 Acc: 73.7919\n",
      "test Loss: 1.0194 Acc: 74.9225\n",
      "\n",
      "Epoch 8/1199\n",
      "------------------------\n",
      "train Loss: 0.9863 Acc: 76.2859\n",
      "test Loss: 0.9796 Acc: 76.5886\n",
      "\n",
      "Epoch 9/1199\n",
      "------------------------\n",
      "train Loss: 0.9230 Acc: 77.7390\n",
      "test Loss: 0.8832 Acc: 78.7084\n",
      "\n",
      "Epoch 10/1199\n",
      "------------------------\n",
      "train Loss: 0.8593 Acc: 79.2728\n",
      "test Loss: 0.8374 Acc: 80.1141\n",
      "\n",
      "Epoch 11/1199\n",
      "------------------------\n",
      "train Loss: 0.8044 Acc: 80.7479\n",
      "test Loss: 0.7846 Acc: 81.0035\n",
      "\n",
      "Epoch 12/1199\n",
      "------------------------\n",
      "train Loss: 0.7679 Acc: 81.8074\n",
      "test Loss: 0.7570 Acc: 81.9736\n",
      "\n",
      "Epoch 13/1199\n",
      "------------------------\n",
      "train Loss: 0.7189 Acc: 82.8130\n",
      "test Loss: 0.7067 Acc: 83.0641\n",
      "\n",
      "Epoch 14/1199\n",
      "------------------------\n",
      "train Loss: 0.6731 Acc: 83.9468\n",
      "test Loss: 0.6758 Acc: 83.9917\n",
      "\n",
      "Epoch 15/1199\n",
      "------------------------\n",
      "train Loss: 0.6450 Acc: 84.7106\n",
      "test Loss: 0.6296 Acc: 85.1091\n",
      "\n",
      "Epoch 16/1199\n",
      "------------------------\n",
      "train Loss: 0.6128 Acc: 85.5341\n",
      "test Loss: 0.5902 Acc: 85.7980\n",
      "\n",
      "Epoch 17/1199\n",
      "------------------------\n",
      "train Loss: 0.5758 Acc: 86.2956\n",
      "test Loss: 0.5888 Acc: 86.2481\n",
      "\n",
      "Epoch 18/1199\n",
      "------------------------\n",
      "train Loss: 0.5496 Acc: 86.9298\n",
      "test Loss: 0.5397 Acc: 87.0120\n",
      "\n",
      "Epoch 19/1199\n",
      "------------------------\n",
      "train Loss: 0.5301 Acc: 87.5047\n",
      "test Loss: 0.5363 Acc: 87.3661\n",
      "\n",
      "Epoch 20/1199\n",
      "------------------------\n",
      "train Loss: 0.5072 Acc: 88.1285\n",
      "test Loss: 0.5165 Acc: 87.6878\n",
      "\n",
      "Epoch 21/1199\n",
      "------------------------\n",
      "train Loss: 0.4884 Acc: 88.5028\n",
      "test Loss: 0.4770 Acc: 88.5040\n",
      "\n",
      "Epoch 22/1199\n",
      "------------------------\n",
      "train Loss: 0.4608 Acc: 89.2397\n",
      "test Loss: 0.4552 Acc: 89.2189\n",
      "\n",
      "Epoch 23/1199\n",
      "------------------------\n",
      "train Loss: 0.4477 Acc: 89.5632\n",
      "test Loss: 0.4297 Acc: 89.7515\n",
      "\n",
      "Epoch 24/1199\n",
      "------------------------\n",
      "train Loss: 0.4296 Acc: 90.0057\n",
      "test Loss: 0.4280 Acc: 90.0275\n",
      "\n",
      "Epoch 25/1199\n",
      "------------------------\n",
      "train Loss: 0.4081 Acc: 90.5122\n",
      "test Loss: 0.4179 Acc: 90.2689\n",
      "\n",
      "Epoch 26/1199\n",
      "------------------------\n",
      "train Loss: 0.3999 Acc: 90.7314\n",
      "test Loss: 0.3884 Acc: 90.9190\n",
      "\n",
      "Epoch 27/1199\n",
      "------------------------\n",
      "train Loss: 0.3770 Acc: 91.2776\n",
      "test Loss: 0.3793 Acc: 91.1147\n",
      "\n",
      "Epoch 28/1199\n",
      "------------------------\n",
      "train Loss: 0.3625 Acc: 91.5955\n",
      "test Loss: 0.3851 Acc: 90.9969\n",
      "\n",
      "Epoch 29/1199\n",
      "------------------------\n",
      "train Loss: 0.3585 Acc: 91.7568\n",
      "test Loss: 0.3666 Acc: 91.4971\n",
      "\n",
      "Epoch 30/1199\n",
      "------------------------\n",
      "train Loss: 0.3483 Acc: 92.0465\n",
      "test Loss: 0.3660 Acc: 91.5242\n",
      "\n",
      "Epoch 31/1199\n",
      "------------------------\n",
      "train Loss: 0.3345 Acc: 92.2638\n",
      "test Loss: 0.3429 Acc: 92.0359\n",
      "\n",
      "Epoch 32/1199\n",
      "------------------------\n",
      "train Loss: 0.3250 Acc: 92.5839\n",
      "test Loss: 0.3390 Acc: 92.0972\n",
      "\n",
      "Epoch 33/1199\n",
      "------------------------\n",
      "train Loss: 0.3184 Acc: 92.7320\n",
      "test Loss: 0.3320 Acc: 92.3986\n",
      "\n",
      "Epoch 34/1199\n",
      "------------------------\n",
      "train Loss: 0.3118 Acc: 92.8441\n",
      "test Loss: 0.3222 Acc: 92.7501\n",
      "\n",
      "Epoch 35/1199\n",
      "------------------------\n",
      "train Loss: 0.3067 Acc: 93.0092\n",
      "test Loss: 0.3194 Acc: 92.6979\n",
      "\n",
      "Epoch 36/1199\n",
      "------------------------\n",
      "train Loss: 0.2958 Acc: 93.3173\n",
      "test Loss: 0.3055 Acc: 92.9086\n",
      "\n",
      "Epoch 37/1199\n",
      "------------------------\n",
      "train Loss: 0.2821 Acc: 93.5572\n",
      "test Loss: 0.2958 Acc: 93.1853\n",
      "\n",
      "Epoch 38/1199\n",
      "------------------------\n",
      "train Loss: 0.2804 Acc: 93.6286\n",
      "test Loss: 0.2827 Acc: 93.5233\n",
      "\n",
      "Epoch 39/1199\n",
      "------------------------\n",
      "train Loss: 0.2646 Acc: 93.9560\n",
      "test Loss: 0.2872 Acc: 93.5305\n",
      "\n",
      "Epoch 40/1199\n",
      "------------------------\n",
      "train Loss: 0.2704 Acc: 93.8443\n",
      "test Loss: 0.2988 Acc: 93.1870\n",
      "\n",
      "Epoch 41/1199\n",
      "------------------------\n",
      "train Loss: 0.2807 Acc: 93.6674\n",
      "test Loss: 0.2829 Acc: 93.6519\n",
      "\n",
      "Epoch 42/1199\n",
      "------------------------\n",
      "train Loss: 0.2587 Acc: 94.1493\n",
      "test Loss: 0.2688 Acc: 93.8468\n",
      "\n",
      "Epoch 43/1199\n",
      "------------------------\n",
      "train Loss: 0.2561 Acc: 94.1423\n",
      "test Loss: 0.2677 Acc: 93.9055\n",
      "\n",
      "Epoch 44/1199\n",
      "------------------------\n",
      "train Loss: 0.2460 Acc: 94.4272\n",
      "test Loss: 0.2622 Acc: 94.0303\n",
      "\n",
      "Epoch 45/1199\n",
      "------------------------\n",
      "train Loss: 0.2375 Acc: 94.5536\n",
      "test Loss: 0.2610 Acc: 94.0739\n",
      "\n",
      "Epoch 46/1199\n",
      "------------------------\n",
      "train Loss: 0.2394 Acc: 94.5812\n",
      "test Loss: 0.2523 Acc: 94.2768\n",
      "\n",
      "Epoch 47/1199\n",
      "------------------------\n",
      "train Loss: 0.2337 Acc: 94.6895\n",
      "test Loss: 0.2482 Acc: 94.3297\n",
      "\n",
      "Epoch 48/1199\n",
      "------------------------\n",
      "train Loss: 0.2344 Acc: 94.7003\n",
      "test Loss: 0.2536 Acc: 94.2044\n",
      "\n",
      "Epoch 49/1199\n",
      "------------------------\n",
      "train Loss: 0.2229 Acc: 94.8951\n",
      "test Loss: 0.2702 Acc: 93.8744\n",
      "\n",
      "Epoch 50/1199\n",
      "------------------------\n",
      "train Loss: 0.2256 Acc: 94.8693\n",
      "test Loss: 0.2484 Acc: 94.3027\n",
      "\n",
      "Epoch 51/1199\n",
      "------------------------\n",
      "train Loss: 0.2186 Acc: 95.0254\n",
      "test Loss: 0.2385 Acc: 94.5227\n",
      "\n",
      "Epoch 52/1199\n",
      "------------------------\n",
      "train Loss: 0.2218 Acc: 94.9583\n",
      "test Loss: 0.2338 Acc: 94.6725\n",
      "\n",
      "Epoch 53/1199\n",
      "------------------------\n",
      "train Loss: 0.2115 Acc: 95.1828\n",
      "test Loss: 0.2294 Acc: 94.6701\n",
      "\n",
      "Epoch 54/1199\n",
      "------------------------\n",
      "train Loss: 0.2144 Acc: 95.0733\n",
      "test Loss: 0.2163 Acc: 95.0702\n",
      "\n",
      "Epoch 55/1199\n",
      "------------------------\n",
      "train Loss: 0.2087 Acc: 95.2854\n",
      "test Loss: 0.2148 Acc: 95.0155\n",
      "\n",
      "Epoch 56/1199\n",
      "------------------------\n",
      "train Loss: 0.2018 Acc: 95.3951\n",
      "test Loss: 0.2168 Acc: 95.0238\n",
      "\n",
      "Epoch 57/1199\n",
      "------------------------\n",
      "train Loss: 0.1995 Acc: 95.4757\n",
      "test Loss: 0.2180 Acc: 95.0397\n",
      "\n",
      "Epoch 58/1199\n",
      "------------------------\n",
      "train Loss: 0.1980 Acc: 95.5111\n",
      "test Loss: 0.2254 Acc: 94.7740\n",
      "\n",
      "Epoch 59/1199\n",
      "------------------------\n",
      "train Loss: 0.2003 Acc: 95.4490\n",
      "test Loss: 0.2292 Acc: 94.7555\n",
      "\n",
      "Epoch 60/1199\n",
      "------------------------\n",
      "train Loss: 0.1925 Acc: 95.5786\n",
      "test Loss: 0.2186 Acc: 94.9573\n",
      "\n",
      "Epoch 61/1199\n",
      "------------------------\n",
      "train Loss: 0.1857 Acc: 95.7281\n",
      "test Loss: 0.2157 Acc: 95.0901\n",
      "\n",
      "Epoch 62/1199\n",
      "------------------------\n",
      "train Loss: 0.1858 Acc: 95.7425\n",
      "test Loss: 0.2133 Acc: 95.1466\n",
      "\n",
      "Epoch 63/1199\n",
      "------------------------\n",
      "train Loss: 0.1840 Acc: 95.8223\n",
      "test Loss: 0.1966 Acc: 95.4742\n",
      "\n",
      "Epoch 64/1199\n",
      "------------------------\n",
      "train Loss: 0.1779 Acc: 95.9202\n",
      "test Loss: 0.2048 Acc: 95.3763\n",
      "\n",
      "Epoch 65/1199\n",
      "------------------------\n",
      "train Loss: 0.1775 Acc: 95.9487\n",
      "test Loss: 0.1983 Acc: 95.4709\n",
      "\n",
      "Epoch 66/1199\n",
      "------------------------\n",
      "train Loss: 0.1808 Acc: 95.8954\n",
      "test Loss: 0.2027 Acc: 95.3523\n",
      "\n",
      "Epoch 67/1199\n",
      "------------------------\n",
      "train Loss: 0.1757 Acc: 95.9790\n",
      "test Loss: 0.2000 Acc: 95.3415\n",
      "\n",
      "Epoch 68/1199\n",
      "------------------------\n",
      "train Loss: 0.1709 Acc: 96.0340\n",
      "test Loss: 0.1934 Acc: 95.5650\n",
      "\n",
      "Epoch 69/1199\n",
      "------------------------\n",
      "train Loss: 0.1701 Acc: 96.0792\n",
      "test Loss: 0.2007 Acc: 95.3920\n",
      "\n",
      "Epoch 70/1199\n",
      "------------------------\n",
      "train Loss: 0.1717 Acc: 96.1090\n",
      "test Loss: 0.2014 Acc: 95.3353\n",
      "\n",
      "Epoch 71/1199\n",
      "------------------------\n",
      "train Loss: 0.1694 Acc: 96.1025\n",
      "test Loss: 0.1946 Acc: 95.5424\n",
      "\n",
      "Epoch 72/1199\n",
      "------------------------\n",
      "train Loss: 0.1649 Acc: 96.1819\n",
      "test Loss: 0.1928 Acc: 95.6227\n",
      "\n",
      "Epoch 73/1199\n",
      "------------------------\n",
      "train Loss: 0.1631 Acc: 96.1881\n",
      "test Loss: 0.1953 Acc: 95.4864\n",
      "\n",
      "Epoch 74/1199\n",
      "------------------------\n",
      "train Loss: 0.1630 Acc: 96.2171\n",
      "test Loss: 0.1940 Acc: 95.5695\n",
      "\n",
      "Epoch 75/1199\n",
      "------------------------\n",
      "train Loss: 0.1621 Acc: 96.3023\n",
      "test Loss: 0.1824 Acc: 95.7042\n",
      "\n",
      "Epoch 76/1199\n",
      "------------------------\n",
      "train Loss: 0.1556 Acc: 96.3923\n",
      "test Loss: 0.1864 Acc: 95.6664\n",
      "\n",
      "Epoch 77/1199\n",
      "------------------------\n",
      "train Loss: 0.1544 Acc: 96.4068\n",
      "test Loss: 0.2007 Acc: 95.4795\n",
      "\n",
      "Epoch 78/1199\n",
      "------------------------\n",
      "train Loss: 0.1538 Acc: 96.4393\n",
      "test Loss: 0.1802 Acc: 95.8246\n",
      "\n",
      "Epoch 79/1199\n",
      "------------------------\n",
      "train Loss: 0.1510 Acc: 96.4881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1778 Acc: 95.8312\n",
      "\n",
      "Epoch 80/1199\n",
      "------------------------\n",
      "train Loss: 0.1542 Acc: 96.4293\n",
      "test Loss: 0.1790 Acc: 95.7956\n",
      "\n",
      "Epoch 81/1199\n",
      "------------------------\n",
      "train Loss: 0.1482 Acc: 96.5681\n",
      "test Loss: 0.1863 Acc: 95.7415\n",
      "\n",
      "Epoch 82/1199\n",
      "------------------------\n",
      "train Loss: 0.1460 Acc: 96.6127\n",
      "test Loss: 0.1802 Acc: 95.8351\n",
      "\n",
      "Epoch 83/1199\n",
      "------------------------\n",
      "train Loss: 0.1507 Acc: 96.4919\n",
      "test Loss: 0.1727 Acc: 95.9612\n",
      "\n",
      "Epoch 84/1199\n",
      "------------------------\n",
      "train Loss: 0.1457 Acc: 96.6321\n",
      "test Loss: 0.1821 Acc: 95.8576\n",
      "\n",
      "Epoch 85/1199\n",
      "------------------------\n",
      "train Loss: 0.1415 Acc: 96.7018\n",
      "test Loss: 0.1780 Acc: 95.9557\n",
      "\n",
      "Epoch 86/1199\n",
      "------------------------\n",
      "train Loss: 0.1451 Acc: 96.5939\n",
      "test Loss: 0.1785 Acc: 95.8566\n",
      "\n",
      "Epoch 87/1199\n",
      "------------------------\n",
      "train Loss: 0.1405 Acc: 96.7277\n",
      "test Loss: 0.1772 Acc: 95.9298\n",
      "\n",
      "Epoch 88/1199\n",
      "------------------------\n",
      "train Loss: 0.1423 Acc: 96.6752\n",
      "test Loss: 0.1775 Acc: 95.8849\n",
      "\n",
      "Epoch 89/1199\n",
      "------------------------\n",
      "train Loss: 0.1408 Acc: 96.7213\n",
      "test Loss: 0.1717 Acc: 96.0006\n",
      "\n",
      "Epoch 90/1199\n",
      "------------------------\n",
      "train Loss: 0.1436 Acc: 96.6682\n",
      "test Loss: 0.1726 Acc: 96.0082\n",
      "\n",
      "Epoch 91/1199\n",
      "------------------------\n",
      "train Loss: 0.1358 Acc: 96.8428\n",
      "test Loss: 0.1727 Acc: 96.0460\n",
      "\n",
      "Epoch 92/1199\n",
      "------------------------\n",
      "train Loss: 0.1375 Acc: 96.7818\n",
      "test Loss: 0.1701 Acc: 96.1293\n",
      "\n",
      "Epoch 93/1199\n",
      "------------------------\n",
      "train Loss: 0.1414 Acc: 96.6746\n",
      "test Loss: 0.1680 Acc: 96.1730\n",
      "\n",
      "Epoch 94/1199\n",
      "------------------------\n",
      "train Loss: 0.1389 Acc: 96.7722\n",
      "test Loss: 0.1652 Acc: 96.1961\n",
      "\n",
      "Epoch 95/1199\n",
      "------------------------\n",
      "train Loss: 0.1346 Acc: 96.8432\n",
      "test Loss: 0.1690 Acc: 96.1160\n",
      "\n",
      "Epoch 96/1199\n",
      "------------------------\n",
      "train Loss: 0.1319 Acc: 96.9224\n",
      "test Loss: 0.1596 Acc: 96.3536\n",
      "\n",
      "Epoch 97/1199\n",
      "------------------------\n",
      "train Loss: 0.1314 Acc: 96.9623\n",
      "test Loss: 0.1650 Acc: 96.1193\n",
      "\n",
      "Epoch 98/1199\n",
      "------------------------\n",
      "train Loss: 0.1339 Acc: 96.8735\n",
      "test Loss: 0.1617 Acc: 96.2920\n",
      "\n",
      "Epoch 99/1199\n",
      "------------------------\n",
      "train Loss: 0.1301 Acc: 96.9344\n",
      "test Loss: 0.1606 Acc: 96.2699\n",
      "\n",
      "Epoch 100/1199\n",
      "------------------------\n",
      "train Loss: 0.1248 Acc: 97.0848\n",
      "test Loss: 0.1736 Acc: 95.9495\n",
      "\n",
      "Epoch 101/1199\n",
      "------------------------\n",
      "train Loss: 0.1257 Acc: 97.0432\n",
      "test Loss: 0.1566 Acc: 96.3831\n",
      "\n",
      "Epoch 102/1199\n",
      "------------------------\n",
      "train Loss: 0.1273 Acc: 97.0264\n",
      "test Loss: 0.1547 Acc: 96.3902\n",
      "\n",
      "Epoch 103/1199\n",
      "------------------------\n",
      "train Loss: 0.1285 Acc: 96.9824\n",
      "test Loss: 0.1638 Acc: 96.1889\n",
      "\n",
      "Epoch 104/1199\n",
      "------------------------\n",
      "train Loss: 0.1234 Acc: 97.0767\n",
      "test Loss: 0.1575 Acc: 96.3054\n",
      "\n",
      "Epoch 105/1199\n",
      "------------------------\n",
      "train Loss: 0.1272 Acc: 97.0064\n",
      "test Loss: 0.1591 Acc: 96.3211\n",
      "\n",
      "Epoch 106/1199\n",
      "------------------------\n",
      "train Loss: 0.1213 Acc: 97.1433\n",
      "test Loss: 0.1508 Acc: 96.4795\n",
      "\n",
      "Epoch 107/1199\n",
      "------------------------\n",
      "train Loss: 0.1201 Acc: 97.1934\n",
      "test Loss: 0.1622 Acc: 96.3101\n",
      "\n",
      "Epoch 108/1199\n",
      "------------------------\n",
      "train Loss: 0.1177 Acc: 97.2310\n",
      "test Loss: 0.1599 Acc: 96.3659\n",
      "\n",
      "Epoch 109/1199\n",
      "------------------------\n",
      "train Loss: 0.1191 Acc: 97.2077\n",
      "test Loss: 0.1549 Acc: 96.4771\n",
      "\n",
      "Epoch 110/1199\n",
      "------------------------\n",
      "train Loss: 0.1174 Acc: 97.2419\n",
      "test Loss: 0.1591 Acc: 96.2666\n",
      "\n",
      "Epoch 111/1199\n",
      "------------------------\n",
      "train Loss: 0.1162 Acc: 97.2344\n",
      "test Loss: 0.1537 Acc: 96.4804\n",
      "\n",
      "Epoch 112/1199\n",
      "------------------------\n",
      "train Loss: 0.1169 Acc: 97.2396\n",
      "test Loss: 0.1621 Acc: 96.2597\n",
      "\n",
      "Epoch 113/1199\n",
      "------------------------\n",
      "train Loss: 0.1141 Acc: 97.2890\n",
      "test Loss: 0.1512 Acc: 96.5624\n",
      "\n",
      "Epoch 114/1199\n",
      "------------------------\n",
      "train Loss: 0.1106 Acc: 97.3676\n",
      "test Loss: 0.1459 Acc: 96.6087\n",
      "\n",
      "Epoch 115/1199\n",
      "------------------------\n",
      "train Loss: 0.1173 Acc: 97.2001\n",
      "test Loss: 0.1461 Acc: 96.5622\n",
      "\n",
      "Epoch 116/1199\n",
      "------------------------\n",
      "train Loss: 0.1157 Acc: 97.2456\n",
      "test Loss: 0.1572 Acc: 96.4255\n",
      "\n",
      "Epoch 117/1199\n",
      "------------------------\n",
      "train Loss: 0.1109 Acc: 97.3587\n",
      "test Loss: 0.1542 Acc: 96.3560\n",
      "\n",
      "Epoch 118/1199\n",
      "------------------------\n",
      "train Loss: 0.1152 Acc: 97.2517\n",
      "test Loss: 0.1516 Acc: 96.4531\n",
      "\n",
      "Epoch 119/1199\n",
      "------------------------\n",
      "train Loss: 0.1141 Acc: 97.3316\n",
      "test Loss: 0.1446 Acc: 96.6215\n",
      "\n",
      "Epoch 120/1199\n",
      "------------------------\n",
      "train Loss: 0.1139 Acc: 97.3023\n",
      "test Loss: 0.1507 Acc: 96.5391\n",
      "\n",
      "Epoch 121/1199\n",
      "------------------------\n",
      "train Loss: 0.1096 Acc: 97.4076\n",
      "test Loss: 0.1498 Acc: 96.5868\n",
      "\n",
      "Epoch 122/1199\n",
      "------------------------\n",
      "train Loss: 0.1070 Acc: 97.4613\n",
      "test Loss: 0.1362 Acc: 96.8163\n",
      "\n",
      "Epoch 123/1199\n",
      "------------------------\n",
      "train Loss: 0.1103 Acc: 97.3825\n",
      "test Loss: 0.1501 Acc: 96.4731\n",
      "\n",
      "Epoch 124/1199\n",
      "------------------------\n",
      "train Loss: 0.1070 Acc: 97.4409\n",
      "test Loss: 0.1524 Acc: 96.4591\n",
      "\n",
      "Epoch 125/1199\n",
      "------------------------\n",
      "train Loss: 0.1066 Acc: 97.4557\n",
      "test Loss: 0.1498 Acc: 96.5768\n",
      "\n",
      "Epoch 126/1199\n",
      "------------------------\n",
      "train Loss: 0.1098 Acc: 97.3847\n",
      "test Loss: 0.1441 Acc: 96.6322\n",
      "\n",
      "Epoch 127/1199\n",
      "------------------------\n",
      "train Loss: 0.1067 Acc: 97.4439\n",
      "test Loss: 0.1424 Acc: 96.6880\n",
      "\n",
      "Epoch 128/1199\n",
      "------------------------\n",
      "train Loss: 0.1081 Acc: 97.4554\n",
      "test Loss: 0.1495 Acc: 96.5458\n",
      "\n",
      "Epoch 129/1199\n",
      "------------------------\n",
      "train Loss: 0.1083 Acc: 97.4397\n",
      "test Loss: 0.1470 Acc: 96.5293\n",
      "\n",
      "Epoch 130/1199\n",
      "------------------------\n",
      "train Loss: 0.1074 Acc: 97.4785\n",
      "test Loss: 0.1543 Acc: 96.5087\n",
      "\n",
      "Epoch 131/1199\n",
      "------------------------\n",
      "train Loss: 0.1010 Acc: 97.5794\n",
      "test Loss: 0.1474 Acc: 96.5462\n",
      "\n",
      "Epoch 132/1199\n",
      "------------------------\n",
      "train Loss: 0.1050 Acc: 97.5052\n",
      "test Loss: 0.1388 Acc: 96.7712\n",
      "\n",
      "Epoch 133/1199\n",
      "------------------------\n",
      "train Loss: 0.1061 Acc: 97.4914\n",
      "test Loss: 0.1379 Acc: 96.8745\n",
      "\n",
      "Epoch 134/1199\n",
      "------------------------\n",
      "train Loss: 0.0984 Acc: 97.6775\n",
      "test Loss: 0.1501 Acc: 96.5678\n",
      "\n",
      "Epoch 135/1199\n",
      "------------------------\n",
      "train Loss: 0.0990 Acc: 97.6570\n",
      "test Loss: 0.1391 Acc: 96.7469\n",
      "\n",
      "Epoch 136/1199\n",
      "------------------------\n",
      "train Loss: 0.1008 Acc: 97.6162\n",
      "test Loss: 0.1476 Acc: 96.6021\n",
      "\n",
      "Epoch 137/1199\n",
      "------------------------\n",
      "train Loss: 0.1089 Acc: 97.3795\n",
      "test Loss: 0.1392 Acc: 96.8075\n",
      "\n",
      "Epoch 138/1199\n",
      "------------------------\n",
      "train Loss: 0.1010 Acc: 97.5871\n",
      "test Loss: 0.1372 Acc: 96.8583\n",
      "\n",
      "Epoch 139/1199\n",
      "------------------------\n",
      "train Loss: 0.1028 Acc: 97.5801\n",
      "test Loss: 0.1396 Acc: 96.8379\n",
      "\n",
      "Epoch 140/1199\n",
      "------------------------\n",
      "train Loss: 0.1011 Acc: 97.5996\n",
      "test Loss: 0.1481 Acc: 96.5999\n",
      "\n",
      "Epoch 141/1199\n",
      "------------------------\n",
      "train Loss: 0.1011 Acc: 97.5855\n",
      "test Loss: 0.1348 Acc: 96.8847\n",
      "\n",
      "Epoch 142/1199\n",
      "------------------------\n",
      "train Loss: 0.1001 Acc: 97.6412\n",
      "test Loss: 0.1389 Acc: 96.8728\n",
      "\n",
      "Epoch 143/1199\n",
      "------------------------\n",
      "train Loss: 0.0964 Acc: 97.7272\n",
      "test Loss: 0.1362 Acc: 96.8978\n",
      "\n",
      "Epoch 144/1199\n",
      "------------------------\n",
      "train Loss: 0.0968 Acc: 97.6931\n",
      "test Loss: 0.1429 Acc: 96.6920\n",
      "\n",
      "Epoch 145/1199\n",
      "------------------------\n",
      "train Loss: 0.0945 Acc: 97.7742\n",
      "test Loss: 0.1402 Acc: 96.8054\n",
      "\n",
      "Epoch 146/1199\n",
      "------------------------\n",
      "train Loss: 0.0927 Acc: 97.7818\n",
      "test Loss: 0.1348 Acc: 96.9445\n",
      "\n",
      "Epoch 147/1199\n",
      "------------------------\n",
      "train Loss: 0.0941 Acc: 97.7370\n",
      "test Loss: 0.1386 Acc: 96.8229\n",
      "\n",
      "Epoch 148/1199\n",
      "------------------------\n",
      "train Loss: 0.0974 Acc: 97.6836\n",
      "test Loss: 0.1349 Acc: 96.8989\n",
      "\n",
      "Epoch 149/1199\n",
      "------------------------\n",
      "train Loss: 0.0932 Acc: 97.7789\n",
      "test Loss: 0.1374 Acc: 96.7612\n",
      "\n",
      "Epoch 150/1199\n",
      "------------------------\n",
      "train Loss: 0.0894 Acc: 97.8586\n",
      "test Loss: 0.1402 Acc: 96.8059\n",
      "\n",
      "Epoch 151/1199\n",
      "------------------------\n",
      "train Loss: 0.0969 Acc: 97.6706\n",
      "test Loss: 0.1376 Acc: 96.8862\n",
      "\n",
      "Epoch 152/1199\n",
      "------------------------\n",
      "train Loss: 0.0908 Acc: 97.8355\n",
      "test Loss: 0.1334 Acc: 96.9683\n",
      "\n",
      "Epoch 153/1199\n",
      "------------------------\n",
      "train Loss: 0.0921 Acc: 97.7984\n",
      "test Loss: 0.1374 Acc: 96.8486\n",
      "\n",
      "Epoch 154/1199\n",
      "------------------------\n",
      "train Loss: 0.0896 Acc: 97.8547\n",
      "test Loss: 0.1337 Acc: 97.0296\n",
      "\n",
      "Epoch 155/1199\n",
      "------------------------\n",
      "train Loss: 0.0911 Acc: 97.8570\n",
      "test Loss: 0.1359 Acc: 96.9305\n",
      "\n",
      "Epoch 156/1199\n",
      "------------------------\n",
      "train Loss: 0.0881 Acc: 97.8974\n",
      "test Loss: 0.1308 Acc: 97.0142\n",
      "\n",
      "Epoch 157/1199\n",
      "------------------------\n",
      "train Loss: 0.0852 Acc: 97.9769\n",
      "test Loss: 0.1370 Acc: 96.8880\n",
      "\n",
      "Epoch 158/1199\n",
      "------------------------\n",
      "train Loss: 0.0893 Acc: 97.8788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1352 Acc: 96.9621\n",
      "\n",
      "Epoch 159/1199\n",
      "------------------------\n",
      "train Loss: 0.0869 Acc: 97.9369\n",
      "test Loss: 0.1419 Acc: 96.7505\n",
      "\n",
      "Epoch 160/1199\n",
      "------------------------\n",
      "train Loss: 0.0999 Acc: 97.5753\n",
      "test Loss: 0.1469 Acc: 96.6156\n",
      "\n",
      "Epoch 161/1199\n",
      "------------------------\n",
      "train Loss: 0.0958 Acc: 97.6761\n",
      "test Loss: 0.1407 Acc: 96.7135\n",
      "\n",
      "Epoch 162/1199\n",
      "------------------------\n",
      "train Loss: 0.0880 Acc: 97.8899\n",
      "test Loss: 0.1309 Acc: 96.9862\n",
      "\n",
      "Epoch 163/1199\n",
      "------------------------\n",
      "train Loss: 0.0883 Acc: 97.8823\n",
      "test Loss: 0.1343 Acc: 96.8906\n",
      "\n",
      "Epoch 164/1199\n",
      "------------------------\n",
      "train Loss: 0.0854 Acc: 97.9711\n",
      "test Loss: 0.1293 Acc: 97.0615\n",
      "\n",
      "Epoch 165/1199\n",
      "------------------------\n",
      "train Loss: 0.0824 Acc: 98.0294\n",
      "test Loss: 0.1298 Acc: 97.0824\n",
      "\n",
      "Epoch 166/1199\n",
      "------------------------\n",
      "train Loss: 0.0824 Acc: 98.0110\n",
      "test Loss: 0.1318 Acc: 97.0520\n",
      "\n",
      "Epoch 167/1199\n",
      "------------------------\n",
      "train Loss: 0.0802 Acc: 98.0810\n",
      "test Loss: 0.1313 Acc: 97.0441\n",
      "\n",
      "Epoch 168/1199\n",
      "------------------------\n",
      "train Loss: 0.0842 Acc: 98.0247\n",
      "test Loss: 0.1341 Acc: 96.9926\n",
      "\n",
      "Epoch 169/1199\n",
      "------------------------\n",
      "train Loss: 0.0853 Acc: 97.9465\n",
      "test Loss: 0.1368 Acc: 96.9591\n",
      "\n",
      "Epoch 170/1199\n",
      "------------------------\n",
      "train Loss: 0.0828 Acc: 97.9936\n",
      "test Loss: 0.1323 Acc: 96.9992\n",
      "\n",
      "Epoch 171/1199\n",
      "------------------------\n",
      "train Loss: 0.0802 Acc: 98.0666\n",
      "test Loss: 0.1293 Acc: 97.0850\n",
      "\n",
      "Epoch 172/1199\n",
      "------------------------\n",
      "train Loss: 0.0834 Acc: 97.9883\n",
      "test Loss: 0.1468 Acc: 96.6671\n",
      "\n",
      "Epoch 173/1199\n",
      "------------------------\n",
      "train Loss: 0.0913 Acc: 97.8107\n",
      "test Loss: 0.1384 Acc: 96.8004\n",
      "\n",
      "Epoch 174/1199\n",
      "------------------------\n",
      "train Loss: 0.0889 Acc: 97.8652\n",
      "test Loss: 0.1340 Acc: 96.9621\n",
      "\n",
      "Epoch 175/1199\n",
      "------------------------\n",
      "train Loss: 0.0786 Acc: 98.1054\n",
      "test Loss: 0.1279 Acc: 97.1225\n",
      "\n",
      "Epoch 176/1199\n",
      "------------------------\n",
      "train Loss: 0.0828 Acc: 98.0217\n",
      "test Loss: 0.1309 Acc: 97.0859\n",
      "\n",
      "Epoch 177/1199\n",
      "------------------------\n",
      "train Loss: 0.0798 Acc: 98.1114\n",
      "test Loss: 0.1259 Acc: 97.1662\n",
      "\n",
      "Epoch 178/1199\n",
      "------------------------\n",
      "train Loss: 0.0787 Acc: 98.1069\n",
      "test Loss: 0.1293 Acc: 97.1467\n",
      "\n",
      "Epoch 179/1199\n",
      "------------------------\n",
      "train Loss: 0.0789 Acc: 98.1193\n",
      "test Loss: 0.1308 Acc: 97.0381\n",
      "\n",
      "Epoch 180/1199\n",
      "------------------------\n",
      "train Loss: 0.0772 Acc: 98.1610\n",
      "test Loss: 0.1302 Acc: 97.0258\n",
      "\n",
      "Epoch 181/1199\n",
      "------------------------\n",
      "train Loss: 0.0900 Acc: 97.8063\n",
      "test Loss: 0.1417 Acc: 96.8280\n",
      "\n",
      "Epoch 182/1199\n",
      "------------------------\n",
      "train Loss: 0.0828 Acc: 98.0024\n",
      "test Loss: 0.1318 Acc: 97.0569\n",
      "\n",
      "Epoch 183/1199\n",
      "------------------------\n",
      "train Loss: 0.0791 Acc: 98.0988\n",
      "test Loss: 0.1285 Acc: 97.0840\n",
      "\n",
      "Epoch 184/1199\n",
      "------------------------\n",
      "train Loss: 0.0791 Acc: 98.1061\n",
      "test Loss: 0.1283 Acc: 97.1232\n",
      "\n",
      "Epoch 185/1199\n",
      "------------------------\n",
      "train Loss: 0.0773 Acc: 98.1436\n",
      "test Loss: 0.1232 Acc: 97.2902\n",
      "\n",
      "Epoch 186/1199\n",
      "------------------------\n",
      "train Loss: 0.0773 Acc: 98.1725\n",
      "test Loss: 0.1257 Acc: 97.1821\n",
      "\n",
      "Epoch 187/1199\n",
      "------------------------\n",
      "train Loss: 0.0749 Acc: 98.2193\n",
      "test Loss: 0.1299 Acc: 97.0845\n",
      "\n",
      "Epoch 188/1199\n",
      "------------------------\n",
      "train Loss: 0.0755 Acc: 98.1984\n",
      "test Loss: 0.1325 Acc: 97.0147\n",
      "\n",
      "Epoch 189/1199\n",
      "------------------------\n",
      "train Loss: 0.0837 Acc: 97.9679\n",
      "test Loss: 0.1285 Acc: 97.0980\n",
      "\n",
      "Epoch 190/1199\n",
      "------------------------\n",
      "train Loss: 0.0791 Acc: 98.1045\n",
      "test Loss: 0.1343 Acc: 97.0001\n",
      "\n",
      "Epoch 191/1199\n",
      "------------------------\n",
      "train Loss: 0.0831 Acc: 97.9840\n",
      "test Loss: 0.1364 Acc: 96.9588\n",
      "\n",
      "Epoch 192/1199\n",
      "------------------------\n",
      "train Loss: 0.0799 Acc: 98.0465\n",
      "test Loss: 0.1344 Acc: 96.9973\n",
      "\n",
      "Epoch 193/1199\n",
      "------------------------\n",
      "train Loss: 0.0789 Acc: 98.1012\n",
      "test Loss: 0.1258 Acc: 97.1847\n",
      "\n",
      "Epoch 194/1199\n",
      "------------------------\n",
      "train Loss: 0.0730 Acc: 98.2237\n",
      "test Loss: 0.1251 Acc: 97.1394\n",
      "\n",
      "Epoch 195/1199\n",
      "------------------------\n",
      "train Loss: 0.0715 Acc: 98.2822\n",
      "test Loss: 0.1221 Acc: 97.2249\n",
      "\n",
      "Epoch 196/1199\n",
      "------------------------\n",
      "train Loss: 0.0741 Acc: 98.2323\n",
      "test Loss: 0.1206 Acc: 97.2892\n",
      "\n",
      "Epoch 197/1199\n",
      "------------------------\n",
      "train Loss: 0.0733 Acc: 98.2646\n",
      "test Loss: 0.1177 Acc: 97.3836\n",
      "\n",
      "Epoch 198/1199\n",
      "------------------------\n",
      "train Loss: 0.0744 Acc: 98.2142\n",
      "test Loss: 0.1292 Acc: 97.1037\n",
      "\n",
      "Epoch 199/1199\n",
      "------------------------\n",
      "train Loss: 0.0765 Acc: 98.1719\n",
      "test Loss: 0.1313 Acc: 97.1244\n",
      "\n",
      "Epoch 200/1199\n",
      "------------------------\n",
      "train Loss: 0.0775 Acc: 98.1595\n",
      "test Loss: 0.1280 Acc: 97.1177\n",
      "\n",
      "Epoch 201/1199\n",
      "------------------------\n",
      "train Loss: 0.0713 Acc: 98.2828\n",
      "test Loss: 0.1246 Acc: 97.2258\n",
      "\n",
      "Epoch 202/1199\n",
      "------------------------\n",
      "train Loss: 0.0761 Acc: 98.1642\n",
      "test Loss: 0.1231 Acc: 97.2959\n",
      "\n",
      "Epoch 203/1199\n",
      "------------------------\n",
      "train Loss: 0.0725 Acc: 98.2553\n",
      "test Loss: 0.1228 Acc: 97.2401\n",
      "\n",
      "Epoch 204/1199\n",
      "------------------------\n",
      "train Loss: 0.0721 Acc: 98.2778\n",
      "test Loss: 0.1249 Acc: 97.3066\n",
      "\n",
      "Epoch 205/1199\n",
      "------------------------\n",
      "train Loss: 0.0737 Acc: 98.2256\n",
      "test Loss: 0.1214 Acc: 97.2926\n",
      "\n",
      "Epoch 206/1199\n",
      "------------------------\n",
      "train Loss: 0.0715 Acc: 98.2667\n",
      "test Loss: 0.1236 Acc: 97.2367\n",
      "\n",
      "Epoch 207/1199\n",
      "------------------------\n",
      "train Loss: 0.0704 Acc: 98.3243\n",
      "test Loss: 0.1280 Acc: 97.1142\n",
      "\n",
      "Epoch 208/1199\n",
      "------------------------\n",
      "train Loss: 0.0704 Acc: 98.2939\n",
      "test Loss: 0.1250 Acc: 97.2063\n",
      "\n",
      "Epoch 209/1199\n",
      "------------------------\n",
      "train Loss: 0.0777 Acc: 98.1552\n",
      "test Loss: 0.1279 Acc: 97.1477\n",
      "\n",
      "Epoch 210/1199\n",
      "------------------------\n",
      "train Loss: 0.0727 Acc: 98.2575\n",
      "test Loss: 0.1229 Acc: 97.2360\n",
      "\n",
      "Epoch 211/1199\n",
      "------------------------\n",
      "train Loss: 0.0687 Acc: 98.3363\n",
      "test Loss: 0.1217 Acc: 97.2622\n",
      "\n",
      "Epoch 212/1199\n",
      "------------------------\n",
      "train Loss: 0.0685 Acc: 98.3411\n",
      "test Loss: 0.1296 Acc: 97.1097\n",
      "\n",
      "Epoch 213/1199\n",
      "------------------------\n",
      "train Loss: 0.0702 Acc: 98.3022\n",
      "test Loss: 0.1228 Acc: 97.2526\n",
      "\n",
      "Epoch 214/1199\n",
      "------------------------\n",
      "train Loss: 0.0691 Acc: 98.3466\n",
      "test Loss: 0.1204 Acc: 97.3427\n",
      "\n",
      "Epoch 215/1199\n",
      "------------------------\n",
      "train Loss: 0.0668 Acc: 98.4221\n",
      "test Loss: 0.1212 Acc: 97.3393\n",
      "\n",
      "Epoch 216/1199\n",
      "------------------------\n",
      "train Loss: 0.0644 Acc: 98.4605\n",
      "test Loss: 0.1218 Acc: 97.3130\n",
      "\n",
      "Epoch 217/1199\n",
      "------------------------\n",
      "train Loss: 0.0684 Acc: 98.3664\n",
      "test Loss: 0.1253 Acc: 97.2158\n",
      "\n",
      "Epoch 218/1199\n",
      "------------------------\n",
      "train Loss: 0.0717 Acc: 98.2962\n",
      "test Loss: 0.1201 Acc: 97.3189\n",
      "\n",
      "Epoch 219/1199\n",
      "------------------------\n",
      "train Loss: 0.0735 Acc: 98.2475\n",
      "test Loss: 0.1251 Acc: 97.2149\n",
      "\n",
      "Epoch 220/1199\n",
      "------------------------\n",
      "train Loss: 0.0679 Acc: 98.3795\n",
      "test Loss: 0.1212 Acc: 97.3102\n",
      "\n",
      "Epoch 221/1199\n",
      "------------------------\n",
      "train Loss: 0.0666 Acc: 98.4004\n",
      "test Loss: 0.1259 Acc: 97.2313\n",
      "\n",
      "Epoch 222/1199\n",
      "------------------------\n",
      "train Loss: 0.0678 Acc: 98.3578\n",
      "test Loss: 0.1289 Acc: 97.1406\n",
      "\n",
      "Epoch 223/1199\n",
      "------------------------\n",
      "train Loss: 0.0681 Acc: 98.3462\n",
      "test Loss: 0.1225 Acc: 97.2959\n",
      "\n",
      "Epoch 224/1199\n",
      "------------------------\n",
      "train Loss: 0.0711 Acc: 98.2660\n",
      "test Loss: 0.1239 Acc: 97.2925\n",
      "\n",
      "Epoch 225/1199\n",
      "------------------------\n",
      "train Loss: 0.0670 Acc: 98.3812\n",
      "test Loss: 0.1218 Acc: 97.3125\n",
      "\n",
      "Epoch 226/1199\n",
      "------------------------\n",
      "train Loss: 0.0695 Acc: 98.2859\n",
      "test Loss: 0.1310 Acc: 97.1106\n",
      "\n",
      "Epoch 227/1199\n",
      "------------------------\n",
      "train Loss: 0.0733 Acc: 98.1991\n",
      "test Loss: 0.1192 Acc: 97.3676\n",
      "\n",
      "Epoch 228/1199\n",
      "------------------------\n",
      "train Loss: 0.0687 Acc: 98.3333\n",
      "test Loss: 0.1241 Acc: 97.2707\n",
      "\n",
      "Epoch 229/1199\n",
      "------------------------\n",
      "train Loss: 0.0647 Acc: 98.4506\n",
      "test Loss: 0.1153 Acc: 97.5001\n",
      "\n",
      "Epoch 230/1199\n",
      "------------------------\n",
      "train Loss: 0.0650 Acc: 98.4484\n",
      "test Loss: 0.1205 Acc: 97.4018\n",
      "\n",
      "Epoch 231/1199\n",
      "------------------------\n",
      "train Loss: 0.0621 Acc: 98.5098\n",
      "test Loss: 0.1212 Acc: 97.3377\n",
      "\n",
      "Epoch 232/1199\n",
      "------------------------\n",
      "train Loss: 0.0617 Acc: 98.5490\n",
      "test Loss: 0.1152 Acc: 97.5266\n",
      "\n",
      "Epoch 233/1199\n",
      "------------------------\n",
      "train Loss: 0.0639 Acc: 98.4850\n",
      "test Loss: 0.1203 Acc: 97.3729\n",
      "\n",
      "Epoch 234/1199\n",
      "------------------------\n",
      "train Loss: 0.0595 Acc: 98.5989\n",
      "test Loss: 0.1228 Acc: 97.2916\n",
      "\n",
      "Epoch 235/1199\n",
      "------------------------\n",
      "train Loss: 0.0638 Acc: 98.4779\n",
      "test Loss: 0.1289 Acc: 97.1090\n",
      "\n",
      "Epoch 236/1199\n",
      "------------------------\n",
      "train Loss: 0.0812 Acc: 98.0337\n",
      "test Loss: 0.1400 Acc: 96.8413\n",
      "\n",
      "Epoch 237/1199\n",
      "------------------------\n",
      "train Loss: 0.0749 Acc: 98.1299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1248 Acc: 97.2593\n",
      "\n",
      "Epoch 238/1199\n",
      "------------------------\n",
      "train Loss: 0.0666 Acc: 98.3706\n",
      "test Loss: 0.1191 Acc: 97.4507\n",
      "\n",
      "Epoch 239/1199\n",
      "------------------------\n",
      "train Loss: 0.0596 Acc: 98.5635\n",
      "test Loss: 0.1162 Acc: 97.4301\n",
      "\n",
      "Epoch 240/1199\n",
      "------------------------\n",
      "train Loss: 0.0611 Acc: 98.5356\n",
      "test Loss: 0.1178 Acc: 97.4261\n",
      "\n",
      "Epoch 241/1199\n",
      "------------------------\n",
      "train Loss: 0.0641 Acc: 98.4565\n",
      "test Loss: 0.1210 Acc: 97.3645\n",
      "\n",
      "Epoch 242/1199\n",
      "------------------------\n",
      "train Loss: 0.0630 Acc: 98.4838\n",
      "test Loss: 0.1267 Acc: 97.1702\n",
      "\n",
      "Epoch 243/1199\n",
      "------------------------\n",
      "train Loss: 0.0650 Acc: 98.4265\n",
      "test Loss: 0.1195 Acc: 97.4439\n",
      "\n",
      "Epoch 244/1199\n",
      "------------------------\n",
      "train Loss: 0.0634 Acc: 98.4850\n",
      "test Loss: 0.1187 Acc: 97.4665\n",
      "\n",
      "Epoch 245/1199\n",
      "------------------------\n",
      "train Loss: 0.0627 Acc: 98.4917\n",
      "test Loss: 0.1214 Acc: 97.3130\n",
      "\n",
      "Epoch 246/1199\n",
      "------------------------\n",
      "train Loss: 0.0605 Acc: 98.5402\n",
      "test Loss: 0.1212 Acc: 97.3056\n",
      "\n",
      "Epoch 247/1199\n",
      "------------------------\n",
      "train Loss: 0.0588 Acc: 98.5843\n",
      "test Loss: 0.1150 Acc: 97.5583\n",
      "\n",
      "Epoch 248/1199\n",
      "------------------------\n",
      "train Loss: 0.0613 Acc: 98.5115\n",
      "test Loss: 0.1176 Acc: 97.4064\n",
      "\n",
      "Epoch 249/1199\n",
      "------------------------\n",
      "train Loss: 0.0596 Acc: 98.5535\n",
      "test Loss: 0.1144 Acc: 97.5049\n",
      "\n",
      "Epoch 250/1199\n",
      "------------------------\n",
      "train Loss: 0.0569 Acc: 98.6424\n",
      "test Loss: 0.1183 Acc: 97.4522\n",
      "\n",
      "Epoch 251/1199\n",
      "------------------------\n",
      "train Loss: 0.0606 Acc: 98.5583\n",
      "test Loss: 0.1241 Acc: 97.2819\n",
      "\n",
      "Epoch 252/1199\n",
      "------------------------\n",
      "train Loss: 0.0639 Acc: 98.4530\n",
      "test Loss: 0.1193 Acc: 97.4218\n",
      "\n",
      "Epoch 253/1199\n",
      "------------------------\n",
      "train Loss: 0.0616 Acc: 98.5381\n",
      "test Loss: 0.1208 Acc: 97.3776\n",
      "\n",
      "Epoch 254/1199\n",
      "------------------------\n",
      "train Loss: 0.0588 Acc: 98.5866\n",
      "test Loss: 0.1152 Acc: 97.4999\n",
      "\n",
      "Epoch 255/1199\n",
      "------------------------\n",
      "train Loss: 0.0573 Acc: 98.6403\n",
      "test Loss: 0.1141 Acc: 97.5030\n",
      "\n",
      "Epoch 256/1199\n",
      "------------------------\n",
      "train Loss: 0.0612 Acc: 98.5526\n",
      "test Loss: 0.1179 Acc: 97.4056\n",
      "\n",
      "Epoch 257/1199\n",
      "------------------------\n",
      "train Loss: 0.0609 Acc: 98.5223\n",
      "test Loss: 0.1195 Acc: 97.4527\n",
      "\n",
      "Epoch 258/1199\n",
      "------------------------\n",
      "train Loss: 0.0596 Acc: 98.5766\n",
      "test Loss: 0.1180 Acc: 97.4769\n",
      "\n",
      "Epoch 259/1199\n",
      "------------------------\n",
      "train Loss: 0.0569 Acc: 98.6415\n",
      "test Loss: 0.1191 Acc: 97.4546\n",
      "\n",
      "Epoch 260/1199\n",
      "------------------------\n",
      "train Loss: 0.0580 Acc: 98.6086\n",
      "test Loss: 0.1149 Acc: 97.4650\n",
      "\n",
      "Epoch 261/1199\n",
      "------------------------\n",
      "train Loss: 0.0588 Acc: 98.5753\n",
      "test Loss: 0.1213 Acc: 97.3605\n",
      "\n",
      "Epoch 262/1199\n",
      "------------------------\n",
      "train Loss: 0.0624 Acc: 98.5246\n",
      "test Loss: 0.1201 Acc: 97.4254\n",
      "\n",
      "Epoch 263/1199\n",
      "------------------------\n",
      "train Loss: 0.0631 Acc: 98.4604\n",
      "test Loss: 0.1229 Acc: 97.3463\n",
      "\n",
      "Epoch 264/1199\n",
      "------------------------\n",
      "train Loss: 0.0707 Acc: 98.2852\n",
      "test Loss: 0.1220 Acc: 97.3356\n",
      "\n",
      "Epoch 265/1199\n",
      "------------------------\n",
      "train Loss: 0.0622 Acc: 98.4910\n",
      "test Loss: 0.1162 Acc: 97.4246\n",
      "\n",
      "Epoch 266/1199\n",
      "------------------------\n",
      "train Loss: 0.0591 Acc: 98.5689\n",
      "test Loss: 0.1185 Acc: 97.3793\n",
      "\n",
      "Epoch 267/1199\n",
      "------------------------\n",
      "train Loss: 0.0586 Acc: 98.5677\n",
      "test Loss: 0.1134 Acc: 97.5643\n",
      "\n",
      "Epoch 268/1199\n",
      "------------------------\n",
      "train Loss: 0.0582 Acc: 98.5943\n",
      "test Loss: 0.1164 Acc: 97.4569\n",
      "\n",
      "Epoch 269/1199\n",
      "------------------------\n",
      "train Loss: 0.0564 Acc: 98.6319\n",
      "test Loss: 0.1208 Acc: 97.3752\n",
      "\n",
      "Epoch 270/1199\n",
      "------------------------\n",
      "train Loss: 0.0586 Acc: 98.5769\n",
      "test Loss: 0.1209 Acc: 97.3557\n",
      "\n",
      "Epoch 271/1199\n",
      "------------------------\n",
      "train Loss: 0.0585 Acc: 98.5725\n",
      "test Loss: 0.1176 Acc: 97.4698\n",
      "\n",
      "Epoch 272/1199\n",
      "------------------------\n",
      "train Loss: 0.0636 Acc: 98.4455\n",
      "test Loss: 0.1192 Acc: 97.3533\n",
      "\n",
      "Epoch 273/1199\n",
      "------------------------\n",
      "train Loss: 0.0559 Acc: 98.6500\n",
      "test Loss: 0.1162 Acc: 97.4700\n",
      "\n",
      "Epoch 274/1199\n",
      "------------------------\n",
      "train Loss: 0.0577 Acc: 98.6222\n",
      "test Loss: 0.1635 Acc: 96.2928\n",
      "\n",
      "Epoch 275/1199\n",
      "------------------------\n",
      "train Loss: 0.0684 Acc: 98.2928\n",
      "test Loss: 0.1248 Acc: 97.2332\n",
      "\n",
      "Epoch 276/1199\n",
      "------------------------\n",
      "train Loss: 0.0573 Acc: 98.5900\n",
      "test Loss: 0.1188 Acc: 97.4695\n",
      "\n",
      "Epoch 277/1199\n",
      "------------------------\n",
      "train Loss: 0.0566 Acc: 98.6246\n",
      "test Loss: 0.1136 Acc: 97.5918\n",
      "\n",
      "Epoch 278/1199\n",
      "------------------------\n",
      "train Loss: 0.0520 Acc: 98.7479\n",
      "test Loss: 0.1146 Acc: 97.5830\n",
      "\n",
      "Epoch 279/1199\n",
      "------------------------\n",
      "train Loss: 0.0518 Acc: 98.7528\n",
      "test Loss: 0.1153 Acc: 97.5374\n",
      "\n",
      "Epoch 280/1199\n",
      "------------------------\n",
      "train Loss: 0.0537 Acc: 98.7167\n",
      "test Loss: 0.1119 Acc: 97.6572\n",
      "\n",
      "Epoch 281/1199\n",
      "------------------------\n",
      "train Loss: 0.0526 Acc: 98.7481\n",
      "test Loss: 0.1112 Acc: 97.5893\n",
      "\n",
      "Epoch 282/1199\n",
      "------------------------\n",
      "train Loss: 0.0502 Acc: 98.8165\n",
      "test Loss: 0.1096 Acc: 97.6952\n",
      "\n",
      "Epoch 283/1199\n",
      "------------------------\n",
      "train Loss: 0.0505 Acc: 98.8097\n",
      "test Loss: 0.1094 Acc: 97.7118\n",
      "\n",
      "Epoch 284/1199\n",
      "------------------------\n",
      "train Loss: 0.0509 Acc: 98.7937\n",
      "test Loss: 0.1107 Acc: 97.6905\n",
      "\n",
      "Epoch 285/1199\n",
      "------------------------\n",
      "train Loss: 0.0484 Acc: 98.8530\n",
      "test Loss: 0.1157 Acc: 97.5785\n",
      "\n",
      "Epoch 286/1199\n",
      "------------------------\n",
      "train Loss: 0.0495 Acc: 98.8281\n",
      "test Loss: 0.1148 Acc: 97.5935\n",
      "\n",
      "Epoch 287/1199\n",
      "------------------------\n",
      "train Loss: 0.0476 Acc: 98.8838\n",
      "test Loss: 0.1117 Acc: 97.6798\n",
      "\n",
      "Epoch 288/1199\n",
      "------------------------\n",
      "train Loss: 0.0481 Acc: 98.8719\n",
      "test Loss: 0.1155 Acc: 97.5731\n",
      "\n",
      "Epoch 289/1199\n",
      "------------------------\n",
      "train Loss: 0.0479 Acc: 98.8539\n",
      "test Loss: 0.1195 Acc: 97.4577\n",
      "\n",
      "Epoch 290/1199\n",
      "------------------------\n",
      "train Loss: 0.0617 Acc: 98.4939\n",
      "test Loss: 0.1316 Acc: 97.0816\n",
      "\n",
      "Epoch 291/1199\n",
      "------------------------\n",
      "train Loss: 0.0664 Acc: 98.3841\n",
      "test Loss: 0.1274 Acc: 97.1717\n",
      "\n",
      "Epoch 292/1199\n",
      "------------------------\n",
      "train Loss: 0.0668 Acc: 98.3552\n",
      "test Loss: 0.1217 Acc: 97.3823\n",
      "\n",
      "Epoch 293/1199\n",
      "------------------------\n",
      "train Loss: 0.0598 Acc: 98.5547\n",
      "test Loss: 0.1212 Acc: 97.3351\n",
      "\n",
      "Epoch 294/1199\n",
      "------------------------\n",
      "train Loss: 0.0572 Acc: 98.6145\n",
      "test Loss: 0.1185 Acc: 97.5016\n",
      "\n",
      "Epoch 295/1199\n",
      "------------------------\n",
      "train Loss: 0.0540 Acc: 98.6861\n",
      "test Loss: 0.1162 Acc: 97.4916\n",
      "\n",
      "Epoch 296/1199\n",
      "------------------------\n",
      "train Loss: 0.0539 Acc: 98.7130\n",
      "test Loss: 0.1188 Acc: 97.5823\n",
      "\n",
      "Epoch 297/1199\n",
      "------------------------\n",
      "train Loss: 0.0519 Acc: 98.7561\n",
      "test Loss: 0.1157 Acc: 97.4546\n",
      "\n",
      "Epoch 298/1199\n",
      "------------------------\n",
      "train Loss: 0.0530 Acc: 98.7303\n",
      "test Loss: 0.1180 Acc: 97.4729\n",
      "\n",
      "Epoch 299/1199\n",
      "------------------------\n",
      "train Loss: 0.0579 Acc: 98.5893\n",
      "test Loss: 0.1245 Acc: 97.3847\n",
      "\n",
      "Epoch 300/1199\n",
      "------------------------\n",
      "train Loss: 0.0483 Acc: 98.8562\n",
      "test Loss: 0.1108 Acc: 97.6477\n",
      "\n",
      "Epoch 301/1199\n",
      "------------------------\n",
      "train Loss: 0.0465 Acc: 98.9133\n",
      "test Loss: 0.1058 Acc: 97.7486\n",
      "\n",
      "Epoch 302/1199\n",
      "------------------------\n",
      "train Loss: 0.0481 Acc: 98.8847\n",
      "test Loss: 0.1054 Acc: 97.8301\n",
      "\n",
      "Epoch 303/1199\n",
      "------------------------\n",
      "train Loss: 0.0448 Acc: 98.9676\n",
      "test Loss: 0.1040 Acc: 97.8118\n",
      "\n",
      "Epoch 304/1199\n",
      "------------------------\n",
      "train Loss: 0.0456 Acc: 98.9371\n",
      "test Loss: 0.1040 Acc: 97.8348\n",
      "\n",
      "Epoch 305/1199\n",
      "------------------------\n",
      "train Loss: 0.0428 Acc: 99.0082\n",
      "test Loss: 0.1057 Acc: 97.7562\n",
      "\n",
      "Epoch 306/1199\n",
      "------------------------\n",
      "train Loss: 0.0460 Acc: 98.9549\n",
      "test Loss: 0.1050 Acc: 97.8099\n",
      "\n",
      "Epoch 307/1199\n",
      "------------------------\n",
      "train Loss: 0.0416 Acc: 99.0429\n",
      "test Loss: 0.1046 Acc: 97.8220\n",
      "\n",
      "Epoch 308/1199\n",
      "------------------------\n",
      "train Loss: 0.0416 Acc: 99.0378\n",
      "test Loss: 0.1050 Acc: 97.8306\n",
      "\n",
      "Epoch 309/1199\n",
      "------------------------\n",
      "train Loss: 0.0418 Acc: 99.0281\n",
      "test Loss: 0.1046 Acc: 97.8159\n",
      "\n",
      "Epoch 310/1199\n",
      "------------------------\n",
      "train Loss: 0.0433 Acc: 99.0058\n",
      "test Loss: 0.1069 Acc: 97.7622\n",
      "\n",
      "Epoch 311/1199\n",
      "------------------------\n",
      "train Loss: 0.0431 Acc: 99.0200\n",
      "test Loss: 0.1069 Acc: 97.7904\n",
      "\n",
      "Epoch 312/1199\n",
      "------------------------\n",
      "train Loss: 0.0426 Acc: 99.0363\n",
      "test Loss: 0.1068 Acc: 97.8201\n",
      "\n",
      "Epoch 313/1199\n",
      "------------------------\n",
      "train Loss: 0.0425 Acc: 99.0257\n",
      "test Loss: 0.1044 Acc: 97.8298\n",
      "\n",
      "Epoch 314/1199\n",
      "------------------------\n",
      "train Loss: 0.0439 Acc: 99.0109\n",
      "test Loss: 0.1058 Acc: 97.8033\n",
      "\n",
      "Epoch 315/1199\n",
      "------------------------\n",
      "train Loss: 0.0429 Acc: 99.0155\n",
      "test Loss: 0.1046 Acc: 97.8083\n",
      "\n",
      "Epoch 316/1199\n",
      "------------------------\n",
      "train Loss: 0.0403 Acc: 99.0760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1082 Acc: 97.7038\n",
      "\n",
      "Epoch 317/1199\n",
      "------------------------\n",
      "train Loss: 0.0427 Acc: 99.0265\n",
      "test Loss: 0.1032 Acc: 97.8453\n",
      "\n",
      "Epoch 318/1199\n",
      "------------------------\n",
      "train Loss: 0.0416 Acc: 99.0540\n",
      "test Loss: 0.1087 Acc: 97.7712\n",
      "\n",
      "Epoch 319/1199\n",
      "------------------------\n",
      "train Loss: 0.0429 Acc: 99.0365\n",
      "test Loss: 0.1061 Acc: 97.8453\n",
      "\n",
      "Epoch 320/1199\n",
      "------------------------\n",
      "train Loss: 0.0433 Acc: 99.0142\n",
      "test Loss: 0.1063 Acc: 97.8109\n",
      "\n",
      "Epoch 321/1199\n",
      "------------------------\n",
      "train Loss: 0.0434 Acc: 99.0252\n",
      "test Loss: 0.1050 Acc: 97.8660\n",
      "\n",
      "Epoch 322/1199\n",
      "------------------------\n",
      "train Loss: 0.0405 Acc: 99.0839\n",
      "test Loss: 0.1049 Acc: 97.8168\n",
      "\n",
      "Epoch 323/1199\n",
      "------------------------\n",
      "train Loss: 0.0416 Acc: 99.0562\n",
      "test Loss: 0.1046 Acc: 97.8667\n",
      "\n",
      "Epoch 324/1199\n",
      "------------------------\n",
      "train Loss: 0.0424 Acc: 99.0457\n",
      "test Loss: 0.1074 Acc: 97.7740\n",
      "\n",
      "Epoch 325/1199\n",
      "------------------------\n",
      "train Loss: 0.0396 Acc: 99.0905\n",
      "test Loss: 0.1076 Acc: 97.8209\n",
      "\n",
      "Epoch 326/1199\n",
      "------------------------\n",
      "train Loss: 0.0411 Acc: 99.0734\n",
      "test Loss: 0.1060 Acc: 97.8227\n",
      "\n",
      "Epoch 327/1199\n",
      "------------------------\n",
      "train Loss: 0.0402 Acc: 99.0717\n",
      "test Loss: 0.1051 Acc: 97.8149\n",
      "\n",
      "Epoch 328/1199\n",
      "------------------------\n",
      "train Loss: 0.0414 Acc: 99.0588\n",
      "test Loss: 0.1082 Acc: 97.7432\n",
      "\n",
      "Epoch 329/1199\n",
      "------------------------\n",
      "train Loss: 0.0409 Acc: 99.0717\n",
      "test Loss: 0.1071 Acc: 97.7660\n",
      "\n",
      "Epoch 330/1199\n",
      "------------------------\n",
      "train Loss: 0.0420 Acc: 99.0482\n",
      "test Loss: 0.1087 Acc: 97.7971\n",
      "\n",
      "Epoch 331/1199\n",
      "------------------------\n",
      "train Loss: 0.0414 Acc: 99.0679\n",
      "test Loss: 0.1054 Acc: 97.8582\n",
      "\n",
      "Epoch 332/1199\n",
      "------------------------\n",
      "train Loss: 0.0397 Acc: 99.0865\n",
      "test Loss: 0.1061 Acc: 97.8249\n",
      "\n",
      "Epoch 333/1199\n",
      "------------------------\n",
      "train Loss: 0.0432 Acc: 99.0363\n",
      "test Loss: 0.1075 Acc: 97.8113\n",
      "\n",
      "Epoch 334/1199\n",
      "------------------------\n",
      "train Loss: 0.0404 Acc: 99.0929\n",
      "test Loss: 0.1077 Acc: 97.7757\n",
      "\n",
      "Epoch 335/1199\n",
      "------------------------\n",
      "train Loss: 0.0409 Acc: 99.0634\n",
      "test Loss: 0.1086 Acc: 97.8429\n",
      "\n",
      "Epoch 336/1199\n",
      "------------------------\n",
      "train Loss: 0.0395 Acc: 99.1127\n",
      "test Loss: 0.1087 Acc: 97.8228\n",
      "\n",
      "Epoch 337/1199\n",
      "------------------------\n",
      "train Loss: 0.0408 Acc: 99.0550\n",
      "test Loss: 0.1034 Acc: 97.8940\n",
      "\n",
      "Epoch 338/1199\n",
      "------------------------\n",
      "train Loss: 0.0411 Acc: 99.0571\n",
      "test Loss: 0.1089 Acc: 97.8439\n",
      "\n",
      "Epoch 339/1199\n",
      "------------------------\n",
      "train Loss: 0.0429 Acc: 99.0347\n",
      "test Loss: 0.1048 Acc: 97.8215\n",
      "\n",
      "Epoch 340/1199\n",
      "------------------------\n",
      "train Loss: 0.0412 Acc: 99.0765\n",
      "test Loss: 0.1074 Acc: 97.8412\n",
      "\n",
      "Epoch 341/1199\n",
      "------------------------\n",
      "train Loss: 0.0416 Acc: 99.0609\n",
      "test Loss: 0.1062 Acc: 97.8596\n",
      "\n",
      "Epoch 342/1199\n",
      "------------------------\n",
      "train Loss: 0.0401 Acc: 99.0988\n",
      "test Loss: 0.1075 Acc: 97.7950\n",
      "\n",
      "Epoch 343/1199\n",
      "------------------------\n",
      "train Loss: 0.0398 Acc: 99.0971\n",
      "test Loss: 0.1065 Acc: 97.7783\n",
      "\n",
      "Epoch 344/1199\n",
      "------------------------\n",
      "train Loss: 0.0403 Acc: 99.0870\n",
      "test Loss: 0.1070 Acc: 97.8393\n",
      "\n",
      "Epoch 345/1199\n",
      "------------------------\n",
      "train Loss: 0.0395 Acc: 99.1047\n",
      "test Loss: 0.1120 Acc: 97.7954\n",
      "\n",
      "Epoch 346/1199\n",
      "------------------------\n",
      "train Loss: 0.0403 Acc: 99.0843\n",
      "test Loss: 0.1073 Acc: 97.7921\n",
      "\n",
      "Epoch 347/1199\n",
      "------------------------\n",
      "train Loss: 0.0399 Acc: 99.0974\n",
      "test Loss: 0.1057 Acc: 97.8342\n",
      "\n",
      "Epoch 348/1199\n",
      "------------------------\n",
      "train Loss: 0.0412 Acc: 99.0721\n",
      "test Loss: 0.1050 Acc: 97.8551\n",
      "\n",
      "Epoch 349/1199\n",
      "------------------------\n",
      "train Loss: 0.0402 Acc: 99.0873\n",
      "test Loss: 0.1058 Acc: 97.8311\n",
      "\n",
      "Epoch 350/1199\n",
      "------------------------\n",
      "train Loss: 0.0406 Acc: 99.0731\n",
      "test Loss: 0.1047 Acc: 97.8574\n",
      "\n",
      "Epoch 351/1199\n",
      "------------------------\n",
      "train Loss: 0.0415 Acc: 99.0630\n",
      "test Loss: 0.1090 Acc: 97.7943\n",
      "\n",
      "Epoch 352/1199\n",
      "------------------------\n",
      "train Loss: 0.0392 Acc: 99.1101\n",
      "test Loss: 0.1084 Acc: 97.8389\n",
      "\n",
      "Epoch 353/1199\n",
      "------------------------\n",
      "train Loss: 0.0384 Acc: 99.1214\n",
      "test Loss: 0.1057 Acc: 97.8995\n",
      "\n",
      "Epoch 354/1199\n",
      "------------------------\n",
      "train Loss: 0.0401 Acc: 99.1050\n",
      "test Loss: 0.1110 Acc: 97.7325\n",
      "\n",
      "Epoch 355/1199\n",
      "------------------------\n",
      "train Loss: 0.0390 Acc: 99.1198\n",
      "test Loss: 0.1055 Acc: 97.8387\n",
      "\n",
      "Epoch 356/1199\n",
      "------------------------\n",
      "train Loss: 0.0398 Acc: 99.0985\n",
      "test Loss: 0.1070 Acc: 97.8092\n",
      "\n",
      "Epoch 357/1199\n",
      "------------------------\n",
      "train Loss: 0.0396 Acc: 99.1097\n",
      "test Loss: 0.1072 Acc: 97.7881\n",
      "\n",
      "Epoch 358/1199\n",
      "------------------------\n",
      "train Loss: 0.0375 Acc: 99.1368\n",
      "test Loss: 0.1060 Acc: 97.8280\n",
      "\n",
      "Epoch 359/1199\n",
      "------------------------\n",
      "train Loss: 0.0380 Acc: 99.1322\n",
      "test Loss: 0.1053 Acc: 97.8487\n",
      "\n",
      "Epoch 360/1199\n",
      "------------------------\n",
      "train Loss: 0.0399 Acc: 99.1065\n",
      "test Loss: 0.1086 Acc: 97.8323\n",
      "\n",
      "Epoch 361/1199\n",
      "------------------------\n",
      "train Loss: 0.0395 Acc: 99.1003\n",
      "test Loss: 0.1083 Acc: 97.7748\n",
      "\n",
      "Epoch 362/1199\n",
      "------------------------\n",
      "train Loss: 0.0369 Acc: 99.1580\n",
      "test Loss: 0.1030 Acc: 97.9265\n",
      "\n",
      "Epoch 363/1199\n",
      "------------------------\n",
      "train Loss: 0.0395 Acc: 99.1050\n",
      "test Loss: 0.1078 Acc: 97.8451\n",
      "\n",
      "Epoch 364/1199\n",
      "------------------------\n",
      "train Loss: 0.0393 Acc: 99.1108\n",
      "test Loss: 0.1077 Acc: 97.8374\n",
      "\n",
      "Epoch 365/1199\n",
      "------------------------\n",
      "train Loss: 0.0393 Acc: 99.1043\n",
      "test Loss: 0.1059 Acc: 97.8251\n",
      "\n",
      "Epoch 366/1199\n",
      "------------------------\n",
      "train Loss: 0.0388 Acc: 99.1289\n",
      "test Loss: 0.1081 Acc: 97.8645\n",
      "\n",
      "Epoch 367/1199\n",
      "------------------------\n",
      "train Loss: 0.0389 Acc: 99.1252\n",
      "test Loss: 0.1065 Acc: 97.8563\n",
      "\n",
      "Epoch 368/1199\n",
      "------------------------\n",
      "train Loss: 0.0391 Acc: 99.1037\n",
      "test Loss: 0.1048 Acc: 97.9265\n",
      "\n",
      "Epoch 369/1199\n",
      "------------------------\n",
      "train Loss: 0.0379 Acc: 99.1524\n",
      "test Loss: 0.1088 Acc: 97.7949\n",
      "\n",
      "Epoch 370/1199\n",
      "------------------------\n",
      "train Loss: 0.0384 Acc: 99.1280\n",
      "test Loss: 0.1063 Acc: 97.8981\n",
      "\n",
      "Epoch 371/1199\n",
      "------------------------\n",
      "train Loss: 0.0386 Acc: 99.1172\n",
      "test Loss: 0.1091 Acc: 97.8515\n",
      "\n",
      "Epoch 372/1199\n",
      "------------------------\n",
      "train Loss: 0.0377 Acc: 99.1423\n",
      "test Loss: 0.1088 Acc: 97.7633\n",
      "\n",
      "Epoch 373/1199\n",
      "------------------------\n",
      "train Loss: 0.0386 Acc: 99.1219\n",
      "test Loss: 0.1062 Acc: 97.8620\n",
      "\n",
      "Epoch 374/1199\n",
      "------------------------\n",
      "train Loss: 0.0390 Acc: 99.1161\n",
      "test Loss: 0.1070 Acc: 97.8845\n",
      "\n",
      "Epoch 375/1199\n",
      "------------------------\n",
      "train Loss: 0.0398 Acc: 99.1137\n",
      "test Loss: 0.1097 Acc: 97.7268\n",
      "\n",
      "Epoch 376/1199\n",
      "------------------------\n",
      "train Loss: 0.0395 Acc: 99.1206\n",
      "test Loss: 0.1056 Acc: 97.8876\n",
      "\n",
      "Epoch 377/1199\n",
      "------------------------\n",
      "train Loss: 0.0388 Acc: 99.1159\n",
      "test Loss: 0.1062 Acc: 97.8536\n",
      "\n",
      "Epoch 378/1199\n",
      "------------------------\n",
      "train Loss: 0.0389 Acc: 99.1115\n",
      "test Loss: 0.1086 Acc: 97.8284\n",
      "\n",
      "Epoch 379/1199\n",
      "------------------------\n",
      "train Loss: 0.0385 Acc: 99.1377\n",
      "test Loss: 0.1058 Acc: 97.8223\n",
      "\n",
      "Epoch 380/1199\n",
      "------------------------\n",
      "train Loss: 0.0393 Acc: 99.1024\n",
      "test Loss: 0.1080 Acc: 97.8598\n",
      "\n",
      "Epoch 381/1199\n",
      "------------------------\n",
      "train Loss: 0.0393 Acc: 99.0916\n",
      "test Loss: 0.1064 Acc: 97.8513\n",
      "\n",
      "Epoch 382/1199\n",
      "------------------------\n",
      "train Loss: 0.0406 Acc: 99.0837\n",
      "test Loss: 0.1053 Acc: 97.8620\n",
      "\n",
      "Epoch 383/1199\n",
      "------------------------\n",
      "train Loss: 0.0385 Acc: 99.1366\n",
      "test Loss: 0.1064 Acc: 97.8975\n",
      "\n",
      "Epoch 384/1199\n",
      "------------------------\n",
      "train Loss: 0.0394 Acc: 99.1121\n",
      "test Loss: 0.1055 Acc: 97.9021\n",
      "\n",
      "Epoch 385/1199\n",
      "------------------------\n",
      "train Loss: 0.0386 Acc: 99.1340\n",
      "test Loss: 0.1070 Acc: 97.9158\n",
      "\n",
      "Epoch 386/1199\n",
      "------------------------\n",
      "train Loss: 0.0380 Acc: 99.1530\n",
      "test Loss: 0.1053 Acc: 97.8952\n",
      "\n",
      "Epoch 387/1199\n",
      "------------------------\n",
      "train Loss: 0.0371 Acc: 99.1470\n",
      "test Loss: 0.1078 Acc: 97.8223\n",
      "\n",
      "Epoch 388/1199\n",
      "------------------------\n",
      "train Loss: 0.0382 Acc: 99.1420\n",
      "test Loss: 0.1066 Acc: 97.7873\n",
      "\n",
      "Epoch 389/1199\n",
      "------------------------\n",
      "train Loss: 0.0378 Acc: 99.1450\n",
      "test Loss: 0.1029 Acc: 97.9014\n",
      "\n",
      "Epoch 390/1199\n",
      "------------------------\n",
      "train Loss: 0.0397 Acc: 99.1083\n",
      "test Loss: 0.1054 Acc: 97.8779\n",
      "\n",
      "Epoch 391/1199\n",
      "------------------------\n",
      "train Loss: 0.0389 Acc: 99.1182\n",
      "test Loss: 0.1051 Acc: 97.8969\n",
      "\n",
      "Epoch 392/1199\n",
      "------------------------\n",
      "train Loss: 0.0376 Acc: 99.1359\n",
      "test Loss: 0.1103 Acc: 97.8230\n",
      "\n",
      "Epoch 393/1199\n",
      "------------------------\n",
      "train Loss: 0.0372 Acc: 99.1568\n",
      "test Loss: 0.1046 Acc: 97.8691\n",
      "\n",
      "Epoch 394/1199\n",
      "------------------------\n",
      "train Loss: 0.0371 Acc: 99.1647\n",
      "test Loss: 0.1064 Acc: 97.8506\n",
      "\n",
      "Epoch 395/1199\n",
      "------------------------\n",
      "train Loss: 0.0369 Acc: 99.1628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1072 Acc: 97.9130\n",
      "\n",
      "Epoch 396/1199\n",
      "------------------------\n",
      "train Loss: 0.0389 Acc: 99.1206\n",
      "test Loss: 0.1070 Acc: 97.8173\n",
      "\n",
      "Epoch 397/1199\n",
      "------------------------\n",
      "train Loss: 0.0358 Acc: 99.1829\n",
      "test Loss: 0.1065 Acc: 97.8726\n",
      "\n",
      "Epoch 398/1199\n",
      "------------------------\n",
      "train Loss: 0.0362 Acc: 99.1787\n",
      "test Loss: 0.1070 Acc: 97.9253\n",
      "\n",
      "Epoch 399/1199\n",
      "------------------------\n",
      "train Loss: 0.0383 Acc: 99.1472\n",
      "test Loss: 0.1086 Acc: 97.7969\n",
      "\n",
      "Epoch 400/1199\n",
      "------------------------\n",
      "train Loss: 0.0371 Acc: 99.1554\n",
      "test Loss: 0.1038 Acc: 97.8850\n",
      "\n",
      "Epoch 401/1199\n",
      "------------------------\n",
      "train Loss: 0.0364 Acc: 99.1628\n",
      "test Loss: 0.1072 Acc: 97.8859\n",
      "\n",
      "Epoch 402/1199\n",
      "------------------------\n",
      "train Loss: 0.0369 Acc: 99.1720\n",
      "test Loss: 0.1061 Acc: 97.8988\n",
      "\n",
      "Epoch 403/1199\n",
      "------------------------\n",
      "train Loss: 0.0378 Acc: 99.1351\n",
      "test Loss: 0.1076 Acc: 97.8695\n",
      "\n",
      "Epoch 404/1199\n",
      "------------------------\n",
      "train Loss: 0.0355 Acc: 99.1906\n",
      "test Loss: 0.1082 Acc: 97.8733\n",
      "\n",
      "Epoch 405/1199\n",
      "------------------------\n",
      "train Loss: 0.0364 Acc: 99.1792\n",
      "test Loss: 0.1041 Acc: 97.9054\n",
      "\n",
      "Epoch 406/1199\n",
      "------------------------\n",
      "train Loss: 0.0370 Acc: 99.1760\n",
      "test Loss: 0.1096 Acc: 97.8541\n",
      "\n",
      "Epoch 407/1199\n",
      "------------------------\n",
      "train Loss: 0.0377 Acc: 99.1469\n",
      "test Loss: 0.1081 Acc: 97.8643\n",
      "\n",
      "Epoch 408/1199\n",
      "------------------------\n",
      "train Loss: 0.0370 Acc: 99.1691\n",
      "test Loss: 0.1059 Acc: 97.8836\n",
      "\n",
      "Epoch 409/1199\n",
      "------------------------\n",
      "train Loss: 0.0358 Acc: 99.1839\n",
      "test Loss: 0.1084 Acc: 97.8615\n",
      "\n",
      "Epoch 410/1199\n",
      "------------------------\n",
      "train Loss: 0.0361 Acc: 99.1843\n",
      "test Loss: 0.1064 Acc: 97.9002\n",
      "\n",
      "Epoch 411/1199\n",
      "------------------------\n",
      "train Loss: 0.0382 Acc: 99.1493\n",
      "test Loss: 0.1097 Acc: 97.8413\n",
      "\n",
      "Epoch 412/1199\n",
      "------------------------\n",
      "train Loss: 0.0362 Acc: 99.1844\n",
      "test Loss: 0.1085 Acc: 97.9151\n",
      "\n",
      "Epoch 413/1199\n",
      "------------------------\n",
      "train Loss: 0.0385 Acc: 99.1480\n",
      "test Loss: 0.1084 Acc: 97.8304\n",
      "\n",
      "Epoch 414/1199\n",
      "------------------------\n",
      "train Loss: 0.0405 Acc: 99.0873\n",
      "test Loss: 0.1130 Acc: 97.8095\n",
      "\n",
      "Epoch 415/1199\n",
      "------------------------\n",
      "train Loss: 0.0379 Acc: 99.1546\n",
      "test Loss: 0.1090 Acc: 97.9149\n",
      "\n",
      "Epoch 416/1199\n",
      "------------------------\n",
      "train Loss: 0.0345 Acc: 99.2070\n",
      "test Loss: 0.1061 Acc: 97.8494\n",
      "\n",
      "Epoch 417/1199\n",
      "------------------------\n",
      "train Loss: 0.0380 Acc: 99.1348\n",
      "test Loss: 0.1126 Acc: 97.6957\n",
      "\n",
      "Epoch 418/1199\n",
      "------------------------\n",
      "train Loss: 0.0371 Acc: 99.1481\n",
      "test Loss: 0.1053 Acc: 97.8622\n",
      "\n",
      "Epoch 419/1199\n",
      "------------------------\n",
      "train Loss: 0.0409 Acc: 99.0734\n",
      "test Loss: 0.1087 Acc: 97.8135\n",
      "\n",
      "Epoch 420/1199\n",
      "------------------------\n",
      "train Loss: 0.0375 Acc: 99.1347\n",
      "test Loss: 0.1080 Acc: 97.8591\n",
      "\n",
      "Epoch 421/1199\n",
      "------------------------\n",
      "train Loss: 0.0405 Acc: 99.0629\n",
      "test Loss: 0.1088 Acc: 97.7323\n",
      "\n",
      "Epoch 422/1199\n",
      "------------------------\n",
      "train Loss: 0.0386 Acc: 99.1077\n",
      "test Loss: 0.1081 Acc: 97.7836\n",
      "\n",
      "Epoch 423/1199\n",
      "------------------------\n",
      "train Loss: 0.0382 Acc: 99.1166\n",
      "test Loss: 0.1073 Acc: 97.8477\n",
      "\n",
      "Epoch 424/1199\n",
      "------------------------\n",
      "train Loss: 0.0355 Acc: 99.1809\n",
      "test Loss: 0.1043 Acc: 97.9232\n",
      "\n",
      "Epoch 425/1199\n",
      "------------------------\n",
      "train Loss: 0.0376 Acc: 99.1279\n",
      "test Loss: 0.1074 Acc: 97.8771\n",
      "\n",
      "Epoch 426/1199\n",
      "------------------------\n",
      "train Loss: 0.0360 Acc: 99.1788\n",
      "test Loss: 0.1076 Acc: 97.8771\n",
      "\n",
      "Epoch 427/1199\n",
      "------------------------\n",
      "train Loss: 0.0354 Acc: 99.1995\n",
      "test Loss: 0.1058 Acc: 97.9063\n",
      "\n",
      "Epoch 428/1199\n",
      "------------------------\n",
      "train Loss: 0.0357 Acc: 99.1825\n",
      "test Loss: 0.1058 Acc: 97.8817\n",
      "\n",
      "Epoch 429/1199\n",
      "------------------------\n",
      "train Loss: 0.0345 Acc: 99.2156\n",
      "test Loss: 0.1044 Acc: 97.9339\n",
      "\n",
      "Epoch 430/1199\n",
      "------------------------\n",
      "train Loss: 0.0354 Acc: 99.1986\n",
      "test Loss: 0.1068 Acc: 97.8905\n",
      "\n",
      "Epoch 431/1199\n",
      "------------------------\n",
      "train Loss: 0.0345 Acc: 99.2094\n",
      "test Loss: 0.1066 Acc: 97.8282\n",
      "\n",
      "Epoch 432/1199\n",
      "------------------------\n",
      "train Loss: 0.0352 Acc: 99.2093\n",
      "test Loss: 0.1069 Acc: 97.9004\n",
      "\n",
      "Epoch 433/1199\n",
      "------------------------\n",
      "train Loss: 0.0362 Acc: 99.1912\n",
      "test Loss: 0.1043 Acc: 97.9159\n",
      "\n",
      "Epoch 434/1199\n",
      "------------------------\n",
      "train Loss: 0.0362 Acc: 99.1710\n",
      "test Loss: 0.1097 Acc: 97.8213\n",
      "\n",
      "Epoch 435/1199\n",
      "------------------------\n",
      "train Loss: 0.0360 Acc: 99.1867\n",
      "test Loss: 0.1026 Acc: 97.9741\n",
      "\n",
      "Epoch 436/1199\n",
      "------------------------\n",
      "train Loss: 0.0354 Acc: 99.2059\n",
      "test Loss: 0.1076 Acc: 97.8304\n",
      "\n",
      "Epoch 437/1199\n",
      "------------------------\n",
      "train Loss: 0.0376 Acc: 99.1511\n",
      "test Loss: 0.1057 Acc: 97.9199\n",
      "\n",
      "Epoch 438/1199\n",
      "------------------------\n",
      "train Loss: 0.0363 Acc: 99.1765\n",
      "test Loss: 0.1083 Acc: 97.8595\n",
      "\n",
      "Epoch 439/1199\n",
      "------------------------\n",
      "train Loss: 0.0361 Acc: 99.1787\n",
      "test Loss: 0.1057 Acc: 97.8249\n",
      "\n",
      "Epoch 440/1199\n",
      "------------------------\n",
      "train Loss: 0.0388 Acc: 99.1306\n",
      "test Loss: 0.1061 Acc: 97.8885\n",
      "\n",
      "Epoch 441/1199\n",
      "------------------------\n",
      "train Loss: 0.0356 Acc: 99.2062\n",
      "test Loss: 0.1104 Acc: 97.7819\n",
      "\n",
      "Epoch 442/1199\n",
      "------------------------\n",
      "train Loss: 0.0354 Acc: 99.1897\n",
      "test Loss: 0.1074 Acc: 97.8790\n",
      "\n",
      "Epoch 443/1199\n",
      "------------------------\n",
      "train Loss: 0.0367 Acc: 99.1747\n",
      "test Loss: 0.1057 Acc: 97.9099\n",
      "\n",
      "Epoch 444/1199\n",
      "------------------------\n",
      "train Loss: 0.0357 Acc: 99.1904\n",
      "test Loss: 0.1095 Acc: 97.7712\n",
      "\n",
      "Epoch 445/1199\n",
      "------------------------\n",
      "train Loss: 0.0365 Acc: 99.1681\n",
      "test Loss: 0.1068 Acc: 97.9146\n",
      "\n",
      "Epoch 446/1199\n",
      "------------------------\n",
      "train Loss: 0.0343 Acc: 99.2178\n",
      "test Loss: 0.1080 Acc: 97.8816\n",
      "\n",
      "Epoch 447/1199\n",
      "------------------------\n",
      "train Loss: 0.0355 Acc: 99.1930\n",
      "test Loss: 0.1079 Acc: 97.8904\n",
      "\n",
      "Epoch 448/1199\n",
      "------------------------\n",
      "train Loss: 0.0357 Acc: 99.1898\n",
      "test Loss: 0.1068 Acc: 97.8733\n",
      "\n",
      "Epoch 449/1199\n",
      "------------------------\n",
      "train Loss: 0.0355 Acc: 99.1952\n",
      "test Loss: 0.1068 Acc: 97.8626\n",
      "\n",
      "Epoch 450/1199\n",
      "------------------------\n",
      "train Loss: 0.0339 Acc: 99.2403\n",
      "test Loss: 0.1044 Acc: 97.9254\n",
      "\n",
      "Epoch 451/1199\n",
      "------------------------\n",
      "train Loss: 0.0380 Acc: 99.1353\n",
      "test Loss: 0.1083 Acc: 97.8203\n",
      "\n",
      "Epoch 452/1199\n",
      "------------------------\n",
      "train Loss: 0.0363 Acc: 99.1784\n",
      "test Loss: 0.1063 Acc: 97.8710\n",
      "\n",
      "Epoch 453/1199\n",
      "------------------------\n",
      "train Loss: 0.0420 Acc: 99.0355\n",
      "test Loss: 0.1166 Acc: 97.6835\n",
      "\n",
      "Epoch 454/1199\n",
      "------------------------\n",
      "train Loss: 0.0425 Acc: 99.0150\n",
      "test Loss: 0.1113 Acc: 97.7997\n",
      "\n",
      "Epoch 455/1199\n",
      "------------------------\n",
      "train Loss: 0.0387 Acc: 99.0977\n",
      "test Loss: 0.1103 Acc: 97.7204\n",
      "\n",
      "Epoch 456/1199\n",
      "------------------------\n",
      "train Loss: 0.0372 Acc: 99.1588\n",
      "test Loss: 0.1083 Acc: 97.7899\n",
      "\n",
      "Epoch 457/1199\n",
      "------------------------\n",
      "train Loss: 0.0367 Acc: 99.1650\n",
      "test Loss: 0.1082 Acc: 97.8285\n",
      "\n",
      "Epoch 458/1199\n",
      "------------------------\n",
      "train Loss: 0.0377 Acc: 99.1311\n",
      "test Loss: 0.1073 Acc: 97.8199\n",
      "\n",
      "Epoch 459/1199\n",
      "------------------------\n",
      "train Loss: 0.0397 Acc: 99.1125\n",
      "test Loss: 0.1136 Acc: 97.7268\n",
      "\n",
      "Epoch 460/1199\n",
      "------------------------\n",
      "train Loss: 0.0351 Acc: 99.2046\n",
      "test Loss: 0.1083 Acc: 97.9016\n",
      "\n",
      "Epoch 461/1199\n",
      "------------------------\n",
      "train Loss: 0.0339 Acc: 99.2319\n",
      "test Loss: 0.1066 Acc: 97.8962\n",
      "\n",
      "Epoch 462/1199\n",
      "------------------------\n",
      "train Loss: 0.0395 Acc: 99.1157\n",
      "test Loss: 0.1056 Acc: 97.9014\n",
      "\n",
      "Epoch 463/1199\n",
      "------------------------\n",
      "train Loss: 0.0343 Acc: 99.2317\n",
      "test Loss: 0.1063 Acc: 97.9557\n",
      "\n",
      "Epoch 464/1199\n",
      "------------------------\n",
      "train Loss: 0.0368 Acc: 99.1778\n",
      "test Loss: 0.1103 Acc: 97.8177\n",
      "\n",
      "Epoch 465/1199\n",
      "------------------------\n",
      "train Loss: 0.0344 Acc: 99.2279\n",
      "test Loss: 0.1051 Acc: 97.8942\n",
      "\n",
      "Epoch 466/1199\n",
      "------------------------\n",
      "train Loss: 0.0344 Acc: 99.2271\n",
      "test Loss: 0.1084 Acc: 97.8520\n",
      "\n",
      "Epoch 467/1199\n",
      "------------------------\n",
      "train Loss: 0.0366 Acc: 99.1685\n",
      "test Loss: 0.1097 Acc: 97.8232\n",
      "\n",
      "Epoch 468/1199\n",
      "------------------------\n",
      "train Loss: 0.0361 Acc: 99.1808\n",
      "test Loss: 0.1151 Acc: 97.7558\n",
      "\n",
      "Epoch 469/1199\n",
      "------------------------\n",
      "train Loss: 0.0447 Acc: 98.9827\n",
      "test Loss: 0.1131 Acc: 97.7904\n",
      "\n",
      "Epoch 470/1199\n",
      "------------------------\n",
      "train Loss: 0.0404 Acc: 99.0439\n",
      "test Loss: 0.1091 Acc: 97.8292\n",
      "\n",
      "Epoch 471/1199\n",
      "------------------------\n",
      "train Loss: 0.0381 Acc: 99.1144\n",
      "test Loss: 0.1054 Acc: 97.9009\n",
      "\n",
      "Epoch 472/1199\n",
      "------------------------\n",
      "train Loss: 0.0389 Acc: 99.0950\n",
      "test Loss: 0.1068 Acc: 97.8422\n",
      "\n",
      "Epoch 473/1199\n",
      "------------------------\n",
      "train Loss: 0.0357 Acc: 99.1705\n",
      "test Loss: 0.1108 Acc: 97.8192\n",
      "\n",
      "Epoch 474/1199\n",
      "------------------------\n",
      "train Loss: 0.0362 Acc: 99.1777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1061 Acc: 97.8318\n",
      "\n",
      "Epoch 475/1199\n",
      "------------------------\n",
      "train Loss: 0.0358 Acc: 99.1732\n",
      "test Loss: 0.1069 Acc: 97.8315\n",
      "\n",
      "Epoch 476/1199\n",
      "------------------------\n",
      "train Loss: 0.0362 Acc: 99.1810\n",
      "test Loss: 0.1066 Acc: 97.8940\n",
      "\n",
      "Epoch 477/1199\n",
      "------------------------\n",
      "train Loss: 0.0328 Acc: 99.2405\n",
      "test Loss: 0.1050 Acc: 97.9410\n",
      "\n",
      "Epoch 478/1199\n",
      "------------------------\n",
      "train Loss: 0.0352 Acc: 99.2027\n",
      "test Loss: 0.1072 Acc: 97.8861\n",
      "\n",
      "Epoch 479/1199\n",
      "------------------------\n",
      "train Loss: 0.0341 Acc: 99.2422\n",
      "test Loss: 0.1038 Acc: 97.9766\n",
      "\n",
      "Epoch 480/1199\n",
      "------------------------\n",
      "train Loss: 0.0356 Acc: 99.1981\n",
      "test Loss: 0.1052 Acc: 97.9132\n",
      "\n",
      "Epoch 481/1199\n",
      "------------------------\n",
      "train Loss: 0.0345 Acc: 99.2319\n",
      "test Loss: 0.1037 Acc: 97.9353\n",
      "\n",
      "Epoch 482/1199\n",
      "------------------------\n",
      "train Loss: 0.0365 Acc: 99.1792\n",
      "test Loss: 0.1063 Acc: 97.9548\n",
      "\n",
      "Epoch 483/1199\n",
      "------------------------\n",
      "train Loss: 0.0364 Acc: 99.1791\n",
      "test Loss: 0.1063 Acc: 97.8681\n",
      "\n",
      "Epoch 484/1199\n",
      "------------------------\n",
      "train Loss: 0.0343 Acc: 99.2318\n",
      "test Loss: 0.1039 Acc: 97.9410\n",
      "\n",
      "Epoch 485/1199\n",
      "------------------------\n",
      "train Loss: 0.0326 Acc: 99.2580\n",
      "test Loss: 0.1083 Acc: 97.8676\n",
      "\n",
      "Epoch 486/1199\n",
      "------------------------\n",
      "train Loss: 0.0353 Acc: 99.1967\n",
      "test Loss: 0.1073 Acc: 97.8750\n",
      "\n",
      "Epoch 487/1199\n",
      "------------------------\n",
      "train Loss: 0.0345 Acc: 99.2301\n",
      "test Loss: 0.1060 Acc: 97.9265\n",
      "\n",
      "Epoch 488/1199\n",
      "------------------------\n",
      "train Loss: 0.0330 Acc: 99.2627\n",
      "test Loss: 0.1095 Acc: 97.9154\n",
      "\n",
      "Epoch 489/1199\n",
      "------------------------\n",
      "train Loss: 0.0355 Acc: 99.1912\n",
      "test Loss: 0.1078 Acc: 97.9130\n",
      "\n",
      "Epoch 490/1199\n",
      "------------------------\n",
      "train Loss: 0.0353 Acc: 99.2071\n",
      "test Loss: 0.1083 Acc: 97.9078\n",
      "\n",
      "Epoch 491/1199\n",
      "------------------------\n",
      "train Loss: 0.0336 Acc: 99.2432\n",
      "test Loss: 0.1061 Acc: 97.8729\n",
      "\n",
      "Epoch 492/1199\n",
      "------------------------\n",
      "train Loss: 0.0356 Acc: 99.1920\n",
      "test Loss: 0.1069 Acc: 97.8636\n",
      "\n",
      "Epoch 493/1199\n",
      "------------------------\n",
      "train Loss: 0.0325 Acc: 99.2570\n",
      "test Loss: 0.1096 Acc: 97.7923\n",
      "\n",
      "Epoch 494/1199\n",
      "------------------------\n",
      "train Loss: 0.0345 Acc: 99.2013\n",
      "test Loss: 0.1145 Acc: 97.7809\n",
      "\n",
      "Epoch 495/1199\n",
      "------------------------\n",
      "train Loss: 0.0357 Acc: 99.1937\n",
      "test Loss: 0.1073 Acc: 97.9127\n",
      "\n",
      "Epoch 496/1199\n",
      "------------------------\n",
      "train Loss: 0.0383 Acc: 99.1221\n",
      "test Loss: 0.1081 Acc: 97.8766\n",
      "\n",
      "Epoch 497/1199\n",
      "------------------------\n",
      "train Loss: 0.0353 Acc: 99.1814\n",
      "test Loss: 0.1124 Acc: 97.8133\n",
      "\n",
      "Epoch 498/1199\n",
      "------------------------\n",
      "train Loss: 0.0350 Acc: 99.1969\n",
      "test Loss: 0.1057 Acc: 97.8679\n",
      "\n",
      "Epoch 499/1199\n",
      "------------------------\n",
      "train Loss: 0.0376 Acc: 99.1470\n",
      "test Loss: 0.1093 Acc: 97.8645\n",
      "\n",
      "Epoch 500/1199\n",
      "------------------------\n",
      "train Loss: 0.0340 Acc: 99.2210\n",
      "test Loss: 0.1053 Acc: 97.9543\n",
      "\n",
      "Epoch 501/1199\n",
      "------------------------\n",
      "train Loss: 0.0361 Acc: 99.2004\n",
      "test Loss: 0.1094 Acc: 97.9133\n",
      "\n",
      "Epoch 502/1199\n",
      "------------------------\n",
      "train Loss: 0.0342 Acc: 99.2413\n",
      "test Loss: 0.1064 Acc: 97.8698\n",
      "\n",
      "Epoch 503/1199\n",
      "------------------------\n",
      "train Loss: 0.0331 Acc: 99.2477\n",
      "test Loss: 0.1091 Acc: 97.9044\n",
      "\n",
      "Epoch 504/1199\n",
      "------------------------\n",
      "train Loss: 0.0331 Acc: 99.2498\n",
      "test Loss: 0.1063 Acc: 97.8810\n",
      "\n",
      "Epoch 505/1199\n",
      "------------------------\n",
      "train Loss: 0.0339 Acc: 99.2291\n",
      "test Loss: 0.1056 Acc: 97.9429\n",
      "\n",
      "Epoch 506/1199\n",
      "------------------------\n",
      "train Loss: 0.0317 Acc: 99.2887\n",
      "test Loss: 0.1059 Acc: 97.9140\n",
      "\n",
      "Epoch 507/1199\n",
      "------------------------\n",
      "train Loss: 0.0346 Acc: 99.2209\n",
      "test Loss: 0.1129 Acc: 97.8406\n",
      "\n",
      "Epoch 508/1199\n",
      "------------------------\n",
      "train Loss: 0.0327 Acc: 99.2636\n",
      "test Loss: 0.1064 Acc: 97.8422\n",
      "\n",
      "Epoch 509/1199\n",
      "------------------------\n",
      "train Loss: 0.0330 Acc: 99.2471\n",
      "test Loss: 0.1086 Acc: 97.9299\n",
      "\n",
      "Epoch 510/1199\n",
      "------------------------\n",
      "train Loss: 0.0330 Acc: 99.2616\n",
      "test Loss: 0.1157 Acc: 97.8500\n",
      "\n",
      "Epoch 511/1199\n",
      "------------------------\n",
      "train Loss: 0.0332 Acc: 99.2273\n",
      "test Loss: 0.1064 Acc: 97.9133\n",
      "\n",
      "Epoch 512/1199\n",
      "------------------------\n",
      "train Loss: 0.0334 Acc: 99.2356\n",
      "test Loss: 0.1080 Acc: 97.8999\n",
      "\n",
      "Epoch 513/1199\n",
      "------------------------\n",
      "train Loss: 0.0342 Acc: 99.2236\n",
      "test Loss: 0.1095 Acc: 97.8781\n",
      "\n",
      "Epoch 514/1199\n",
      "------------------------\n",
      "train Loss: 0.0334 Acc: 99.2360\n",
      "test Loss: 0.1085 Acc: 97.8962\n",
      "\n",
      "Epoch 515/1199\n",
      "------------------------\n",
      "train Loss: 0.0317 Acc: 99.2828\n",
      "test Loss: 0.1090 Acc: 97.8228\n",
      "\n",
      "Epoch 516/1199\n",
      "------------------------\n",
      "train Loss: 0.0326 Acc: 99.2577\n",
      "test Loss: 0.1058 Acc: 97.9329\n",
      "\n",
      "Epoch 517/1199\n",
      "------------------------\n",
      "train Loss: 0.0335 Acc: 99.2511\n",
      "test Loss: 0.1076 Acc: 97.9297\n",
      "\n",
      "Epoch 518/1199\n",
      "------------------------\n",
      "train Loss: 0.0324 Acc: 99.2674\n",
      "test Loss: 0.1104 Acc: 97.8714\n",
      "\n",
      "Epoch 519/1199\n",
      "------------------------\n",
      "train Loss: 0.0330 Acc: 99.2584\n",
      "test Loss: 0.1122 Acc: 97.8646\n",
      "\n",
      "Epoch 520/1199\n",
      "------------------------\n",
      "train Loss: 0.0362 Acc: 99.1789\n",
      "test Loss: 0.1105 Acc: 97.8691\n",
      "\n",
      "Epoch 521/1199\n",
      "------------------------\n",
      "train Loss: 0.0372 Acc: 99.1590\n",
      "test Loss: 0.1091 Acc: 97.8643\n",
      "\n",
      "Epoch 522/1199\n",
      "------------------------\n",
      "train Loss: 0.0356 Acc: 99.1894\n",
      "test Loss: 0.1072 Acc: 97.8769\n",
      "\n",
      "Epoch 523/1199\n",
      "------------------------\n",
      "train Loss: 0.0327 Acc: 99.2743\n",
      "test Loss: 0.1082 Acc: 97.8715\n",
      "\n",
      "Epoch 524/1199\n",
      "------------------------\n",
      "train Loss: 0.0314 Acc: 99.2843\n",
      "test Loss: 0.1077 Acc: 97.9156\n",
      "\n",
      "Epoch 525/1199\n",
      "------------------------\n",
      "train Loss: 0.0343 Acc: 99.2327\n",
      "test Loss: 0.1088 Acc: 97.9396\n",
      "\n",
      "Epoch 526/1199\n",
      "------------------------\n",
      "train Loss: 0.0332 Acc: 99.2617\n",
      "test Loss: 0.1119 Acc: 97.8598\n",
      "\n",
      "Epoch 527/1199\n",
      "------------------------\n",
      "train Loss: 0.0330 Acc: 99.2534\n",
      "test Loss: 0.1050 Acc: 97.9731\n",
      "\n",
      "Epoch 528/1199\n",
      "------------------------\n",
      "train Loss: 0.0328 Acc: 99.2687\n",
      "test Loss: 0.1072 Acc: 97.9748\n",
      "\n",
      "Epoch 529/1199\n",
      "------------------------\n",
      "train Loss: 0.0330 Acc: 99.2631\n",
      "test Loss: 0.1078 Acc: 97.9125\n",
      "\n",
      "Epoch 530/1199\n",
      "------------------------\n",
      "train Loss: 0.0319 Acc: 99.2887\n",
      "test Loss: 0.1044 Acc: 97.9522\n",
      "\n",
      "Epoch 531/1199\n",
      "------------------------\n",
      "train Loss: 0.0326 Acc: 99.2644\n",
      "test Loss: 0.1065 Acc: 97.9753\n",
      "\n",
      "Epoch 532/1199\n",
      "------------------------\n",
      "train Loss: 0.0323 Acc: 99.2686\n",
      "test Loss: 0.1082 Acc: 97.8688\n",
      "\n",
      "Epoch 533/1199\n",
      "------------------------\n",
      "train Loss: 0.0336 Acc: 99.2419\n",
      "test Loss: 0.1058 Acc: 97.9719\n",
      "\n",
      "Epoch 534/1199\n",
      "------------------------\n",
      "train Loss: 0.0315 Acc: 99.2821\n",
      "test Loss: 0.1089 Acc: 97.9056\n",
      "\n",
      "Epoch 535/1199\n",
      "------------------------\n",
      "train Loss: 0.0319 Acc: 99.2749\n",
      "test Loss: 0.1050 Acc: 97.9747\n",
      "\n",
      "Epoch 536/1199\n",
      "------------------------\n",
      "train Loss: 0.0328 Acc: 99.2649\n",
      "test Loss: 0.1064 Acc: 98.0180\n",
      "\n",
      "Epoch 537/1199\n",
      "------------------------\n",
      "train Loss: 0.0325 Acc: 99.2703\n",
      "test Loss: 0.1092 Acc: 97.7931\n",
      "\n",
      "Epoch 538/1199\n",
      "------------------------\n",
      "train Loss: 0.0335 Acc: 99.2500\n",
      "test Loss: 0.1072 Acc: 97.9242\n",
      "\n",
      "Epoch 539/1199\n",
      "------------------------\n",
      "train Loss: 0.0339 Acc: 99.2396\n",
      "test Loss: 0.1056 Acc: 97.9726\n",
      "\n",
      "Epoch 540/1199\n",
      "------------------------\n",
      "train Loss: 0.0323 Acc: 99.2663\n",
      "test Loss: 0.1114 Acc: 97.9064\n",
      "\n",
      "Epoch 541/1199\n",
      "------------------------\n",
      "train Loss: 0.0321 Acc: 99.2678\n",
      "test Loss: 0.1053 Acc: 98.0092\n",
      "\n",
      "Epoch 542/1199\n",
      "------------------------\n",
      "train Loss: 0.0324 Acc: 99.2707\n",
      "test Loss: 0.1094 Acc: 97.9451\n",
      "\n",
      "Epoch 543/1199\n",
      "------------------------\n",
      "train Loss: 0.0327 Acc: 99.2647\n",
      "test Loss: 0.1096 Acc: 97.9030\n",
      "\n",
      "Epoch 544/1199\n",
      "------------------------\n",
      "train Loss: 0.0310 Acc: 99.3133\n",
      "test Loss: 0.1081 Acc: 97.9415\n",
      "\n",
      "Epoch 545/1199\n",
      "------------------------\n",
      "train Loss: 0.0328 Acc: 99.2776\n",
      "test Loss: 0.1055 Acc: 97.9994\n",
      "\n",
      "Epoch 546/1199\n",
      "------------------------\n",
      "train Loss: 0.0410 Acc: 99.0769\n",
      "test Loss: 0.1096 Acc: 97.8677\n",
      "\n",
      "Epoch 547/1199\n",
      "------------------------\n",
      "train Loss: 0.0408 Acc: 99.0631\n",
      "test Loss: 0.1124 Acc: 97.8140\n",
      "\n",
      "Epoch 548/1199\n",
      "------------------------\n",
      "train Loss: 0.0392 Acc: 99.0854\n",
      "test Loss: 0.1092 Acc: 97.8182\n",
      "\n",
      "Epoch 549/1199\n",
      "------------------------\n",
      "train Loss: 0.0342 Acc: 99.1978\n",
      "test Loss: 0.1083 Acc: 97.8759\n",
      "\n",
      "Epoch 550/1199\n",
      "------------------------\n",
      "train Loss: 0.0354 Acc: 99.1871\n",
      "test Loss: 0.1090 Acc: 97.8800\n",
      "\n",
      "Epoch 551/1199\n",
      "------------------------\n",
      "train Loss: 0.0339 Acc: 99.2196\n",
      "test Loss: 0.1057 Acc: 97.9242\n",
      "\n",
      "Epoch 552/1199\n",
      "------------------------\n",
      "train Loss: 0.0332 Acc: 99.2383\n",
      "test Loss: 0.1061 Acc: 97.8220\n",
      "\n",
      "Epoch 553/1199\n",
      "------------------------\n",
      "train Loss: 0.0355 Acc: 99.1955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1088 Acc: 97.8515\n",
      "\n",
      "Epoch 554/1199\n",
      "------------------------\n",
      "train Loss: 0.0339 Acc: 99.2415\n",
      "test Loss: 0.1087 Acc: 97.9099\n",
      "\n",
      "Epoch 555/1199\n",
      "------------------------\n",
      "train Loss: 0.0313 Acc: 99.2831\n",
      "test Loss: 0.1070 Acc: 97.8928\n",
      "\n",
      "Epoch 556/1199\n",
      "------------------------\n",
      "train Loss: 0.0332 Acc: 99.2376\n",
      "test Loss: 0.1098 Acc: 97.9275\n",
      "\n",
      "Epoch 557/1199\n",
      "------------------------\n",
      "train Loss: 0.0332 Acc: 99.2466\n",
      "test Loss: 0.1062 Acc: 97.9327\n",
      "\n",
      "Epoch 558/1199\n",
      "------------------------\n",
      "train Loss: 0.0332 Acc: 99.2481\n",
      "test Loss: 0.1090 Acc: 97.9173\n",
      "\n",
      "Epoch 559/1199\n",
      "------------------------\n",
      "train Loss: 0.0300 Acc: 99.3194\n",
      "test Loss: 0.1078 Acc: 97.9095\n",
      "\n",
      "Epoch 560/1199\n",
      "------------------------\n",
      "train Loss: 0.0306 Acc: 99.2955\n",
      "test Loss: 0.1076 Acc: 97.9234\n",
      "\n",
      "Epoch 561/1199\n",
      "------------------------\n",
      "train Loss: 0.0333 Acc: 99.2482\n",
      "test Loss: 0.1070 Acc: 97.9289\n",
      "\n",
      "Epoch 562/1199\n",
      "------------------------\n",
      "train Loss: 0.0317 Acc: 99.2823\n",
      "test Loss: 0.1068 Acc: 97.9729\n",
      "\n",
      "Epoch 563/1199\n",
      "------------------------\n",
      "train Loss: 0.0309 Acc: 99.3139\n",
      "test Loss: 0.1072 Acc: 97.9346\n",
      "\n",
      "Epoch 564/1199\n",
      "------------------------\n",
      "train Loss: 0.0311 Acc: 99.2875\n",
      "test Loss: 0.1077 Acc: 97.9320\n",
      "\n",
      "Epoch 565/1199\n",
      "------------------------\n",
      "train Loss: 0.0315 Acc: 99.2910\n",
      "test Loss: 0.1086 Acc: 97.8696\n",
      "\n",
      "Epoch 566/1199\n",
      "------------------------\n",
      "train Loss: 0.0317 Acc: 99.2855\n",
      "test Loss: 0.1094 Acc: 97.9222\n",
      "\n",
      "Epoch 567/1199\n",
      "------------------------\n",
      "train Loss: 0.0321 Acc: 99.2742\n",
      "test Loss: 0.1079 Acc: 97.9311\n",
      "\n",
      "Epoch 568/1199\n",
      "------------------------\n",
      "train Loss: 0.0320 Acc: 99.2754\n",
      "test Loss: 0.1085 Acc: 97.8810\n",
      "\n",
      "Epoch 569/1199\n",
      "------------------------\n",
      "train Loss: 0.0330 Acc: 99.2458\n",
      "test Loss: 0.1039 Acc: 97.9405\n",
      "\n",
      "Epoch 570/1199\n",
      "------------------------\n",
      "train Loss: 0.0338 Acc: 99.2269\n",
      "test Loss: 0.1076 Acc: 97.9215\n",
      "\n",
      "Epoch 571/1199\n",
      "------------------------\n",
      "train Loss: 0.0338 Acc: 99.2390\n",
      "test Loss: 0.1084 Acc: 97.9116\n",
      "\n",
      "Epoch 572/1199\n",
      "------------------------\n",
      "train Loss: 0.0311 Acc: 99.2960\n",
      "test Loss: 0.1077 Acc: 97.9543\n",
      "\n",
      "Epoch 573/1199\n",
      "------------------------\n",
      "train Loss: 0.0321 Acc: 99.2766\n",
      "test Loss: 0.1101 Acc: 97.8933\n",
      "\n",
      "Epoch 574/1199\n",
      "------------------------\n",
      "train Loss: 0.0322 Acc: 99.2716\n",
      "test Loss: 0.1079 Acc: 97.9719\n",
      "\n",
      "Epoch 575/1199\n",
      "------------------------\n",
      "train Loss: 0.0322 Acc: 99.2708\n",
      "test Loss: 0.1057 Acc: 97.9707\n",
      "\n",
      "Epoch 576/1199\n",
      "------------------------\n",
      "train Loss: 0.0315 Acc: 99.3054\n",
      "test Loss: 0.1080 Acc: 97.9104\n",
      "\n",
      "Epoch 577/1199\n",
      "------------------------\n",
      "train Loss: 0.0317 Acc: 99.2855\n",
      "test Loss: 0.1091 Acc: 97.9000\n",
      "\n",
      "Epoch 578/1199\n",
      "------------------------\n",
      "train Loss: 0.0322 Acc: 99.2776\n",
      "test Loss: 0.1047 Acc: 97.9211\n",
      "\n",
      "Epoch 579/1199\n",
      "------------------------\n",
      "train Loss: 0.0307 Acc: 99.3035\n",
      "test Loss: 0.1069 Acc: 97.9168\n",
      "\n",
      "Epoch 580/1199\n",
      "------------------------\n",
      "train Loss: 0.0289 Acc: 99.3408\n",
      "test Loss: 0.1108 Acc: 97.9253\n",
      "\n",
      "Epoch 581/1199\n",
      "------------------------\n",
      "train Loss: 0.0330 Acc: 99.2628\n",
      "test Loss: 0.1085 Acc: 97.9023\n",
      "\n",
      "Epoch 582/1199\n",
      "------------------------\n",
      "train Loss: 0.0306 Acc: 99.3229\n",
      "test Loss: 0.1077 Acc: 98.0310\n",
      "\n",
      "Epoch 583/1199\n",
      "------------------------\n",
      "train Loss: 0.0319 Acc: 99.2841\n",
      "test Loss: 0.1073 Acc: 98.0142\n",
      "\n",
      "Epoch 584/1199\n",
      "------------------------\n",
      "train Loss: 0.0300 Acc: 99.3341\n",
      "test Loss: 0.1076 Acc: 97.9277\n",
      "\n",
      "Epoch 585/1199\n",
      "------------------------\n",
      "train Loss: 0.0294 Acc: 99.3283\n",
      "test Loss: 0.1096 Acc: 97.8517\n",
      "\n",
      "Epoch 586/1199\n",
      "------------------------\n",
      "train Loss: 0.0297 Acc: 99.3297\n",
      "test Loss: 0.1095 Acc: 97.9184\n",
      "\n",
      "Epoch 587/1199\n",
      "------------------------\n",
      "train Loss: 0.0326 Acc: 99.2752\n",
      "test Loss: 0.1044 Acc: 97.9631\n",
      "\n",
      "Epoch 588/1199\n",
      "------------------------\n",
      "train Loss: 0.0299 Acc: 99.3163\n",
      "test Loss: 0.1041 Acc: 97.9000\n",
      "\n",
      "Epoch 589/1199\n",
      "------------------------\n",
      "train Loss: 0.0303 Acc: 99.3255\n",
      "test Loss: 0.1072 Acc: 97.9764\n",
      "\n",
      "Epoch 590/1199\n",
      "------------------------\n",
      "train Loss: 0.0326 Acc: 99.2747\n",
      "test Loss: 0.1103 Acc: 97.9184\n",
      "\n",
      "Epoch 591/1199\n",
      "------------------------\n",
      "train Loss: 0.0312 Acc: 99.3012\n",
      "test Loss: 0.1092 Acc: 97.9399\n",
      "\n",
      "Epoch 592/1199\n",
      "------------------------\n",
      "train Loss: 0.0330 Acc: 99.2441\n",
      "test Loss: 0.1213 Acc: 97.6458\n",
      "\n",
      "Epoch 593/1199\n",
      "------------------------\n",
      "train Loss: 0.0353 Acc: 99.1706\n",
      "test Loss: 0.1202 Acc: 97.6802\n",
      "\n",
      "Epoch 594/1199\n",
      "------------------------\n",
      "train Loss: 0.0382 Acc: 99.0938\n",
      "test Loss: 0.1106 Acc: 97.8626\n",
      "\n",
      "Epoch 595/1199\n",
      "------------------------\n",
      "train Loss: 0.0352 Acc: 99.1695\n",
      "test Loss: 0.1041 Acc: 97.9645\n",
      "\n",
      "Epoch 596/1199\n",
      "------------------------\n",
      "train Loss: 0.0327 Acc: 99.2432\n",
      "test Loss: 0.1080 Acc: 97.9142\n",
      "\n",
      "Epoch 597/1199\n",
      "------------------------\n",
      "train Loss: 0.0311 Acc: 99.2825\n",
      "test Loss: 0.1070 Acc: 97.9370\n",
      "\n",
      "Epoch 598/1199\n",
      "------------------------\n",
      "train Loss: 0.0325 Acc: 99.2703\n",
      "test Loss: 0.1092 Acc: 97.9482\n",
      "\n",
      "Epoch 599/1199\n",
      "------------------------\n",
      "train Loss: 0.0347 Acc: 99.2169\n",
      "test Loss: 0.1057 Acc: 97.9536\n",
      "\n",
      "Epoch 600/1199\n",
      "------------------------\n",
      "train Loss: 0.0322 Acc: 99.2744\n",
      "test Loss: 0.1059 Acc: 97.9724\n",
      "\n",
      "Epoch 601/1199\n",
      "------------------------\n",
      "train Loss: 0.0322 Acc: 99.2681\n",
      "test Loss: 0.1087 Acc: 97.8942\n",
      "\n",
      "Epoch 602/1199\n",
      "------------------------\n",
      "train Loss: 0.0298 Acc: 99.3416\n",
      "test Loss: 0.1055 Acc: 97.9947\n",
      "\n",
      "Epoch 603/1199\n",
      "------------------------\n",
      "train Loss: 0.0319 Acc: 99.2966\n",
      "test Loss: 0.1100 Acc: 97.9116\n",
      "\n",
      "Epoch 604/1199\n",
      "------------------------\n",
      "train Loss: 0.0303 Acc: 99.3222\n",
      "test Loss: 0.1074 Acc: 97.9481\n",
      "\n",
      "Epoch 605/1199\n",
      "------------------------\n",
      "train Loss: 0.0291 Acc: 99.3598\n",
      "test Loss: 0.1067 Acc: 97.9897\n",
      "\n",
      "Epoch 606/1199\n",
      "------------------------\n",
      "train Loss: 0.0298 Acc: 99.3355\n",
      "test Loss: 0.1058 Acc: 97.9995\n",
      "\n",
      "Epoch 607/1199\n",
      "------------------------\n",
      "train Loss: 0.0308 Acc: 99.3207\n",
      "test Loss: 0.1060 Acc: 97.9809\n",
      "\n",
      "Epoch 608/1199\n",
      "------------------------\n",
      "train Loss: 0.0296 Acc: 99.3337\n",
      "test Loss: 0.1089 Acc: 97.9289\n",
      "\n",
      "Epoch 609/1199\n",
      "------------------------\n",
      "train Loss: 0.0303 Acc: 99.3207\n",
      "test Loss: 0.1071 Acc: 97.9674\n",
      "\n",
      "Epoch 610/1199\n",
      "------------------------\n",
      "train Loss: 0.0313 Acc: 99.3198\n",
      "test Loss: 0.1072 Acc: 97.9538\n",
      "\n",
      "Epoch 611/1199\n",
      "------------------------\n",
      "train Loss: 0.0290 Acc: 99.3564\n",
      "test Loss: 0.1061 Acc: 97.9589\n",
      "\n",
      "Epoch 612/1199\n",
      "------------------------\n",
      "train Loss: 0.0298 Acc: 99.3445\n",
      "test Loss: 0.1051 Acc: 97.9639\n",
      "\n",
      "Epoch 613/1199\n",
      "------------------------\n",
      "train Loss: 0.0305 Acc: 99.3206\n",
      "test Loss: 0.1057 Acc: 97.9479\n",
      "\n",
      "Epoch 614/1199\n",
      "------------------------\n",
      "train Loss: 0.0304 Acc: 99.3156\n",
      "test Loss: 0.1068 Acc: 97.9111\n",
      "\n",
      "Epoch 615/1199\n",
      "------------------------\n",
      "train Loss: 0.0310 Acc: 99.3028\n",
      "test Loss: 0.1068 Acc: 97.9577\n",
      "\n",
      "Epoch 616/1199\n",
      "------------------------\n",
      "train Loss: 0.0298 Acc: 99.3324\n",
      "test Loss: 0.1081 Acc: 97.9755\n",
      "\n",
      "Epoch 617/1199\n",
      "------------------------\n",
      "train Loss: 0.0291 Acc: 99.3476\n",
      "test Loss: 0.1095 Acc: 97.9177\n",
      "\n",
      "Epoch 618/1199\n",
      "------------------------\n",
      "train Loss: 0.0299 Acc: 99.3406\n",
      "test Loss: 0.1054 Acc: 97.9429\n",
      "\n",
      "Epoch 619/1199\n",
      "------------------------\n",
      "train Loss: 0.0288 Acc: 99.3546\n",
      "test Loss: 0.1092 Acc: 97.9743\n",
      "\n",
      "Epoch 620/1199\n",
      "------------------------\n",
      "train Loss: 0.0310 Acc: 99.3065\n",
      "test Loss: 0.1050 Acc: 98.0151\n",
      "\n",
      "Epoch 621/1199\n",
      "------------------------\n",
      "train Loss: 0.0289 Acc: 99.3552\n",
      "test Loss: 0.1078 Acc: 97.9128\n",
      "\n",
      "Epoch 622/1199\n",
      "------------------------\n",
      "train Loss: 0.0289 Acc: 99.3516\n",
      "test Loss: 0.1092 Acc: 98.0066\n",
      "\n",
      "Epoch 623/1199\n",
      "------------------------\n",
      "train Loss: 0.0306 Acc: 99.3141\n",
      "test Loss: 0.1067 Acc: 97.9728\n",
      "\n",
      "Epoch 624/1199\n",
      "------------------------\n",
      "train Loss: 0.0297 Acc: 99.3384\n",
      "test Loss: 0.1085 Acc: 97.9132\n",
      "\n",
      "Epoch 625/1199\n",
      "------------------------\n",
      "train Loss: 0.0296 Acc: 99.3425\n",
      "test Loss: 0.1091 Acc: 97.8971\n",
      "\n",
      "Epoch 626/1199\n",
      "------------------------\n",
      "train Loss: 0.0291 Acc: 99.3362\n",
      "test Loss: 0.1035 Acc: 98.0145\n",
      "\n",
      "Epoch 627/1199\n",
      "------------------------\n",
      "train Loss: 0.0315 Acc: 99.2968\n",
      "test Loss: 0.1185 Acc: 97.7006\n",
      "\n",
      "Epoch 628/1199\n",
      "------------------------\n",
      "train Loss: 0.0367 Acc: 99.1619\n",
      "test Loss: 0.1080 Acc: 97.9436\n",
      "\n",
      "Epoch 629/1199\n",
      "------------------------\n",
      "train Loss: 0.0324 Acc: 99.2745\n",
      "test Loss: 0.1088 Acc: 97.9291\n",
      "\n",
      "Epoch 630/1199\n",
      "------------------------\n",
      "train Loss: 0.0314 Acc: 99.2913\n",
      "test Loss: 0.1045 Acc: 97.9558\n",
      "\n",
      "Epoch 631/1199\n",
      "------------------------\n",
      "train Loss: 0.0374 Acc: 99.1542\n",
      "test Loss: 0.1115 Acc: 97.7821\n",
      "\n",
      "Epoch 632/1199\n",
      "------------------------\n",
      "train Loss: 0.0365 Acc: 99.1448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1110 Acc: 97.8133\n",
      "\n",
      "Epoch 633/1199\n",
      "------------------------\n",
      "train Loss: 0.0344 Acc: 99.2143\n",
      "test Loss: 0.1095 Acc: 97.8728\n",
      "\n",
      "Epoch 634/1199\n",
      "------------------------\n",
      "train Loss: 0.0322 Acc: 99.2451\n",
      "test Loss: 0.1073 Acc: 97.9387\n",
      "\n",
      "Epoch 635/1199\n",
      "------------------------\n",
      "train Loss: 0.0319 Acc: 99.2631\n",
      "test Loss: 0.1103 Acc: 97.9258\n",
      "\n",
      "Epoch 636/1199\n",
      "------------------------\n",
      "train Loss: 0.0339 Acc: 99.2131\n",
      "test Loss: 0.1061 Acc: 97.9180\n",
      "\n",
      "Epoch 637/1199\n",
      "------------------------\n",
      "train Loss: 0.0312 Acc: 99.2868\n",
      "test Loss: 0.1071 Acc: 97.8945\n",
      "\n",
      "Epoch 638/1199\n",
      "------------------------\n",
      "train Loss: 0.0310 Acc: 99.3063\n",
      "test Loss: 0.1068 Acc: 97.9741\n",
      "\n",
      "Epoch 639/1199\n",
      "------------------------\n",
      "train Loss: 0.0306 Acc: 99.3125\n",
      "test Loss: 0.1068 Acc: 97.9918\n",
      "\n",
      "Epoch 640/1199\n",
      "------------------------\n",
      "train Loss: 0.0305 Acc: 99.3179\n",
      "test Loss: 0.1056 Acc: 97.9596\n",
      "\n",
      "Epoch 641/1199\n",
      "------------------------\n",
      "train Loss: 0.0306 Acc: 99.3077\n",
      "test Loss: 0.1041 Acc: 97.9829\n",
      "\n",
      "Epoch 642/1199\n",
      "------------------------\n",
      "train Loss: 0.0295 Acc: 99.3401\n",
      "test Loss: 0.1069 Acc: 97.9382\n",
      "\n",
      "Epoch 643/1199\n",
      "------------------------\n",
      "train Loss: 0.0329 Acc: 99.2745\n",
      "test Loss: 0.1064 Acc: 98.0387\n",
      "\n",
      "Epoch 644/1199\n",
      "------------------------\n",
      "train Loss: 0.0303 Acc: 99.3269\n",
      "test Loss: 0.1089 Acc: 97.8907\n",
      "\n",
      "Epoch 645/1199\n",
      "------------------------\n",
      "train Loss: 0.0296 Acc: 99.3454\n",
      "test Loss: 0.1051 Acc: 97.9709\n",
      "\n",
      "Epoch 646/1199\n",
      "------------------------\n",
      "train Loss: 0.0300 Acc: 99.3264\n",
      "test Loss: 0.1060 Acc: 97.9639\n",
      "\n",
      "Epoch 647/1199\n",
      "------------------------\n",
      "train Loss: 0.0297 Acc: 99.3450\n",
      "test Loss: 0.1072 Acc: 98.0104\n",
      "\n",
      "Epoch 648/1199\n",
      "------------------------\n",
      "train Loss: 0.0290 Acc: 99.3488\n",
      "test Loss: 0.1082 Acc: 97.9667\n",
      "\n",
      "Epoch 649/1199\n",
      "------------------------\n",
      "train Loss: 0.0292 Acc: 99.3554\n",
      "test Loss: 0.1062 Acc: 97.9569\n",
      "\n",
      "Epoch 650/1199\n",
      "------------------------\n",
      "train Loss: 0.0300 Acc: 99.3218\n",
      "test Loss: 0.1059 Acc: 97.9786\n",
      "\n",
      "Epoch 651/1199\n",
      "------------------------\n",
      "train Loss: 0.0285 Acc: 99.3610\n",
      "test Loss: 0.1052 Acc: 98.0018\n",
      "\n",
      "Epoch 652/1199\n",
      "------------------------\n",
      "train Loss: 0.0278 Acc: 99.3816\n",
      "test Loss: 0.1069 Acc: 98.0161\n",
      "\n",
      "Epoch 653/1199\n",
      "------------------------\n",
      "train Loss: 0.0290 Acc: 99.3525\n",
      "test Loss: 0.1075 Acc: 97.9850\n",
      "\n",
      "Epoch 654/1199\n",
      "------------------------\n",
      "train Loss: 0.0282 Acc: 99.3743\n",
      "test Loss: 0.1062 Acc: 97.9470\n",
      "\n",
      "Epoch 655/1199\n",
      "------------------------\n",
      "train Loss: 0.0295 Acc: 99.3459\n",
      "test Loss: 0.1077 Acc: 97.9788\n",
      "\n",
      "Epoch 656/1199\n",
      "------------------------\n",
      "train Loss: 0.0287 Acc: 99.3605\n",
      "test Loss: 0.1022 Acc: 98.0703\n",
      "\n",
      "Epoch 657/1199\n",
      "------------------------\n",
      "train Loss: 0.0293 Acc: 99.3411\n",
      "test Loss: 0.1085 Acc: 97.9360\n",
      "\n",
      "Epoch 658/1199\n",
      "------------------------\n",
      "train Loss: 0.0293 Acc: 99.3532\n",
      "test Loss: 0.1086 Acc: 97.9259\n",
      "\n",
      "Epoch 659/1199\n",
      "------------------------\n",
      "train Loss: 0.0301 Acc: 99.3298\n",
      "test Loss: 0.1073 Acc: 97.9983\n",
      "\n",
      "Epoch 660/1199\n",
      "------------------------\n",
      "train Loss: 0.0284 Acc: 99.3552\n",
      "test Loss: 0.1080 Acc: 97.9439\n",
      "\n",
      "Epoch 661/1199\n",
      "------------------------\n",
      "train Loss: 0.0282 Acc: 99.3696\n",
      "test Loss: 0.1082 Acc: 97.9885\n",
      "\n",
      "Epoch 662/1199\n",
      "------------------------\n",
      "train Loss: 0.0305 Acc: 99.3250\n",
      "test Loss: 0.1100 Acc: 97.9674\n",
      "\n",
      "Epoch 663/1199\n",
      "------------------------\n",
      "train Loss: 0.0291 Acc: 99.3558\n",
      "test Loss: 0.1089 Acc: 97.9638\n",
      "\n",
      "Epoch 664/1199\n",
      "------------------------\n",
      "train Loss: 0.0265 Acc: 99.4011\n",
      "test Loss: 0.1059 Acc: 98.0292\n",
      "\n",
      "Epoch 665/1199\n",
      "------------------------\n",
      "train Loss: 0.0287 Acc: 99.3583\n",
      "test Loss: 0.1084 Acc: 97.9653\n",
      "\n",
      "Epoch 666/1199\n",
      "------------------------\n",
      "train Loss: 0.0292 Acc: 99.3539\n",
      "test Loss: 0.1078 Acc: 97.9738\n",
      "\n",
      "Epoch 667/1199\n",
      "------------------------\n",
      "train Loss: 0.0293 Acc: 99.3425\n",
      "test Loss: 0.1076 Acc: 97.9802\n",
      "\n",
      "Epoch 668/1199\n",
      "------------------------\n",
      "train Loss: 0.0299 Acc: 99.3371\n",
      "test Loss: 0.1076 Acc: 98.0194\n",
      "\n",
      "Epoch 669/1199\n",
      "------------------------\n",
      "train Loss: 0.0280 Acc: 99.3732\n",
      "test Loss: 0.1074 Acc: 97.9862\n",
      "\n",
      "Epoch 670/1199\n",
      "------------------------\n",
      "train Loss: 0.0292 Acc: 99.3406\n",
      "test Loss: 0.1079 Acc: 98.0026\n",
      "\n",
      "Epoch 671/1199\n",
      "------------------------\n",
      "train Loss: 0.0289 Acc: 99.3633\n",
      "test Loss: 0.1078 Acc: 98.0145\n",
      "\n",
      "Epoch 672/1199\n",
      "------------------------\n",
      "train Loss: 0.0291 Acc: 99.3508\n",
      "test Loss: 0.1062 Acc: 98.0113\n",
      "\n",
      "Epoch 673/1199\n",
      "------------------------\n",
      "train Loss: 0.0292 Acc: 99.3560\n",
      "test Loss: 0.1093 Acc: 97.9688\n",
      "\n",
      "Epoch 674/1199\n",
      "------------------------\n",
      "train Loss: 0.0285 Acc: 99.3636\n",
      "test Loss: 0.1076 Acc: 97.9572\n",
      "\n",
      "Epoch 675/1199\n",
      "------------------------\n",
      "train Loss: 0.0284 Acc: 99.3733\n",
      "test Loss: 0.1062 Acc: 98.0099\n",
      "\n",
      "Epoch 676/1199\n",
      "------------------------\n",
      "train Loss: 0.0291 Acc: 99.3597\n",
      "test Loss: 0.1043 Acc: 97.9921\n",
      "\n",
      "Epoch 677/1199\n",
      "------------------------\n",
      "train Loss: 0.0304 Acc: 99.3249\n",
      "test Loss: 0.1073 Acc: 98.0263\n",
      "\n",
      "Epoch 678/1199\n",
      "------------------------\n",
      "train Loss: 0.0289 Acc: 99.3540\n",
      "test Loss: 0.1081 Acc: 97.9508\n",
      "\n",
      "Epoch 679/1199\n",
      "------------------------\n",
      "train Loss: 0.0302 Acc: 99.3309\n",
      "test Loss: 0.1065 Acc: 97.9002\n",
      "\n",
      "Epoch 680/1199\n",
      "------------------------\n",
      "train Loss: 0.0305 Acc: 99.3285\n",
      "test Loss: 0.1102 Acc: 97.9557\n",
      "\n",
      "Epoch 681/1199\n",
      "------------------------\n",
      "train Loss: 0.0280 Acc: 99.3828\n",
      "test Loss: 0.1084 Acc: 97.9665\n",
      "\n",
      "Epoch 682/1199\n",
      "------------------------\n",
      "train Loss: 0.0298 Acc: 99.3310\n",
      "test Loss: 0.1088 Acc: 97.9500\n",
      "\n",
      "Epoch 683/1199\n",
      "------------------------\n",
      "train Loss: 0.0274 Acc: 99.3914\n",
      "test Loss: 0.1092 Acc: 97.9881\n",
      "\n",
      "Epoch 684/1199\n",
      "------------------------\n",
      "train Loss: 0.0288 Acc: 99.3523\n",
      "test Loss: 0.1071 Acc: 97.9802\n",
      "\n",
      "Epoch 685/1199\n",
      "------------------------\n",
      "train Loss: 0.0291 Acc: 99.3541\n",
      "test Loss: 0.1100 Acc: 97.8966\n",
      "\n",
      "Epoch 686/1199\n",
      "------------------------\n",
      "train Loss: 0.0280 Acc: 99.3794\n",
      "test Loss: 0.1076 Acc: 98.0170\n",
      "\n",
      "Epoch 687/1199\n",
      "------------------------\n",
      "train Loss: 0.0269 Acc: 99.3946\n",
      "test Loss: 0.1123 Acc: 97.9843\n",
      "\n",
      "Epoch 688/1199\n",
      "------------------------\n",
      "train Loss: 0.0295 Acc: 99.3420\n",
      "test Loss: 0.1094 Acc: 97.9175\n",
      "\n",
      "Epoch 689/1199\n",
      "------------------------\n",
      "train Loss: 0.0299 Acc: 99.3351\n",
      "test Loss: 0.1062 Acc: 97.9805\n",
      "\n",
      "Epoch 690/1199\n",
      "------------------------\n",
      "train Loss: 0.0282 Acc: 99.3824\n",
      "test Loss: 0.1061 Acc: 97.9966\n",
      "\n",
      "Epoch 691/1199\n",
      "------------------------\n",
      "train Loss: 0.0312 Acc: 99.3235\n",
      "test Loss: 0.1071 Acc: 97.9904\n",
      "\n",
      "Epoch 692/1199\n",
      "------------------------\n",
      "train Loss: 0.0285 Acc: 99.3631\n",
      "test Loss: 0.1068 Acc: 97.9608\n",
      "\n",
      "Epoch 693/1199\n",
      "------------------------\n",
      "train Loss: 0.0293 Acc: 99.3518\n",
      "test Loss: 0.1065 Acc: 97.9786\n",
      "\n",
      "Epoch 694/1199\n",
      "------------------------\n",
      "train Loss: 0.0279 Acc: 99.3890\n",
      "test Loss: 0.1083 Acc: 97.8760\n",
      "\n",
      "Epoch 695/1199\n",
      "------------------------\n",
      "train Loss: 0.0285 Acc: 99.3698\n",
      "test Loss: 0.1063 Acc: 97.9522\n",
      "\n",
      "Epoch 696/1199\n",
      "------------------------\n",
      "train Loss: 0.0275 Acc: 99.3792\n",
      "test Loss: 0.1090 Acc: 97.9228\n",
      "\n",
      "Epoch 697/1199\n",
      "------------------------\n",
      "train Loss: 0.0275 Acc: 99.3821\n",
      "test Loss: 0.1097 Acc: 97.9584\n",
      "\n",
      "Epoch 698/1199\n",
      "------------------------\n",
      "train Loss: 0.0291 Acc: 99.3561\n",
      "test Loss: 0.1069 Acc: 97.9614\n",
      "\n",
      "Epoch 699/1199\n",
      "------------------------\n",
      "train Loss: 0.0284 Acc: 99.3691\n",
      "test Loss: 0.1067 Acc: 98.0201\n",
      "\n",
      "Epoch 700/1199\n",
      "------------------------\n",
      "train Loss: 0.0258 Acc: 99.4142\n",
      "test Loss: 0.1116 Acc: 97.9458\n",
      "\n",
      "Epoch 701/1199\n",
      "------------------------\n",
      "train Loss: 0.0286 Acc: 99.3458\n",
      "test Loss: 0.1067 Acc: 98.0054\n",
      "\n",
      "Epoch 702/1199\n",
      "------------------------\n",
      "train Loss: 0.0268 Acc: 99.3973\n",
      "test Loss: 0.1096 Acc: 97.9337\n",
      "\n",
      "Epoch 703/1199\n",
      "------------------------\n",
      "train Loss: 0.0297 Acc: 99.3387\n",
      "test Loss: 0.1077 Acc: 98.0180\n",
      "\n",
      "Epoch 704/1199\n",
      "------------------------\n",
      "train Loss: 0.0298 Acc: 99.3376\n",
      "test Loss: 0.1095 Acc: 97.9902\n",
      "\n",
      "Epoch 705/1199\n",
      "------------------------\n",
      "train Loss: 0.0270 Acc: 99.3998\n",
      "test Loss: 0.1119 Acc: 97.9534\n",
      "\n",
      "Epoch 706/1199\n",
      "------------------------\n",
      "train Loss: 0.0279 Acc: 99.3834\n",
      "test Loss: 0.1076 Acc: 97.9261\n",
      "\n",
      "Epoch 707/1199\n",
      "------------------------\n",
      "train Loss: 0.0270 Acc: 99.3964\n",
      "test Loss: 0.1054 Acc: 98.0101\n",
      "\n",
      "Epoch 708/1199\n",
      "------------------------\n",
      "train Loss: 0.0307 Acc: 99.3253\n",
      "test Loss: 0.1069 Acc: 98.0550\n",
      "\n",
      "Epoch 709/1199\n",
      "------------------------\n",
      "train Loss: 0.0295 Acc: 99.3577\n",
      "test Loss: 0.1057 Acc: 97.9337\n",
      "\n",
      "Epoch 710/1199\n",
      "------------------------\n",
      "train Loss: 0.0286 Acc: 99.3653\n",
      "test Loss: 0.1104 Acc: 97.9975\n",
      "\n",
      "Epoch 711/1199\n",
      "------------------------\n",
      "train Loss: 0.0293 Acc: 99.3429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1094 Acc: 97.9095\n",
      "\n",
      "Epoch 712/1199\n",
      "------------------------\n",
      "train Loss: 0.0278 Acc: 99.3903\n",
      "test Loss: 0.1085 Acc: 97.9418\n",
      "\n",
      "Epoch 713/1199\n",
      "------------------------\n",
      "train Loss: 0.0278 Acc: 99.3833\n",
      "test Loss: 0.1085 Acc: 98.0351\n",
      "\n",
      "Epoch 714/1199\n",
      "------------------------\n",
      "train Loss: 0.0277 Acc: 99.3805\n",
      "test Loss: 0.1051 Acc: 98.0482\n",
      "\n",
      "Epoch 715/1199\n",
      "------------------------\n",
      "train Loss: 0.0283 Acc: 99.3714\n",
      "test Loss: 0.1094 Acc: 97.9501\n",
      "\n",
      "Epoch 716/1199\n",
      "------------------------\n",
      "train Loss: 0.0286 Acc: 99.3642\n",
      "test Loss: 0.1078 Acc: 97.9838\n",
      "\n",
      "Epoch 717/1199\n",
      "------------------------\n",
      "train Loss: 0.0273 Acc: 99.3939\n",
      "test Loss: 0.1095 Acc: 98.0234\n",
      "\n",
      "Epoch 718/1199\n",
      "------------------------\n",
      "train Loss: 0.0264 Acc: 99.3993\n",
      "test Loss: 0.1052 Acc: 98.0448\n",
      "\n",
      "Epoch 719/1199\n",
      "------------------------\n",
      "train Loss: 0.0283 Acc: 99.3759\n",
      "test Loss: 0.1101 Acc: 97.9318\n",
      "\n",
      "Epoch 720/1199\n",
      "------------------------\n",
      "train Loss: 0.0266 Acc: 99.4060\n",
      "test Loss: 0.1082 Acc: 98.0113\n",
      "\n",
      "Epoch 721/1199\n",
      "------------------------\n",
      "train Loss: 0.0270 Acc: 99.4071\n",
      "test Loss: 0.1108 Acc: 97.8918\n",
      "\n",
      "Epoch 722/1199\n",
      "------------------------\n",
      "train Loss: 0.0277 Acc: 99.3811\n",
      "test Loss: 0.1069 Acc: 97.9975\n",
      "\n",
      "Epoch 723/1199\n",
      "------------------------\n",
      "train Loss: 0.0273 Acc: 99.3906\n",
      "test Loss: 0.1107 Acc: 98.0353\n",
      "\n",
      "Epoch 724/1199\n",
      "------------------------\n",
      "train Loss: 0.0284 Acc: 99.3701\n",
      "test Loss: 0.1061 Acc: 98.0272\n",
      "\n",
      "Epoch 725/1199\n",
      "------------------------\n",
      "train Loss: 0.0281 Acc: 99.3817\n",
      "test Loss: 0.1080 Acc: 98.0109\n",
      "\n",
      "Epoch 726/1199\n",
      "------------------------\n",
      "train Loss: 0.0290 Acc: 99.3827\n",
      "test Loss: 0.1089 Acc: 97.8938\n",
      "\n",
      "Epoch 727/1199\n",
      "------------------------\n",
      "train Loss: 0.0280 Acc: 99.3747\n",
      "test Loss: 0.1065 Acc: 97.9741\n",
      "\n",
      "Epoch 728/1199\n",
      "------------------------\n",
      "train Loss: 0.0294 Acc: 99.3424\n",
      "test Loss: 0.1108 Acc: 97.9807\n",
      "\n",
      "Epoch 729/1199\n",
      "------------------------\n",
      "train Loss: 0.0295 Acc: 99.3436\n",
      "test Loss: 0.1129 Acc: 97.9149\n",
      "\n",
      "Epoch 730/1199\n",
      "------------------------\n",
      "train Loss: 0.0276 Acc: 99.3747\n",
      "test Loss: 0.1120 Acc: 97.8601\n",
      "\n",
      "Epoch 731/1199\n",
      "------------------------\n",
      "train Loss: 0.0293 Acc: 99.3451\n",
      "test Loss: 0.1112 Acc: 97.9299\n",
      "\n",
      "Epoch 732/1199\n",
      "------------------------\n",
      "train Loss: 0.0300 Acc: 99.3243\n",
      "test Loss: 0.1063 Acc: 98.0206\n",
      "\n",
      "Epoch 733/1199\n",
      "------------------------\n",
      "train Loss: 0.0287 Acc: 99.3571\n",
      "test Loss: 0.1110 Acc: 97.9311\n",
      "\n",
      "Epoch 734/1199\n",
      "------------------------\n",
      "train Loss: 0.0312 Acc: 99.2949\n",
      "test Loss: 0.1156 Acc: 97.8209\n",
      "\n",
      "Epoch 735/1199\n",
      "------------------------\n",
      "train Loss: 0.0374 Acc: 99.1672\n",
      "test Loss: 0.1110 Acc: 97.8912\n",
      "\n",
      "Epoch 736/1199\n",
      "------------------------\n",
      "train Loss: 0.0307 Acc: 99.3012\n",
      "test Loss: 0.1057 Acc: 97.9790\n",
      "\n",
      "Epoch 737/1199\n",
      "------------------------\n",
      "train Loss: 0.0320 Acc: 99.2830\n",
      "test Loss: 0.1095 Acc: 97.8546\n",
      "\n",
      "Epoch 738/1199\n",
      "------------------------\n",
      "train Loss: 0.0325 Acc: 99.2463\n",
      "test Loss: 0.1074 Acc: 97.9997\n",
      "\n",
      "Epoch 739/1199\n",
      "------------------------\n",
      "train Loss: 0.0303 Acc: 99.3013\n",
      "test Loss: 0.1104 Acc: 97.9408\n",
      "\n",
      "Epoch 740/1199\n",
      "------------------------\n",
      "train Loss: 0.0302 Acc: 99.3207\n",
      "test Loss: 0.1067 Acc: 97.9614\n",
      "\n",
      "Epoch 741/1199\n",
      "------------------------\n",
      "train Loss: 0.0287 Acc: 99.3399\n",
      "test Loss: 0.1064 Acc: 97.9608\n",
      "\n",
      "Epoch 742/1199\n",
      "------------------------\n",
      "train Loss: 0.0265 Acc: 99.4116\n",
      "test Loss: 0.1089 Acc: 97.8942\n",
      "\n",
      "Epoch 743/1199\n",
      "------------------------\n",
      "train Loss: 0.0280 Acc: 99.3773\n",
      "test Loss: 0.1099 Acc: 97.9360\n",
      "\n",
      "Epoch 744/1199\n",
      "------------------------\n",
      "train Loss: 0.0287 Acc: 99.3588\n",
      "test Loss: 0.1065 Acc: 98.0361\n",
      "\n",
      "Epoch 745/1199\n",
      "------------------------\n",
      "train Loss: 0.0281 Acc: 99.3678\n",
      "test Loss: 0.1064 Acc: 98.0066\n",
      "\n",
      "Epoch 746/1199\n",
      "------------------------\n",
      "train Loss: 0.0288 Acc: 99.3682\n",
      "test Loss: 0.1095 Acc: 97.9712\n",
      "\n",
      "Epoch 747/1199\n",
      "------------------------\n",
      "train Loss: 0.0283 Acc: 99.3690\n",
      "test Loss: 0.1075 Acc: 97.9589\n",
      "\n",
      "Epoch 748/1199\n",
      "------------------------\n",
      "train Loss: 0.0267 Acc: 99.4113\n",
      "test Loss: 0.1079 Acc: 97.9931\n",
      "\n",
      "Epoch 749/1199\n",
      "------------------------\n",
      "train Loss: 0.0271 Acc: 99.4078\n",
      "test Loss: 0.1088 Acc: 97.9942\n",
      "\n",
      "Epoch 750/1199\n",
      "------------------------\n",
      "train Loss: 0.0289 Acc: 99.3652\n",
      "test Loss: 0.1077 Acc: 98.0178\n",
      "\n",
      "Epoch 751/1199\n",
      "------------------------\n",
      "train Loss: 0.0271 Acc: 99.3987\n",
      "test Loss: 0.1099 Acc: 97.9481\n",
      "\n",
      "Epoch 752/1199\n",
      "------------------------\n",
      "train Loss: 0.0282 Acc: 99.3734\n",
      "test Loss: 0.1075 Acc: 98.0097\n",
      "\n",
      "Epoch 753/1199\n",
      "------------------------\n",
      "train Loss: 0.0273 Acc: 99.3904\n",
      "test Loss: 0.1085 Acc: 97.9907\n",
      "\n",
      "Epoch 754/1199\n",
      "------------------------\n",
      "train Loss: 0.0266 Acc: 99.4011\n",
      "test Loss: 0.1084 Acc: 98.0154\n",
      "\n",
      "Epoch 755/1199\n",
      "------------------------\n",
      "train Loss: 0.0285 Acc: 99.3827\n",
      "test Loss: 0.1101 Acc: 98.0025\n",
      "\n",
      "Epoch 756/1199\n",
      "------------------------\n",
      "train Loss: 0.0282 Acc: 99.3690\n",
      "test Loss: 0.1073 Acc: 98.0658\n",
      "\n",
      "Epoch 757/1199\n",
      "------------------------\n",
      "train Loss: 0.0269 Acc: 99.3999\n",
      "test Loss: 0.1098 Acc: 97.9360\n",
      "\n",
      "Epoch 758/1199\n",
      "------------------------\n",
      "train Loss: 0.0280 Acc: 99.3753\n",
      "test Loss: 0.1080 Acc: 98.0275\n",
      "\n",
      "Epoch 759/1199\n",
      "------------------------\n",
      "train Loss: 0.0270 Acc: 99.4007\n",
      "test Loss: 0.1079 Acc: 97.9690\n",
      "\n",
      "Epoch 760/1199\n",
      "------------------------\n",
      "train Loss: 0.0266 Acc: 99.3991\n",
      "test Loss: 0.1099 Acc: 97.9980\n",
      "\n",
      "Epoch 761/1199\n",
      "------------------------\n",
      "train Loss: 0.0286 Acc: 99.3680\n",
      "test Loss: 0.1085 Acc: 98.0128\n",
      "\n",
      "Epoch 762/1199\n",
      "------------------------\n",
      "train Loss: 0.0267 Acc: 99.4160\n",
      "test Loss: 0.1066 Acc: 98.0802\n",
      "\n",
      "Epoch 763/1199\n",
      "------------------------\n",
      "train Loss: 0.0283 Acc: 99.3711\n",
      "test Loss: 0.1099 Acc: 98.0063\n",
      "\n",
      "Epoch 764/1199\n",
      "------------------------\n",
      "train Loss: 0.0264 Acc: 99.4070\n",
      "test Loss: 0.1055 Acc: 97.9548\n",
      "\n",
      "Epoch 765/1199\n",
      "------------------------\n",
      "train Loss: 0.0271 Acc: 99.4081\n",
      "test Loss: 0.1083 Acc: 97.9432\n",
      "\n",
      "Epoch 766/1199\n",
      "------------------------\n",
      "train Loss: 0.0283 Acc: 99.3773\n",
      "test Loss: 0.1101 Acc: 98.0140\n",
      "\n",
      "Epoch 767/1199\n",
      "------------------------\n",
      "train Loss: 0.0276 Acc: 99.3927\n",
      "test Loss: 0.1088 Acc: 98.0244\n",
      "\n",
      "Epoch 768/1199\n",
      "------------------------\n",
      "train Loss: 0.0300 Acc: 99.3510\n",
      "test Loss: 0.1105 Acc: 97.9731\n",
      "\n",
      "Epoch 769/1199\n",
      "------------------------\n",
      "train Loss: 0.0270 Acc: 99.4045\n",
      "test Loss: 0.1063 Acc: 97.9935\n",
      "\n",
      "Epoch 770/1199\n",
      "------------------------\n",
      "train Loss: 0.0270 Acc: 99.3946\n",
      "test Loss: 0.1062 Acc: 97.9330\n",
      "\n",
      "Epoch 771/1199\n",
      "------------------------\n",
      "train Loss: 0.0257 Acc: 99.4304\n",
      "test Loss: 0.1109 Acc: 97.9408\n",
      "\n",
      "Epoch 772/1199\n",
      "------------------------\n",
      "train Loss: 0.0268 Acc: 99.4151\n",
      "test Loss: 0.1083 Acc: 97.9391\n",
      "\n",
      "Epoch 773/1199\n",
      "------------------------\n",
      "train Loss: 0.0257 Acc: 99.4281\n",
      "test Loss: 0.1092 Acc: 97.9448\n",
      "\n",
      "Epoch 774/1199\n",
      "------------------------\n",
      "train Loss: 0.0279 Acc: 99.3881\n",
      "test Loss: 0.1067 Acc: 97.9798\n",
      "\n",
      "Epoch 775/1199\n",
      "------------------------\n",
      "train Loss: 0.0277 Acc: 99.3971\n",
      "test Loss: 0.1101 Acc: 97.9591\n",
      "\n",
      "Epoch 776/1199\n",
      "------------------------\n",
      "train Loss: 0.0275 Acc: 99.3958\n",
      "test Loss: 0.1058 Acc: 97.9776\n",
      "\n",
      "Epoch 777/1199\n",
      "------------------------\n",
      "train Loss: 0.0250 Acc: 99.4450\n",
      "test Loss: 0.1073 Acc: 97.9569\n",
      "\n",
      "Epoch 778/1199\n",
      "------------------------\n",
      "train Loss: 0.0290 Acc: 99.3479\n",
      "test Loss: 0.1068 Acc: 98.0126\n",
      "\n",
      "Epoch 779/1199\n",
      "------------------------\n",
      "train Loss: 0.0276 Acc: 99.3840\n",
      "test Loss: 0.1076 Acc: 98.0063\n",
      "\n",
      "Epoch 780/1199\n",
      "------------------------\n",
      "train Loss: 0.0266 Acc: 99.4083\n",
      "test Loss: 0.1048 Acc: 97.9719\n",
      "\n",
      "Epoch 781/1199\n",
      "------------------------\n",
      "train Loss: 0.0258 Acc: 99.4212\n",
      "test Loss: 0.1064 Acc: 97.9914\n",
      "\n",
      "Epoch 782/1199\n",
      "------------------------\n",
      "train Loss: 0.0277 Acc: 99.3902\n",
      "test Loss: 0.1106 Acc: 98.0076\n",
      "\n",
      "Epoch 783/1199\n",
      "------------------------\n",
      "train Loss: 0.0259 Acc: 99.4320\n",
      "test Loss: 0.1071 Acc: 97.9672\n",
      "\n",
      "Epoch 784/1199\n",
      "------------------------\n",
      "train Loss: 0.0267 Acc: 99.4067\n",
      "test Loss: 0.1101 Acc: 97.9842\n",
      "\n",
      "Epoch 785/1199\n",
      "------------------------\n",
      "train Loss: 0.0262 Acc: 99.4154\n",
      "test Loss: 0.1101 Acc: 97.9563\n",
      "\n",
      "Epoch 786/1199\n",
      "------------------------\n",
      "train Loss: 0.0259 Acc: 99.4356\n",
      "test Loss: 0.1089 Acc: 98.0197\n",
      "\n",
      "Epoch 787/1199\n",
      "------------------------\n",
      "train Loss: 0.0268 Acc: 99.4087\n",
      "test Loss: 0.1089 Acc: 98.0455\n",
      "\n",
      "Epoch 788/1199\n",
      "------------------------\n",
      "train Loss: 0.0267 Acc: 99.4153\n",
      "test Loss: 0.1065 Acc: 98.0111\n",
      "\n",
      "Epoch 789/1199\n",
      "------------------------\n",
      "train Loss: 0.0262 Acc: 99.4159\n",
      "test Loss: 0.1074 Acc: 97.9714\n",
      "\n",
      "Epoch 790/1199\n",
      "------------------------\n",
      "train Loss: 0.0260 Acc: 99.4244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1083 Acc: 98.0382\n",
      "\n",
      "Epoch 791/1199\n",
      "------------------------\n",
      "train Loss: 0.0264 Acc: 99.4092\n",
      "test Loss: 0.1086 Acc: 97.9705\n",
      "\n",
      "Epoch 792/1199\n",
      "------------------------\n",
      "train Loss: 0.0266 Acc: 99.4196\n",
      "test Loss: 0.1080 Acc: 97.9943\n",
      "\n",
      "Epoch 793/1199\n",
      "------------------------\n",
      "train Loss: 0.0280 Acc: 99.3700\n",
      "test Loss: 0.1078 Acc: 98.0268\n",
      "\n",
      "Epoch 794/1199\n",
      "------------------------\n",
      "train Loss: 0.0295 Acc: 99.3557\n",
      "test Loss: 0.1128 Acc: 97.9106\n",
      "\n",
      "Epoch 795/1199\n",
      "------------------------\n",
      "train Loss: 0.0371 Acc: 99.1497\n",
      "test Loss: 0.1142 Acc: 97.8570\n",
      "\n",
      "Epoch 796/1199\n",
      "------------------------\n",
      "train Loss: 0.0335 Acc: 99.2182\n",
      "test Loss: 0.1105 Acc: 97.8743\n",
      "\n",
      "Epoch 797/1199\n",
      "------------------------\n",
      "train Loss: 0.0326 Acc: 99.2645\n",
      "test Loss: 0.1105 Acc: 97.9335\n",
      "\n",
      "Epoch 798/1199\n",
      "------------------------\n",
      "train Loss: 0.0298 Acc: 99.3245\n",
      "test Loss: 0.1111 Acc: 97.8931\n",
      "\n",
      "Epoch 799/1199\n",
      "------------------------\n",
      "train Loss: 0.0297 Acc: 99.3216\n",
      "test Loss: 0.1094 Acc: 97.9638\n",
      "\n",
      "Epoch 800/1199\n",
      "------------------------\n",
      "train Loss: 0.0271 Acc: 99.3897\n",
      "test Loss: 0.1070 Acc: 97.9759\n",
      "\n",
      "Epoch 801/1199\n",
      "------------------------\n",
      "train Loss: 0.0265 Acc: 99.3993\n",
      "test Loss: 0.1119 Acc: 97.8883\n",
      "\n",
      "Epoch 802/1199\n",
      "------------------------\n",
      "train Loss: 0.0280 Acc: 99.3714\n",
      "test Loss: 0.1126 Acc: 97.9857\n",
      "\n",
      "Epoch 803/1199\n",
      "------------------------\n",
      "train Loss: 0.0269 Acc: 99.4077\n",
      "test Loss: 0.1094 Acc: 97.9871\n",
      "\n",
      "Epoch 804/1199\n",
      "------------------------\n",
      "train Loss: 0.0276 Acc: 99.3852\n",
      "test Loss: 0.1067 Acc: 98.0054\n",
      "\n",
      "Epoch 805/1199\n",
      "------------------------\n",
      "train Loss: 0.0261 Acc: 99.4213\n",
      "test Loss: 0.1073 Acc: 97.9992\n",
      "\n",
      "Epoch 806/1199\n",
      "------------------------\n",
      "train Loss: 0.0259 Acc: 99.4231\n",
      "test Loss: 0.1115 Acc: 97.9242\n",
      "\n",
      "Epoch 807/1199\n",
      "------------------------\n",
      "train Loss: 0.0251 Acc: 99.4467\n",
      "test Loss: 0.1111 Acc: 97.9848\n",
      "\n",
      "Epoch 808/1199\n",
      "------------------------\n",
      "train Loss: 0.0273 Acc: 99.3943\n",
      "test Loss: 0.1068 Acc: 97.9819\n",
      "\n",
      "Epoch 809/1199\n",
      "------------------------\n",
      "train Loss: 0.0273 Acc: 99.3990\n",
      "test Loss: 0.1092 Acc: 98.0493\n",
      "\n",
      "Epoch 810/1199\n",
      "------------------------\n",
      "train Loss: 0.0269 Acc: 99.4035\n",
      "test Loss: 0.1084 Acc: 97.9976\n",
      "\n",
      "Epoch 811/1199\n",
      "------------------------\n",
      "train Loss: 0.0267 Acc: 99.4149\n",
      "test Loss: 0.1091 Acc: 97.9356\n",
      "\n",
      "Epoch 812/1199\n",
      "------------------------\n",
      "train Loss: 0.0269 Acc: 99.4062\n",
      "test Loss: 0.1068 Acc: 98.0147\n",
      "\n",
      "Epoch 813/1199\n",
      "------------------------\n",
      "train Loss: 0.0273 Acc: 99.3984\n",
      "test Loss: 0.1128 Acc: 97.9562\n",
      "\n",
      "Epoch 814/1199\n",
      "------------------------\n",
      "train Loss: 0.0276 Acc: 99.3878\n",
      "test Loss: 0.1120 Acc: 97.9748\n",
      "\n",
      "Epoch 815/1199\n",
      "------------------------\n",
      "train Loss: 0.0292 Acc: 99.3476\n",
      "test Loss: 0.1161 Acc: 97.7318\n",
      "\n",
      "Epoch 816/1199\n",
      "------------------------\n",
      "train Loss: 0.0332 Acc: 99.2625\n",
      "test Loss: 0.1147 Acc: 97.7776\n",
      "\n",
      "Epoch 817/1199\n",
      "------------------------\n",
      "train Loss: 0.0348 Acc: 99.2105\n",
      "test Loss: 0.1163 Acc: 97.8161\n",
      "\n",
      "Epoch 818/1199\n",
      "------------------------\n",
      "train Loss: 0.0300 Acc: 99.3100\n",
      "test Loss: 0.1126 Acc: 97.9044\n",
      "\n",
      "Epoch 819/1199\n",
      "------------------------\n",
      "train Loss: 0.0291 Acc: 99.3275\n",
      "test Loss: 0.1103 Acc: 97.9752\n",
      "\n",
      "Epoch 820/1199\n",
      "------------------------\n",
      "train Loss: 0.0301 Acc: 99.3123\n",
      "test Loss: 0.1122 Acc: 97.9280\n",
      "\n",
      "Epoch 821/1199\n",
      "------------------------\n",
      "train Loss: 0.0303 Acc: 99.2999\n",
      "test Loss: 0.1087 Acc: 97.9525\n",
      "\n",
      "Epoch 822/1199\n",
      "------------------------\n",
      "train Loss: 0.0281 Acc: 99.3490\n",
      "test Loss: 0.1135 Acc: 97.8658\n",
      "\n",
      "Epoch 823/1199\n",
      "------------------------\n",
      "train Loss: 0.0292 Acc: 99.3334\n",
      "test Loss: 0.1044 Acc: 98.0075\n",
      "\n",
      "Epoch 824/1199\n",
      "------------------------\n",
      "train Loss: 0.0282 Acc: 99.3583\n",
      "test Loss: 0.1085 Acc: 98.0040\n",
      "\n",
      "Epoch 825/1199\n",
      "------------------------\n",
      "train Loss: 0.0283 Acc: 99.3621\n",
      "test Loss: 0.1076 Acc: 97.9928\n",
      "\n",
      "Epoch 826/1199\n",
      "------------------------\n",
      "train Loss: 0.0278 Acc: 99.3833\n",
      "test Loss: 0.1116 Acc: 97.9515\n",
      "\n",
      "Epoch 827/1199\n",
      "------------------------\n",
      "train Loss: 0.0274 Acc: 99.3893\n",
      "test Loss: 0.1123 Acc: 97.9579\n",
      "\n",
      "Epoch 828/1199\n",
      "------------------------\n",
      "train Loss: 0.0265 Acc: 99.4165\n",
      "test Loss: 0.1068 Acc: 98.0040\n",
      "\n",
      "Epoch 829/1199\n",
      "------------------------\n",
      "train Loss: 0.0257 Acc: 99.4225\n",
      "test Loss: 0.1060 Acc: 98.0871\n",
      "\n",
      "Epoch 830/1199\n",
      "------------------------\n",
      "train Loss: 0.0252 Acc: 99.4462\n",
      "test Loss: 0.1089 Acc: 97.9964\n",
      "\n",
      "Epoch 831/1199\n",
      "------------------------\n",
      "train Loss: 0.0254 Acc: 99.4345\n",
      "test Loss: 0.1058 Acc: 98.0156\n",
      "\n",
      "Epoch 832/1199\n",
      "------------------------\n",
      "train Loss: 0.0268 Acc: 99.4003\n",
      "test Loss: 0.1079 Acc: 98.0710\n",
      "\n",
      "Epoch 833/1199\n",
      "------------------------\n",
      "train Loss: 0.0254 Acc: 99.4355\n",
      "test Loss: 0.1057 Acc: 98.0626\n",
      "\n",
      "Epoch 834/1199\n",
      "------------------------\n",
      "train Loss: 0.0259 Acc: 99.4231\n",
      "test Loss: 0.1076 Acc: 98.0087\n",
      "\n",
      "Epoch 835/1199\n",
      "------------------------\n",
      "train Loss: 0.0252 Acc: 99.4381\n",
      "test Loss: 0.1125 Acc: 97.9589\n",
      "\n",
      "Epoch 836/1199\n",
      "------------------------\n",
      "train Loss: 0.0267 Acc: 99.4146\n",
      "test Loss: 0.1115 Acc: 98.0227\n",
      "\n",
      "Epoch 837/1199\n",
      "------------------------\n",
      "train Loss: 0.0282 Acc: 99.3871\n",
      "test Loss: 0.1092 Acc: 98.0209\n",
      "\n",
      "Epoch 838/1199\n",
      "------------------------\n",
      "train Loss: 0.0254 Acc: 99.4416\n",
      "test Loss: 0.1089 Acc: 98.0173\n",
      "\n",
      "Epoch 839/1199\n",
      "------------------------\n",
      "train Loss: 0.0273 Acc: 99.3928\n",
      "test Loss: 0.1091 Acc: 98.0185\n",
      "\n",
      "Epoch 840/1199\n",
      "------------------------\n",
      "train Loss: 0.0272 Acc: 99.3977\n",
      "test Loss: 0.1068 Acc: 98.0183\n",
      "\n",
      "Epoch 841/1199\n",
      "------------------------\n",
      "train Loss: 0.0253 Acc: 99.4294\n",
      "test Loss: 0.1047 Acc: 98.0301\n",
      "\n",
      "Epoch 842/1199\n",
      "------------------------\n",
      "train Loss: 0.0269 Acc: 99.4067\n",
      "test Loss: 0.1123 Acc: 98.0071\n",
      "\n",
      "Epoch 843/1199\n",
      "------------------------\n",
      "train Loss: 0.0253 Acc: 99.4381\n",
      "test Loss: 0.1088 Acc: 98.0152\n",
      "\n",
      "Epoch 844/1199\n",
      "------------------------\n",
      "train Loss: 0.0264 Acc: 99.4099\n",
      "test Loss: 0.1082 Acc: 98.0163\n",
      "\n",
      "Epoch 845/1199\n",
      "------------------------\n",
      "train Loss: 0.0265 Acc: 99.4104\n",
      "test Loss: 0.1073 Acc: 98.0199\n",
      "\n",
      "Epoch 846/1199\n",
      "------------------------\n",
      "train Loss: 0.0261 Acc: 99.4261\n",
      "test Loss: 0.1084 Acc: 98.0135\n",
      "\n",
      "Epoch 847/1199\n",
      "------------------------\n",
      "train Loss: 0.0253 Acc: 99.4474\n",
      "test Loss: 0.1100 Acc: 98.0030\n",
      "\n",
      "Epoch 848/1199\n",
      "------------------------\n",
      "train Loss: 0.0257 Acc: 99.4332\n",
      "test Loss: 0.1097 Acc: 97.8966\n",
      "\n",
      "Epoch 849/1199\n",
      "------------------------\n",
      "train Loss: 0.0255 Acc: 99.4293\n",
      "test Loss: 0.1074 Acc: 98.0178\n",
      "\n",
      "Epoch 850/1199\n",
      "------------------------\n",
      "train Loss: 0.0264 Acc: 99.4149\n",
      "test Loss: 0.1109 Acc: 97.9905\n",
      "\n",
      "Epoch 851/1199\n",
      "------------------------\n",
      "train Loss: 0.0263 Acc: 99.4223\n",
      "test Loss: 0.1103 Acc: 98.0268\n",
      "\n",
      "Epoch 852/1199\n",
      "------------------------\n",
      "train Loss: 0.0245 Acc: 99.4523\n",
      "test Loss: 0.1076 Acc: 97.9942\n",
      "\n",
      "Epoch 853/1199\n",
      "------------------------\n",
      "train Loss: 0.0256 Acc: 99.4335\n",
      "test Loss: 0.1082 Acc: 98.0125\n",
      "\n",
      "Epoch 854/1199\n",
      "------------------------\n",
      "train Loss: 0.0265 Acc: 99.4179\n",
      "test Loss: 0.1098 Acc: 97.9272\n",
      "\n",
      "Epoch 855/1199\n",
      "------------------------\n",
      "train Loss: 0.0266 Acc: 99.4250\n",
      "test Loss: 0.1078 Acc: 98.0215\n",
      "\n",
      "Epoch 856/1199\n",
      "------------------------\n",
      "train Loss: 0.0248 Acc: 99.4472\n",
      "test Loss: 0.1090 Acc: 97.9862\n",
      "\n",
      "Epoch 857/1199\n",
      "------------------------\n",
      "train Loss: 0.0249 Acc: 99.4586\n",
      "test Loss: 0.1111 Acc: 97.9154\n",
      "\n",
      "Epoch 858/1199\n",
      "------------------------\n",
      "train Loss: 0.0249 Acc: 99.4433\n",
      "test Loss: 0.1107 Acc: 98.0047\n",
      "\n",
      "Epoch 859/1199\n",
      "------------------------\n",
      "train Loss: 0.0256 Acc: 99.4358\n",
      "test Loss: 0.1095 Acc: 97.9956\n",
      "\n",
      "Epoch 860/1199\n",
      "------------------------\n",
      "train Loss: 0.0246 Acc: 99.4470\n",
      "test Loss: 0.1095 Acc: 97.9672\n",
      "\n",
      "Epoch 861/1199\n",
      "------------------------\n",
      "train Loss: 0.0278 Acc: 99.3892\n",
      "test Loss: 0.1056 Acc: 98.0392\n",
      "\n",
      "Epoch 862/1199\n",
      "------------------------\n",
      "train Loss: 0.0262 Acc: 99.4184\n",
      "test Loss: 0.1081 Acc: 98.0429\n",
      "\n",
      "Epoch 863/1199\n",
      "------------------------\n",
      "train Loss: 0.0260 Acc: 99.4188\n",
      "test Loss: 0.1069 Acc: 98.0365\n",
      "\n",
      "Epoch 864/1199\n",
      "------------------------\n",
      "train Loss: 0.0243 Acc: 99.4527\n",
      "test Loss: 0.1113 Acc: 98.0139\n",
      "\n",
      "Epoch 865/1199\n",
      "------------------------\n",
      "train Loss: 0.0259 Acc: 99.4356\n",
      "test Loss: 0.1109 Acc: 97.9862\n",
      "\n",
      "Epoch 866/1199\n",
      "------------------------\n",
      "train Loss: 0.0261 Acc: 99.4245\n",
      "test Loss: 0.1106 Acc: 98.0574\n",
      "\n",
      "Epoch 867/1199\n",
      "------------------------\n",
      "train Loss: 0.0253 Acc: 99.4435\n",
      "test Loss: 0.1073 Acc: 98.0382\n",
      "\n",
      "Epoch 868/1199\n",
      "------------------------\n",
      "train Loss: 0.0251 Acc: 99.4464\n",
      "test Loss: 0.1051 Acc: 98.0584\n",
      "\n",
      "Epoch 869/1199\n",
      "------------------------\n",
      "train Loss: 0.0256 Acc: 99.4351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1119 Acc: 97.9835\n",
      "\n",
      "Epoch 870/1199\n",
      "------------------------\n",
      "train Loss: 0.0276 Acc: 99.3843\n",
      "test Loss: 0.1086 Acc: 98.0401\n",
      "\n",
      "Epoch 871/1199\n",
      "------------------------\n",
      "train Loss: 0.0254 Acc: 99.4432\n",
      "test Loss: 0.1080 Acc: 98.0033\n",
      "\n",
      "Epoch 872/1199\n",
      "------------------------\n",
      "train Loss: 0.0263 Acc: 99.4303\n",
      "test Loss: 0.1094 Acc: 98.0102\n",
      "\n",
      "Epoch 873/1199\n",
      "------------------------\n",
      "train Loss: 0.0246 Acc: 99.4495\n",
      "test Loss: 0.1112 Acc: 97.9753\n",
      "\n",
      "Epoch 874/1199\n",
      "------------------------\n",
      "train Loss: 0.0263 Acc: 99.4235\n",
      "test Loss: 0.1119 Acc: 97.9769\n",
      "\n",
      "Epoch 875/1199\n",
      "------------------------\n",
      "train Loss: 0.0279 Acc: 99.3851\n",
      "test Loss: 0.1112 Acc: 98.0080\n",
      "\n",
      "Epoch 876/1199\n",
      "------------------------\n",
      "train Loss: 0.0255 Acc: 99.4404\n",
      "test Loss: 0.1068 Acc: 98.0460\n",
      "\n",
      "Epoch 877/1199\n",
      "------------------------\n",
      "train Loss: 0.0253 Acc: 99.4366\n",
      "test Loss: 0.1102 Acc: 97.9935\n",
      "\n",
      "Epoch 878/1199\n",
      "------------------------\n",
      "train Loss: 0.0259 Acc: 99.4292\n",
      "test Loss: 0.1107 Acc: 98.0023\n",
      "\n",
      "Epoch 879/1199\n",
      "------------------------\n",
      "train Loss: 0.0262 Acc: 99.4450\n",
      "test Loss: 0.1092 Acc: 98.0016\n",
      "\n",
      "Epoch 880/1199\n",
      "------------------------\n",
      "train Loss: 0.0266 Acc: 99.4169\n",
      "test Loss: 0.1089 Acc: 98.0353\n",
      "\n",
      "Epoch 881/1199\n",
      "------------------------\n",
      "train Loss: 0.0259 Acc: 99.4172\n",
      "test Loss: 0.1118 Acc: 97.9886\n",
      "\n",
      "Epoch 882/1199\n",
      "------------------------\n",
      "train Loss: 0.0255 Acc: 99.4309\n",
      "test Loss: 0.1083 Acc: 98.0373\n",
      "\n",
      "Epoch 883/1199\n",
      "------------------------\n",
      "train Loss: 0.0255 Acc: 99.4413\n",
      "test Loss: 0.1097 Acc: 98.0645\n",
      "\n",
      "Epoch 884/1199\n",
      "------------------------\n",
      "train Loss: 0.0231 Acc: 99.4762\n",
      "test Loss: 0.1094 Acc: 98.0082\n",
      "\n",
      "Epoch 885/1199\n",
      "------------------------\n",
      "train Loss: 0.0251 Acc: 99.4498\n",
      "test Loss: 0.1069 Acc: 98.0481\n",
      "\n",
      "Epoch 886/1199\n",
      "------------------------\n",
      "train Loss: 0.0255 Acc: 99.4315\n",
      "test Loss: 0.1071 Acc: 98.0013\n",
      "\n",
      "Epoch 887/1199\n",
      "------------------------\n",
      "train Loss: 0.0271 Acc: 99.4014\n",
      "test Loss: 0.1074 Acc: 98.0235\n",
      "\n",
      "Epoch 888/1199\n",
      "------------------------\n",
      "train Loss: 0.0247 Acc: 99.4573\n",
      "test Loss: 0.1114 Acc: 97.9215\n",
      "\n",
      "Epoch 889/1199\n",
      "------------------------\n",
      "train Loss: 0.0245 Acc: 99.4453\n",
      "test Loss: 0.1076 Acc: 97.9320\n",
      "\n",
      "Epoch 890/1199\n",
      "------------------------\n",
      "train Loss: 0.0272 Acc: 99.3953\n",
      "test Loss: 0.1116 Acc: 98.0076\n",
      "\n",
      "Epoch 891/1199\n",
      "------------------------\n",
      "train Loss: 0.0252 Acc: 99.4395\n",
      "test Loss: 0.1115 Acc: 97.9596\n",
      "\n",
      "Epoch 892/1199\n",
      "------------------------\n",
      "train Loss: 0.0262 Acc: 99.4205\n",
      "test Loss: 0.1106 Acc: 98.0139\n",
      "\n",
      "Epoch 893/1199\n",
      "------------------------\n",
      "train Loss: 0.0282 Acc: 99.3767\n",
      "test Loss: 0.1087 Acc: 97.9804\n",
      "\n",
      "Epoch 894/1199\n",
      "------------------------\n",
      "train Loss: 0.0246 Acc: 99.4591\n",
      "test Loss: 0.1098 Acc: 98.0056\n",
      "\n",
      "Epoch 895/1199\n",
      "------------------------\n",
      "train Loss: 0.0254 Acc: 99.4372\n",
      "test Loss: 0.1120 Acc: 97.9805\n",
      "\n",
      "Epoch 896/1199\n",
      "------------------------\n",
      "train Loss: 0.0272 Acc: 99.4081\n",
      "test Loss: 0.1087 Acc: 98.0208\n",
      "\n",
      "Epoch 897/1199\n",
      "------------------------\n",
      "train Loss: 0.0250 Acc: 99.4601\n",
      "test Loss: 0.1073 Acc: 98.0064\n",
      "\n",
      "Epoch 898/1199\n",
      "------------------------\n",
      "train Loss: 0.0262 Acc: 99.4174\n",
      "test Loss: 0.1092 Acc: 98.0140\n",
      "\n",
      "Epoch 899/1199\n",
      "------------------------\n",
      "train Loss: 0.0251 Acc: 99.4439\n",
      "test Loss: 0.1094 Acc: 98.0318\n",
      "\n",
      "Epoch 900/1199\n",
      "------------------------\n",
      "train Loss: 0.0261 Acc: 99.4423\n",
      "test Loss: 0.1083 Acc: 98.0227\n",
      "\n",
      "Epoch 901/1199\n",
      "------------------------\n",
      "train Loss: 0.0244 Acc: 99.4667\n",
      "test Loss: 0.1097 Acc: 98.0513\n",
      "\n",
      "Epoch 902/1199\n",
      "------------------------\n",
      "train Loss: 0.0237 Acc: 99.4690\n",
      "test Loss: 0.1114 Acc: 98.0292\n",
      "\n",
      "Epoch 903/1199\n",
      "------------------------\n",
      "train Loss: 0.0250 Acc: 99.4498\n",
      "test Loss: 0.1149 Acc: 97.7957\n",
      "\n",
      "Epoch 904/1199\n",
      "------------------------\n",
      "train Loss: 0.0239 Acc: 99.4806\n",
      "test Loss: 0.1084 Acc: 98.0239\n",
      "\n",
      "Epoch 905/1199\n",
      "------------------------\n",
      "train Loss: 0.0253 Acc: 99.4529\n",
      "test Loss: 0.1127 Acc: 97.9645\n",
      "\n",
      "Epoch 906/1199\n",
      "------------------------\n",
      "train Loss: 0.0244 Acc: 99.4710\n",
      "test Loss: 0.1107 Acc: 98.0734\n",
      "\n",
      "Epoch 907/1199\n",
      "------------------------\n",
      "train Loss: 0.0249 Acc: 99.4599\n",
      "test Loss: 0.1100 Acc: 97.9399\n",
      "\n",
      "Epoch 908/1199\n",
      "------------------------\n",
      "train Loss: 0.0254 Acc: 99.4454\n",
      "test Loss: 0.1090 Acc: 98.0524\n",
      "\n",
      "Epoch 909/1199\n",
      "------------------------\n",
      "train Loss: 0.0238 Acc: 99.4788\n",
      "test Loss: 0.1107 Acc: 97.9752\n",
      "\n",
      "Epoch 910/1199\n",
      "------------------------\n",
      "train Loss: 0.0245 Acc: 99.4730\n",
      "test Loss: 0.1090 Acc: 98.0221\n",
      "\n",
      "Epoch 911/1199\n",
      "------------------------\n",
      "train Loss: 0.0251 Acc: 99.4634\n",
      "test Loss: 0.1068 Acc: 98.0104\n",
      "\n",
      "Epoch 912/1199\n",
      "------------------------\n",
      "train Loss: 0.0249 Acc: 99.4590\n",
      "test Loss: 0.1094 Acc: 98.0722\n",
      "\n",
      "Epoch 913/1199\n",
      "------------------------\n",
      "train Loss: 0.0272 Acc: 99.4144\n",
      "test Loss: 0.1088 Acc: 98.0551\n",
      "\n",
      "Epoch 914/1199\n",
      "------------------------\n",
      "train Loss: 0.0240 Acc: 99.4722\n",
      "test Loss: 0.1086 Acc: 98.0769\n",
      "\n",
      "Epoch 915/1199\n",
      "------------------------\n",
      "train Loss: 0.0245 Acc: 99.4635\n",
      "test Loss: 0.1061 Acc: 98.0391\n",
      "\n",
      "Epoch 916/1199\n",
      "------------------------\n",
      "train Loss: 0.0243 Acc: 99.4773\n",
      "test Loss: 0.1065 Acc: 98.0582\n",
      "\n",
      "Epoch 917/1199\n",
      "------------------------\n",
      "train Loss: 0.0236 Acc: 99.4868\n",
      "test Loss: 0.1086 Acc: 98.0505\n",
      "\n",
      "Epoch 918/1199\n",
      "------------------------\n",
      "train Loss: 0.0255 Acc: 99.4381\n",
      "test Loss: 0.1101 Acc: 98.0228\n",
      "\n",
      "Epoch 919/1199\n",
      "------------------------\n",
      "train Loss: 0.0239 Acc: 99.4789\n",
      "test Loss: 0.1104 Acc: 97.9807\n",
      "\n",
      "Epoch 920/1199\n",
      "------------------------\n",
      "train Loss: 0.0240 Acc: 99.4881\n",
      "test Loss: 0.1089 Acc: 98.0484\n",
      "\n",
      "Epoch 921/1199\n",
      "------------------------\n",
      "train Loss: 0.0248 Acc: 99.4579\n",
      "test Loss: 0.1068 Acc: 98.0389\n",
      "\n",
      "Epoch 922/1199\n",
      "------------------------\n",
      "train Loss: 0.0240 Acc: 99.4786\n",
      "test Loss: 0.1094 Acc: 98.0703\n",
      "\n",
      "Epoch 923/1199\n",
      "------------------------\n",
      "train Loss: 0.0247 Acc: 99.4687\n",
      "test Loss: 0.1086 Acc: 98.0114\n",
      "\n",
      "Epoch 924/1199\n",
      "------------------------\n",
      "train Loss: 0.0252 Acc: 99.4537\n",
      "test Loss: 0.1108 Acc: 98.0738\n",
      "\n",
      "Epoch 925/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.4867\n",
      "test Loss: 0.1128 Acc: 97.9878\n",
      "\n",
      "Epoch 926/1199\n",
      "------------------------\n",
      "train Loss: 0.0236 Acc: 99.4842\n",
      "test Loss: 0.1127 Acc: 97.9589\n",
      "\n",
      "Epoch 927/1199\n",
      "------------------------\n",
      "train Loss: 0.0237 Acc: 99.4803\n",
      "test Loss: 0.1124 Acc: 97.9726\n",
      "\n",
      "Epoch 928/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.4944\n",
      "test Loss: 0.1114 Acc: 98.0456\n",
      "\n",
      "Epoch 929/1199\n",
      "------------------------\n",
      "train Loss: 0.0234 Acc: 99.4919\n",
      "test Loss: 0.1115 Acc: 98.0456\n",
      "\n",
      "Epoch 930/1199\n",
      "------------------------\n",
      "train Loss: 0.0259 Acc: 99.4346\n",
      "test Loss: 0.1082 Acc: 98.0665\n",
      "\n",
      "Epoch 931/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.4921\n",
      "test Loss: 0.1113 Acc: 98.0244\n",
      "\n",
      "Epoch 932/1199\n",
      "------------------------\n",
      "train Loss: 0.0244 Acc: 99.4641\n",
      "test Loss: 0.1081 Acc: 98.0525\n",
      "\n",
      "Epoch 933/1199\n",
      "------------------------\n",
      "train Loss: 0.0240 Acc: 99.4772\n",
      "test Loss: 0.1118 Acc: 98.0481\n",
      "\n",
      "Epoch 934/1199\n",
      "------------------------\n",
      "train Loss: 0.0248 Acc: 99.4596\n",
      "test Loss: 0.1099 Acc: 98.0810\n",
      "\n",
      "Epoch 935/1199\n",
      "------------------------\n",
      "train Loss: 0.0240 Acc: 99.4808\n",
      "test Loss: 0.1102 Acc: 98.0786\n",
      "\n",
      "Epoch 936/1199\n",
      "------------------------\n",
      "train Loss: 0.0251 Acc: 99.4565\n",
      "test Loss: 0.1124 Acc: 97.9721\n",
      "\n",
      "Epoch 937/1199\n",
      "------------------------\n",
      "train Loss: 0.0245 Acc: 99.4576\n",
      "test Loss: 0.1118 Acc: 98.1009\n",
      "\n",
      "Epoch 938/1199\n",
      "------------------------\n",
      "train Loss: 0.0253 Acc: 99.4549\n",
      "test Loss: 0.1125 Acc: 98.0715\n",
      "\n",
      "Epoch 939/1199\n",
      "------------------------\n",
      "train Loss: 0.0262 Acc: 99.4363\n",
      "test Loss: 0.1148 Acc: 97.9436\n",
      "\n",
      "Epoch 940/1199\n",
      "------------------------\n",
      "train Loss: 0.0245 Acc: 99.4793\n",
      "test Loss: 0.1104 Acc: 98.0056\n",
      "\n",
      "Epoch 941/1199\n",
      "------------------------\n",
      "train Loss: 0.0240 Acc: 99.4846\n",
      "test Loss: 0.1110 Acc: 98.0420\n",
      "\n",
      "Epoch 942/1199\n",
      "------------------------\n",
      "train Loss: 0.0230 Acc: 99.4923\n",
      "test Loss: 0.1102 Acc: 98.0807\n",
      "\n",
      "Epoch 943/1199\n",
      "------------------------\n",
      "train Loss: 0.0236 Acc: 99.4957\n",
      "test Loss: 0.1103 Acc: 98.0703\n",
      "\n",
      "Epoch 944/1199\n",
      "------------------------\n",
      "train Loss: 0.0259 Acc: 99.4411\n",
      "test Loss: 0.1108 Acc: 98.0204\n",
      "\n",
      "Epoch 945/1199\n",
      "------------------------\n",
      "train Loss: 0.0222 Acc: 99.5178\n",
      "test Loss: 0.1107 Acc: 98.0430\n",
      "\n",
      "Epoch 946/1199\n",
      "------------------------\n",
      "train Loss: 0.0235 Acc: 99.4884\n",
      "test Loss: 0.1088 Acc: 98.0063\n",
      "\n",
      "Epoch 947/1199\n",
      "------------------------\n",
      "train Loss: 0.0235 Acc: 99.4824\n",
      "test Loss: 0.1067 Acc: 98.0574\n",
      "\n",
      "Epoch 948/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.4933\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1089 Acc: 98.0529\n",
      "\n",
      "Epoch 949/1199\n",
      "------------------------\n",
      "train Loss: 0.0237 Acc: 99.4877\n",
      "test Loss: 0.1105 Acc: 97.9658\n",
      "\n",
      "Epoch 950/1199\n",
      "------------------------\n",
      "train Loss: 0.0251 Acc: 99.4473\n",
      "test Loss: 0.1093 Acc: 98.0258\n",
      "\n",
      "Epoch 951/1199\n",
      "------------------------\n",
      "train Loss: 0.0230 Acc: 99.5051\n",
      "test Loss: 0.1096 Acc: 97.9959\n",
      "\n",
      "Epoch 952/1199\n",
      "------------------------\n",
      "train Loss: 0.0231 Acc: 99.4964\n",
      "test Loss: 0.1093 Acc: 98.0486\n",
      "\n",
      "Epoch 953/1199\n",
      "------------------------\n",
      "train Loss: 0.0248 Acc: 99.4644\n",
      "test Loss: 0.1124 Acc: 98.0111\n",
      "\n",
      "Epoch 954/1199\n",
      "------------------------\n",
      "train Loss: 0.0250 Acc: 99.4566\n",
      "test Loss: 0.1083 Acc: 97.9888\n",
      "\n",
      "Epoch 955/1199\n",
      "------------------------\n",
      "train Loss: 0.0236 Acc: 99.4764\n",
      "test Loss: 0.1085 Acc: 98.0643\n",
      "\n",
      "Epoch 956/1199\n",
      "------------------------\n",
      "train Loss: 0.0246 Acc: 99.4602\n",
      "test Loss: 0.1108 Acc: 98.0102\n",
      "\n",
      "Epoch 957/1199\n",
      "------------------------\n",
      "train Loss: 0.0237 Acc: 99.4757\n",
      "test Loss: 0.1109 Acc: 97.9762\n",
      "\n",
      "Epoch 958/1199\n",
      "------------------------\n",
      "train Loss: 0.0239 Acc: 99.4814\n",
      "test Loss: 0.1122 Acc: 98.0021\n",
      "\n",
      "Epoch 959/1199\n",
      "------------------------\n",
      "train Loss: 0.0251 Acc: 99.4536\n",
      "test Loss: 0.1102 Acc: 97.9964\n",
      "\n",
      "Epoch 960/1199\n",
      "------------------------\n",
      "train Loss: 0.0243 Acc: 99.4712\n",
      "test Loss: 0.1124 Acc: 98.0316\n",
      "\n",
      "Epoch 961/1199\n",
      "------------------------\n",
      "train Loss: 0.0232 Acc: 99.4843\n",
      "test Loss: 0.1102 Acc: 97.9878\n",
      "\n",
      "Epoch 962/1199\n",
      "------------------------\n",
      "train Loss: 0.0240 Acc: 99.4763\n",
      "test Loss: 0.1102 Acc: 98.0724\n",
      "\n",
      "Epoch 963/1199\n",
      "------------------------\n",
      "train Loss: 0.0241 Acc: 99.4759\n",
      "test Loss: 0.1150 Acc: 97.9382\n",
      "\n",
      "Epoch 964/1199\n",
      "------------------------\n",
      "train Loss: 0.0234 Acc: 99.4937\n",
      "test Loss: 0.1124 Acc: 97.9854\n",
      "\n",
      "Epoch 965/1199\n",
      "------------------------\n",
      "train Loss: 0.0243 Acc: 99.4700\n",
      "test Loss: 0.1084 Acc: 98.0491\n",
      "\n",
      "Epoch 966/1199\n",
      "------------------------\n",
      "train Loss: 0.0221 Acc: 99.5177\n",
      "test Loss: 0.1072 Acc: 98.0052\n",
      "\n",
      "Epoch 967/1199\n",
      "------------------------\n",
      "train Loss: 0.0248 Acc: 99.4695\n",
      "test Loss: 0.1089 Acc: 98.0481\n",
      "\n",
      "Epoch 968/1199\n",
      "------------------------\n",
      "train Loss: 0.0246 Acc: 99.4668\n",
      "test Loss: 0.1094 Acc: 98.0636\n",
      "\n",
      "Epoch 969/1199\n",
      "------------------------\n",
      "train Loss: 0.0247 Acc: 99.4769\n",
      "test Loss: 0.1146 Acc: 97.8916\n",
      "\n",
      "Epoch 970/1199\n",
      "------------------------\n",
      "train Loss: 0.0226 Acc: 99.5003\n",
      "test Loss: 0.1095 Acc: 98.0479\n",
      "\n",
      "Epoch 971/1199\n",
      "------------------------\n",
      "train Loss: 0.0252 Acc: 99.4542\n",
      "test Loss: 0.1089 Acc: 98.0372\n",
      "\n",
      "Epoch 972/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.4871\n",
      "test Loss: 0.1085 Acc: 98.0598\n",
      "\n",
      "Epoch 973/1199\n",
      "------------------------\n",
      "train Loss: 0.0240 Acc: 99.4816\n",
      "test Loss: 0.1116 Acc: 98.0430\n",
      "\n",
      "Epoch 974/1199\n",
      "------------------------\n",
      "train Loss: 0.0246 Acc: 99.4506\n",
      "test Loss: 0.1122 Acc: 98.0467\n",
      "\n",
      "Epoch 975/1199\n",
      "------------------------\n",
      "train Loss: 0.0270 Acc: 99.4005\n",
      "test Loss: 0.1108 Acc: 98.0152\n",
      "\n",
      "Epoch 976/1199\n",
      "------------------------\n",
      "train Loss: 0.0237 Acc: 99.4852\n",
      "test Loss: 0.1125 Acc: 97.9980\n",
      "\n",
      "Epoch 977/1199\n",
      "------------------------\n",
      "train Loss: 0.0249 Acc: 99.4552\n",
      "test Loss: 0.1120 Acc: 98.0365\n",
      "\n",
      "Epoch 978/1199\n",
      "------------------------\n",
      "train Loss: 0.0258 Acc: 99.4479\n",
      "test Loss: 0.1094 Acc: 97.9907\n",
      "\n",
      "Epoch 979/1199\n",
      "------------------------\n",
      "train Loss: 0.0247 Acc: 99.4554\n",
      "test Loss: 0.1114 Acc: 97.9356\n",
      "\n",
      "Epoch 980/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.4966\n",
      "test Loss: 0.1109 Acc: 98.0232\n",
      "\n",
      "Epoch 981/1199\n",
      "------------------------\n",
      "train Loss: 0.0232 Acc: 99.4926\n",
      "test Loss: 0.1122 Acc: 98.0285\n",
      "\n",
      "Epoch 982/1199\n",
      "------------------------\n",
      "train Loss: 0.0244 Acc: 99.4708\n",
      "test Loss: 0.1123 Acc: 98.0548\n",
      "\n",
      "Epoch 983/1199\n",
      "------------------------\n",
      "train Loss: 0.0230 Acc: 99.4961\n",
      "test Loss: 0.1101 Acc: 98.0209\n",
      "\n",
      "Epoch 984/1199\n",
      "------------------------\n",
      "train Loss: 0.0244 Acc: 99.4634\n",
      "test Loss: 0.1144 Acc: 97.9890\n",
      "\n",
      "Epoch 985/1199\n",
      "------------------------\n",
      "train Loss: 0.0237 Acc: 99.4792\n",
      "test Loss: 0.1114 Acc: 98.0152\n",
      "\n",
      "Epoch 986/1199\n",
      "------------------------\n",
      "train Loss: 0.0229 Acc: 99.5021\n",
      "test Loss: 0.1074 Acc: 98.0460\n",
      "\n",
      "Epoch 987/1199\n",
      "------------------------\n",
      "train Loss: 0.0237 Acc: 99.4798\n",
      "test Loss: 0.1089 Acc: 98.0653\n",
      "\n",
      "Epoch 988/1199\n",
      "------------------------\n",
      "train Loss: 0.0244 Acc: 99.4729\n",
      "test Loss: 0.1122 Acc: 98.0114\n",
      "\n",
      "Epoch 989/1199\n",
      "------------------------\n",
      "train Loss: 0.0241 Acc: 99.4783\n",
      "test Loss: 0.1114 Acc: 98.0296\n",
      "\n",
      "Epoch 990/1199\n",
      "------------------------\n",
      "train Loss: 0.0238 Acc: 99.4793\n",
      "test Loss: 0.1093 Acc: 98.0646\n",
      "\n",
      "Epoch 991/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.4952\n",
      "test Loss: 0.1078 Acc: 98.0368\n",
      "\n",
      "Epoch 992/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.4899\n",
      "test Loss: 0.1073 Acc: 98.0817\n",
      "\n",
      "Epoch 993/1199\n",
      "------------------------\n",
      "train Loss: 0.0245 Acc: 99.4619\n",
      "test Loss: 0.1091 Acc: 98.0085\n",
      "\n",
      "Epoch 994/1199\n",
      "------------------------\n",
      "train Loss: 0.0243 Acc: 99.4765\n",
      "test Loss: 0.1106 Acc: 98.0411\n",
      "\n",
      "Epoch 995/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.4972\n",
      "test Loss: 0.1164 Acc: 97.9424\n",
      "\n",
      "Epoch 996/1199\n",
      "------------------------\n",
      "train Loss: 0.0222 Acc: 99.5156\n",
      "test Loss: 0.1118 Acc: 98.0123\n",
      "\n",
      "Epoch 997/1199\n",
      "------------------------\n",
      "train Loss: 0.0240 Acc: 99.4674\n",
      "test Loss: 0.1113 Acc: 98.0208\n",
      "\n",
      "Epoch 998/1199\n",
      "------------------------\n",
      "train Loss: 0.0228 Acc: 99.5046\n",
      "test Loss: 0.1094 Acc: 98.0285\n",
      "\n",
      "Epoch 999/1199\n",
      "------------------------\n",
      "train Loss: 0.0225 Acc: 99.5036\n",
      "test Loss: 0.1101 Acc: 98.0394\n",
      "\n",
      "Epoch 1000/1199\n",
      "------------------------\n",
      "train Loss: 0.0235 Acc: 99.4987\n",
      "test Loss: 0.1083 Acc: 98.0367\n",
      "\n",
      "Epoch 1001/1199\n",
      "------------------------\n",
      "train Loss: 0.0248 Acc: 99.4713\n",
      "test Loss: 0.1119 Acc: 98.0156\n",
      "\n",
      "Epoch 1002/1199\n",
      "------------------------\n",
      "train Loss: 0.0238 Acc: 99.4773\n",
      "test Loss: 0.1085 Acc: 98.0679\n",
      "\n",
      "Epoch 1003/1199\n",
      "------------------------\n",
      "train Loss: 0.0234 Acc: 99.4879\n",
      "test Loss: 0.1097 Acc: 98.0641\n",
      "\n",
      "Epoch 1004/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.4988\n",
      "test Loss: 0.1121 Acc: 97.9607\n",
      "\n",
      "Epoch 1005/1199\n",
      "------------------------\n",
      "train Loss: 0.0252 Acc: 99.4565\n",
      "test Loss: 0.1124 Acc: 98.0372\n",
      "\n",
      "Epoch 1006/1199\n",
      "------------------------\n",
      "train Loss: 0.0242 Acc: 99.4870\n",
      "test Loss: 0.1096 Acc: 97.9759\n",
      "\n",
      "Epoch 1007/1199\n",
      "------------------------\n",
      "train Loss: 0.0232 Acc: 99.4985\n",
      "test Loss: 0.1088 Acc: 97.9992\n",
      "\n",
      "Epoch 1008/1199\n",
      "------------------------\n",
      "train Loss: 0.0247 Acc: 99.4697\n",
      "test Loss: 0.1141 Acc: 98.0014\n",
      "\n",
      "Epoch 1009/1199\n",
      "------------------------\n",
      "train Loss: 0.0235 Acc: 99.4918\n",
      "test Loss: 0.1070 Acc: 98.0221\n",
      "\n",
      "Epoch 1010/1199\n",
      "------------------------\n",
      "train Loss: 0.0236 Acc: 99.4753\n",
      "test Loss: 0.1126 Acc: 98.0308\n",
      "\n",
      "Epoch 1011/1199\n",
      "------------------------\n",
      "train Loss: 0.0243 Acc: 99.4714\n",
      "test Loss: 0.1146 Acc: 98.0263\n",
      "\n",
      "Epoch 1012/1199\n",
      "------------------------\n",
      "train Loss: 0.0222 Acc: 99.5263\n",
      "test Loss: 0.1079 Acc: 98.0107\n",
      "\n",
      "Epoch 1013/1199\n",
      "------------------------\n",
      "train Loss: 0.0229 Acc: 99.5059\n",
      "test Loss: 0.1093 Acc: 98.0204\n",
      "\n",
      "Epoch 1014/1199\n",
      "------------------------\n",
      "train Loss: 0.0235 Acc: 99.4984\n",
      "test Loss: 0.1113 Acc: 98.0076\n",
      "\n",
      "Epoch 1015/1199\n",
      "------------------------\n",
      "train Loss: 0.0243 Acc: 99.4772\n",
      "test Loss: 0.1116 Acc: 98.0054\n",
      "\n",
      "Epoch 1016/1199\n",
      "------------------------\n",
      "train Loss: 0.0236 Acc: 99.4946\n",
      "test Loss: 0.1085 Acc: 98.0717\n",
      "\n",
      "Epoch 1017/1199\n",
      "------------------------\n",
      "train Loss: 0.0242 Acc: 99.4744\n",
      "test Loss: 0.1117 Acc: 98.0641\n",
      "\n",
      "Epoch 1018/1199\n",
      "------------------------\n",
      "train Loss: 0.0242 Acc: 99.4779\n",
      "test Loss: 0.1101 Acc: 98.0363\n",
      "\n",
      "Epoch 1019/1199\n",
      "------------------------\n",
      "train Loss: 0.0246 Acc: 99.4634\n",
      "test Loss: 0.1109 Acc: 98.0206\n",
      "\n",
      "Epoch 1020/1199\n",
      "------------------------\n",
      "train Loss: 0.0237 Acc: 99.4731\n",
      "test Loss: 0.1100 Acc: 98.0137\n",
      "\n",
      "Epoch 1021/1199\n",
      "------------------------\n",
      "train Loss: 0.0243 Acc: 99.4727\n",
      "test Loss: 0.1134 Acc: 98.0289\n",
      "\n",
      "Epoch 1022/1199\n",
      "------------------------\n",
      "train Loss: 0.0248 Acc: 99.4592\n",
      "test Loss: 0.1126 Acc: 97.9779\n",
      "\n",
      "Epoch 1023/1199\n",
      "------------------------\n",
      "train Loss: 0.0249 Acc: 99.4634\n",
      "test Loss: 0.1127 Acc: 98.0161\n",
      "\n",
      "Epoch 1024/1199\n",
      "------------------------\n",
      "train Loss: 0.0225 Acc: 99.5017\n",
      "test Loss: 0.1076 Acc: 98.0938\n",
      "\n",
      "Epoch 1025/1199\n",
      "------------------------\n",
      "train Loss: 0.0246 Acc: 99.4670\n",
      "test Loss: 0.1108 Acc: 98.0325\n",
      "\n",
      "Epoch 1026/1199\n",
      "------------------------\n",
      "train Loss: 0.0256 Acc: 99.4470\n",
      "test Loss: 0.1115 Acc: 97.9503\n",
      "\n",
      "Epoch 1027/1199\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0233 Acc: 99.4920\n",
      "test Loss: 0.1102 Acc: 97.9612\n",
      "\n",
      "Epoch 1028/1199\n",
      "------------------------\n",
      "train Loss: 0.0227 Acc: 99.4996\n",
      "test Loss: 0.1126 Acc: 98.0173\n",
      "\n",
      "Epoch 1029/1199\n",
      "------------------------\n",
      "train Loss: 0.0244 Acc: 99.4666\n",
      "test Loss: 0.1109 Acc: 97.9522\n",
      "\n",
      "Epoch 1030/1199\n",
      "------------------------\n",
      "train Loss: 0.0272 Acc: 99.4066\n",
      "test Loss: 0.1075 Acc: 98.0101\n",
      "\n",
      "Epoch 1031/1199\n",
      "------------------------\n",
      "train Loss: 0.0264 Acc: 99.4320\n",
      "test Loss: 0.1114 Acc: 97.9935\n",
      "\n",
      "Epoch 1032/1199\n",
      "------------------------\n",
      "train Loss: 0.0243 Acc: 99.4662\n",
      "test Loss: 0.1075 Acc: 98.0418\n",
      "\n",
      "Epoch 1033/1199\n",
      "------------------------\n",
      "train Loss: 0.0219 Acc: 99.5257\n",
      "test Loss: 0.1081 Acc: 98.0845\n",
      "\n",
      "Epoch 1034/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.4911\n",
      "test Loss: 0.1038 Acc: 98.0710\n",
      "\n",
      "Epoch 1035/1199\n",
      "------------------------\n",
      "train Loss: 0.0234 Acc: 99.4908\n",
      "test Loss: 0.1119 Acc: 97.9928\n",
      "\n",
      "Epoch 1036/1199\n",
      "------------------------\n",
      "train Loss: 0.0231 Acc: 99.4907\n",
      "test Loss: 0.1087 Acc: 98.0612\n",
      "\n",
      "Epoch 1037/1199\n",
      "------------------------\n",
      "train Loss: 0.0259 Acc: 99.4467\n",
      "test Loss: 0.1112 Acc: 98.0335\n",
      "\n",
      "Epoch 1038/1199\n",
      "------------------------\n",
      "train Loss: 0.0246 Acc: 99.4625\n",
      "test Loss: 0.1078 Acc: 98.0430\n",
      "\n",
      "Epoch 1039/1199\n",
      "------------------------\n",
      "train Loss: 0.0236 Acc: 99.4952\n",
      "test Loss: 0.1068 Acc: 98.0525\n",
      "\n",
      "Epoch 1040/1199\n",
      "------------------------\n",
      "train Loss: 0.0241 Acc: 99.4767\n",
      "test Loss: 0.1096 Acc: 98.0650\n",
      "\n",
      "Epoch 1041/1199\n",
      "------------------------\n",
      "train Loss: 0.0253 Acc: 99.4463\n",
      "test Loss: 0.1135 Acc: 98.0253\n",
      "\n",
      "Epoch 1042/1199\n",
      "------------------------\n",
      "train Loss: 0.0236 Acc: 99.4991\n",
      "test Loss: 0.1087 Acc: 98.0399\n",
      "\n",
      "Epoch 1043/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.5052\n",
      "test Loss: 0.1116 Acc: 98.0353\n",
      "\n",
      "Epoch 1044/1199\n",
      "------------------------\n",
      "train Loss: 0.0230 Acc: 99.4990\n",
      "test Loss: 0.1123 Acc: 98.0498\n",
      "\n",
      "Epoch 1045/1199\n",
      "------------------------\n",
      "train Loss: 0.0227 Acc: 99.5018\n",
      "test Loss: 0.1120 Acc: 98.0154\n",
      "\n",
      "Epoch 1046/1199\n",
      "------------------------\n",
      "train Loss: 0.0232 Acc: 99.4897\n",
      "test Loss: 0.1085 Acc: 98.0809\n",
      "\n",
      "Epoch 1047/1199\n",
      "------------------------\n",
      "train Loss: 0.0230 Acc: 99.5064\n",
      "test Loss: 0.1113 Acc: 97.9662\n",
      "\n",
      "Epoch 1048/1199\n",
      "------------------------\n",
      "train Loss: 0.0261 Acc: 99.4401\n",
      "test Loss: 0.1115 Acc: 98.0253\n",
      "\n",
      "Epoch 1049/1199\n",
      "------------------------\n",
      "train Loss: 0.0231 Acc: 99.4996\n",
      "test Loss: 0.1101 Acc: 97.9935\n",
      "\n",
      "Epoch 1050/1199\n",
      "------------------------\n",
      "train Loss: 0.0232 Acc: 99.4899\n",
      "test Loss: 0.1095 Acc: 98.0593\n",
      "\n",
      "Epoch 1051/1199\n",
      "------------------------\n",
      "train Loss: 0.0222 Acc: 99.5194\n",
      "test Loss: 0.1079 Acc: 98.0802\n",
      "\n",
      "Epoch 1052/1199\n",
      "------------------------\n",
      "train Loss: 0.0241 Acc: 99.4775\n",
      "test Loss: 0.1108 Acc: 98.0422\n",
      "\n",
      "Epoch 1053/1199\n",
      "------------------------\n",
      "train Loss: 0.0251 Acc: 99.4633\n",
      "test Loss: 0.1081 Acc: 97.9817\n",
      "\n",
      "Epoch 1054/1199\n",
      "------------------------\n",
      "train Loss: 0.0231 Acc: 99.4939\n",
      "test Loss: 0.1103 Acc: 98.0318\n",
      "\n",
      "Epoch 1055/1199\n",
      "------------------------\n",
      "train Loss: 0.0241 Acc: 99.4770\n",
      "test Loss: 0.1113 Acc: 98.0629\n",
      "\n",
      "Epoch 1056/1199\n",
      "------------------------\n",
      "train Loss: 0.0231 Acc: 99.4998\n",
      "test Loss: 0.1086 Acc: 98.0289\n",
      "\n",
      "Epoch 1057/1199\n",
      "------------------------\n",
      "train Loss: 0.0250 Acc: 99.4693\n",
      "test Loss: 0.1138 Acc: 98.0083\n",
      "\n",
      "Epoch 1058/1199\n",
      "------------------------\n",
      "train Loss: 0.0223 Acc: 99.5178\n",
      "test Loss: 0.1109 Acc: 97.9990\n",
      "\n",
      "Epoch 1059/1199\n",
      "------------------------\n",
      "train Loss: 0.0239 Acc: 99.4909\n",
      "test Loss: 0.1103 Acc: 98.0550\n",
      "\n",
      "Epoch 1060/1199\n",
      "------------------------\n",
      "train Loss: 0.0227 Acc: 99.5064\n",
      "test Loss: 0.1117 Acc: 98.0565\n",
      "\n",
      "Epoch 1061/1199\n",
      "------------------------\n",
      "train Loss: 0.0244 Acc: 99.4712\n",
      "test Loss: 0.1113 Acc: 97.9902\n",
      "\n",
      "Epoch 1062/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.4848\n",
      "test Loss: 0.1079 Acc: 98.1038\n",
      "\n",
      "Epoch 1063/1199\n",
      "------------------------\n",
      "train Loss: 0.0226 Acc: 99.5009\n",
      "test Loss: 0.1116 Acc: 97.9671\n",
      "\n",
      "Epoch 1064/1199\n",
      "------------------------\n",
      "train Loss: 0.0230 Acc: 99.5034\n",
      "test Loss: 0.1126 Acc: 98.0491\n",
      "\n",
      "Epoch 1065/1199\n",
      "------------------------\n",
      "train Loss: 0.0227 Acc: 99.4998\n",
      "test Loss: 0.1080 Acc: 98.0867\n",
      "\n",
      "Epoch 1066/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.4931\n",
      "test Loss: 0.1066 Acc: 98.0823\n",
      "\n",
      "Epoch 1067/1199\n",
      "------------------------\n",
      "train Loss: 0.0223 Acc: 99.5119\n",
      "test Loss: 0.1105 Acc: 98.0804\n",
      "\n",
      "Epoch 1068/1199\n",
      "------------------------\n",
      "train Loss: 0.0222 Acc: 99.5129\n",
      "test Loss: 0.1082 Acc: 98.0759\n",
      "\n",
      "Epoch 1069/1199\n",
      "------------------------\n",
      "train Loss: 0.0218 Acc: 99.5266\n",
      "test Loss: 0.1112 Acc: 98.0251\n",
      "\n",
      "Epoch 1070/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.5063\n",
      "test Loss: 0.1107 Acc: 98.0949\n",
      "\n",
      "Epoch 1071/1199\n",
      "------------------------\n",
      "train Loss: 0.0236 Acc: 99.4826\n",
      "test Loss: 0.1125 Acc: 97.9557\n",
      "\n",
      "Epoch 1072/1199\n",
      "------------------------\n",
      "train Loss: 0.0241 Acc: 99.4797\n",
      "test Loss: 0.1102 Acc: 97.9890\n",
      "\n",
      "Epoch 1073/1199\n",
      "------------------------\n",
      "train Loss: 0.0219 Acc: 99.5206\n",
      "test Loss: 0.1132 Acc: 97.9771\n",
      "\n",
      "Epoch 1074/1199\n",
      "------------------------\n",
      "train Loss: 0.0221 Acc: 99.5231\n",
      "test Loss: 0.1103 Acc: 97.9952\n",
      "\n",
      "Epoch 1075/1199\n",
      "------------------------\n",
      "train Loss: 0.0228 Acc: 99.4965\n",
      "test Loss: 0.1112 Acc: 98.0626\n",
      "\n",
      "Epoch 1076/1199\n",
      "------------------------\n",
      "train Loss: 0.0229 Acc: 99.5080\n",
      "test Loss: 0.1131 Acc: 97.9950\n",
      "\n",
      "Epoch 1077/1199\n",
      "------------------------\n",
      "train Loss: 0.0243 Acc: 99.4716\n",
      "test Loss: 0.1116 Acc: 98.0113\n",
      "\n",
      "Epoch 1078/1199\n",
      "------------------------\n",
      "train Loss: 0.0224 Acc: 99.5015\n",
      "test Loss: 0.1120 Acc: 97.9928\n",
      "\n",
      "Epoch 1079/1199\n",
      "------------------------\n",
      "train Loss: 0.0232 Acc: 99.4972\n",
      "test Loss: 0.1146 Acc: 98.0145\n",
      "\n",
      "Epoch 1080/1199\n",
      "------------------------\n",
      "train Loss: 0.0229 Acc: 99.5028\n",
      "test Loss: 0.1106 Acc: 98.0273\n",
      "\n",
      "Epoch 1081/1199\n",
      "------------------------\n",
      "train Loss: 0.0243 Acc: 99.4640\n",
      "test Loss: 0.1104 Acc: 98.0496\n",
      "\n",
      "Epoch 1082/1199\n",
      "------------------------\n",
      "train Loss: 0.0237 Acc: 99.4908\n",
      "test Loss: 0.1112 Acc: 98.0563\n",
      "\n",
      "Epoch 1083/1199\n",
      "------------------------\n",
      "train Loss: 0.0243 Acc: 99.4839\n",
      "test Loss: 0.1099 Acc: 97.9888\n",
      "\n",
      "Epoch 1084/1199\n",
      "------------------------\n",
      "train Loss: 0.0238 Acc: 99.4859\n",
      "test Loss: 0.1126 Acc: 98.0738\n",
      "\n",
      "Epoch 1085/1199\n",
      "------------------------\n",
      "train Loss: 0.0226 Acc: 99.5139\n",
      "test Loss: 0.1109 Acc: 98.0197\n",
      "\n",
      "Epoch 1086/1199\n",
      "------------------------\n",
      "train Loss: 0.0226 Acc: 99.5101\n",
      "test Loss: 0.1120 Acc: 98.0330\n",
      "\n",
      "Epoch 1087/1199\n",
      "------------------------\n",
      "train Loss: 0.0232 Acc: 99.4856\n",
      "test Loss: 0.1144 Acc: 98.0955\n",
      "\n",
      "Epoch 1088/1199\n",
      "------------------------\n",
      "train Loss: 0.0226 Acc: 99.5052\n",
      "test Loss: 0.1145 Acc: 97.9923\n",
      "\n",
      "Epoch 1089/1199\n",
      "------------------------\n",
      "train Loss: 0.0219 Acc: 99.5175\n",
      "test Loss: 0.1097 Acc: 98.0990\n",
      "\n",
      "Epoch 1090/1199\n",
      "------------------------\n",
      "train Loss: 0.0222 Acc: 99.5294\n",
      "test Loss: 0.1114 Acc: 98.0292\n",
      "\n",
      "Epoch 1091/1199\n",
      "------------------------\n",
      "train Loss: 0.0235 Acc: 99.4844\n",
      "test Loss: 0.1095 Acc: 98.0531\n",
      "\n",
      "Epoch 1092/1199\n",
      "------------------------\n",
      "train Loss: 0.0213 Acc: 99.5308\n",
      "test Loss: 0.1109 Acc: 98.0289\n",
      "\n",
      "Epoch 1093/1199\n",
      "------------------------\n",
      "train Loss: 0.0220 Acc: 99.5290\n",
      "test Loss: 0.1130 Acc: 98.0216\n",
      "\n",
      "Epoch 1094/1199\n",
      "------------------------\n",
      "train Loss: 0.0241 Acc: 99.4836\n",
      "test Loss: 0.1119 Acc: 98.0140\n",
      "\n",
      "Epoch 1095/1199\n",
      "------------------------\n",
      "train Loss: 0.0221 Acc: 99.5222\n",
      "test Loss: 0.1101 Acc: 98.0168\n",
      "\n",
      "Epoch 1096/1199\n",
      "------------------------\n",
      "train Loss: 0.0231 Acc: 99.4999\n",
      "test Loss: 0.1105 Acc: 98.0120\n",
      "\n",
      "Epoch 1097/1199\n",
      "------------------------\n",
      "train Loss: 0.0228 Acc: 99.5009\n",
      "test Loss: 0.1121 Acc: 97.9990\n",
      "\n",
      "Epoch 1098/1199\n",
      "------------------------\n",
      "train Loss: 0.0230 Acc: 99.4888\n",
      "test Loss: 0.1113 Acc: 98.0291\n",
      "\n",
      "Epoch 1099/1199\n",
      "------------------------\n",
      "train Loss: 0.0230 Acc: 99.5020\n",
      "test Loss: 0.1121 Acc: 98.0531\n",
      "\n",
      "Epoch 1100/1199\n",
      "------------------------\n",
      "train Loss: 0.0236 Acc: 99.4903\n",
      "test Loss: 0.1130 Acc: 98.0057\n",
      "\n",
      "Epoch 1101/1199\n",
      "------------------------\n",
      "train Loss: 0.0231 Acc: 99.4941\n",
      "test Loss: 0.1123 Acc: 98.0617\n",
      "\n",
      "Epoch 1102/1199\n",
      "------------------------\n",
      "train Loss: 0.0228 Acc: 99.5181\n",
      "test Loss: 0.1123 Acc: 97.9859\n",
      "\n",
      "Epoch 1103/1199\n",
      "------------------------\n",
      "train Loss: 0.0227 Acc: 99.5041\n",
      "test Loss: 0.1122 Acc: 98.0645\n",
      "\n",
      "Epoch 1104/1199\n",
      "------------------------\n",
      "train Loss: 0.0243 Acc: 99.4909\n",
      "test Loss: 0.1118 Acc: 98.0268\n",
      "\n",
      "Epoch 1105/1199\n",
      "------------------------\n",
      "train Loss: 0.0248 Acc: 99.4696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1137 Acc: 98.0090\n",
      "\n",
      "Epoch 1106/1199\n",
      "------------------------\n",
      "train Loss: 0.0224 Acc: 99.5257\n",
      "test Loss: 0.1122 Acc: 98.0584\n",
      "\n",
      "Epoch 1107/1199\n",
      "------------------------\n",
      "train Loss: 0.0242 Acc: 99.4767\n",
      "test Loss: 0.1144 Acc: 98.0272\n",
      "\n",
      "Epoch 1108/1199\n",
      "------------------------\n",
      "train Loss: 0.0230 Acc: 99.4960\n",
      "test Loss: 0.1132 Acc: 98.0313\n",
      "\n",
      "Epoch 1109/1199\n",
      "------------------------\n",
      "train Loss: 0.0243 Acc: 99.4742\n",
      "test Loss: 0.1111 Acc: 98.0779\n",
      "\n",
      "Epoch 1110/1199\n",
      "------------------------\n",
      "train Loss: 0.0228 Acc: 99.5124\n",
      "test Loss: 0.1143 Acc: 98.0850\n",
      "\n",
      "Epoch 1111/1199\n",
      "------------------------\n",
      "train Loss: 0.0235 Acc: 99.4921\n",
      "test Loss: 0.1140 Acc: 98.0384\n",
      "\n",
      "Epoch 1112/1199\n",
      "------------------------\n",
      "train Loss: 0.0236 Acc: 99.4937\n",
      "test Loss: 0.1114 Acc: 98.0503\n",
      "\n",
      "Epoch 1113/1199\n",
      "------------------------\n",
      "train Loss: 0.0246 Acc: 99.4791\n",
      "test Loss: 0.1108 Acc: 98.0266\n",
      "\n",
      "Epoch 1114/1199\n",
      "------------------------\n",
      "train Loss: 0.0237 Acc: 99.4975\n",
      "test Loss: 0.1120 Acc: 98.0408\n",
      "\n",
      "Epoch 1115/1199\n",
      "------------------------\n",
      "train Loss: 0.0237 Acc: 99.4903\n",
      "test Loss: 0.1119 Acc: 98.0099\n",
      "\n",
      "Epoch 1116/1199\n",
      "------------------------\n",
      "train Loss: 0.0253 Acc: 99.4630\n",
      "test Loss: 0.1120 Acc: 98.0353\n",
      "\n",
      "Epoch 1117/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.4991\n",
      "test Loss: 0.1133 Acc: 98.0413\n",
      "\n",
      "Epoch 1118/1199\n",
      "------------------------\n",
      "train Loss: 0.0227 Acc: 99.5195\n",
      "test Loss: 0.1101 Acc: 98.0363\n",
      "\n",
      "Epoch 1119/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.4935\n",
      "test Loss: 0.1133 Acc: 98.0363\n",
      "\n",
      "Epoch 1120/1199\n",
      "------------------------\n",
      "train Loss: 0.0239 Acc: 99.4869\n",
      "test Loss: 0.1105 Acc: 98.0156\n",
      "\n",
      "Epoch 1121/1199\n",
      "------------------------\n",
      "train Loss: 0.0237 Acc: 99.4837\n",
      "test Loss: 0.1102 Acc: 98.0329\n",
      "\n",
      "Epoch 1122/1199\n",
      "------------------------\n",
      "train Loss: 0.0244 Acc: 99.4831\n",
      "test Loss: 0.1120 Acc: 98.0392\n",
      "\n",
      "Epoch 1123/1199\n",
      "------------------------\n",
      "train Loss: 0.0245 Acc: 99.4708\n",
      "test Loss: 0.1115 Acc: 98.0209\n",
      "\n",
      "Epoch 1124/1199\n",
      "------------------------\n",
      "train Loss: 0.0231 Acc: 99.5007\n",
      "test Loss: 0.1095 Acc: 98.0213\n",
      "\n",
      "Epoch 1125/1199\n",
      "------------------------\n",
      "train Loss: 0.0231 Acc: 99.5061\n",
      "test Loss: 0.1111 Acc: 98.0318\n",
      "\n",
      "Epoch 1126/1199\n",
      "------------------------\n",
      "train Loss: 0.0232 Acc: 99.4995\n",
      "test Loss: 0.1109 Acc: 98.0522\n",
      "\n",
      "Epoch 1127/1199\n",
      "------------------------\n",
      "train Loss: 0.0238 Acc: 99.4865\n",
      "test Loss: 0.1124 Acc: 98.0876\n",
      "\n",
      "Epoch 1128/1199\n",
      "------------------------\n",
      "train Loss: 0.0241 Acc: 99.4890\n",
      "test Loss: 0.1105 Acc: 98.0256\n",
      "\n",
      "Epoch 1129/1199\n",
      "------------------------\n",
      "train Loss: 0.0221 Acc: 99.5227\n",
      "test Loss: 0.1125 Acc: 97.9893\n",
      "\n",
      "Epoch 1130/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.4937\n",
      "test Loss: 0.1110 Acc: 98.0422\n",
      "\n",
      "Epoch 1131/1199\n",
      "------------------------\n",
      "train Loss: 0.0228 Acc: 99.5144\n",
      "test Loss: 0.1140 Acc: 98.1040\n",
      "\n",
      "Epoch 1132/1199\n",
      "------------------------\n",
      "train Loss: 0.0228 Acc: 99.5059\n",
      "test Loss: 0.1113 Acc: 98.0798\n",
      "\n",
      "Epoch 1133/1199\n",
      "------------------------\n",
      "train Loss: 0.0249 Acc: 99.4519\n",
      "test Loss: 0.1122 Acc: 97.9981\n",
      "\n",
      "Epoch 1134/1199\n",
      "------------------------\n",
      "train Loss: 0.0237 Acc: 99.4906\n",
      "test Loss: 0.1121 Acc: 98.0370\n",
      "\n",
      "Epoch 1135/1199\n",
      "------------------------\n",
      "train Loss: 0.0261 Acc: 99.4344\n",
      "test Loss: 0.1097 Acc: 97.9859\n",
      "\n",
      "Epoch 1136/1199\n",
      "------------------------\n",
      "train Loss: 0.0243 Acc: 99.4704\n",
      "test Loss: 0.1099 Acc: 98.0517\n",
      "\n",
      "Epoch 1137/1199\n",
      "------------------------\n",
      "train Loss: 0.0240 Acc: 99.4844\n",
      "test Loss: 0.1152 Acc: 97.9251\n",
      "\n",
      "Epoch 1138/1199\n",
      "------------------------\n",
      "train Loss: 0.0274 Acc: 99.4017\n",
      "test Loss: 0.1129 Acc: 97.9871\n",
      "\n",
      "Epoch 1139/1199\n",
      "------------------------\n",
      "train Loss: 0.0258 Acc: 99.4472\n",
      "test Loss: 0.1126 Acc: 97.9700\n",
      "\n",
      "Epoch 1140/1199\n",
      "------------------------\n",
      "train Loss: 0.0227 Acc: 99.5023\n",
      "test Loss: 0.1094 Acc: 98.0430\n",
      "\n",
      "Epoch 1141/1199\n",
      "------------------------\n",
      "train Loss: 0.0226 Acc: 99.5004\n",
      "test Loss: 0.1134 Acc: 98.0114\n",
      "\n",
      "Epoch 1142/1199\n",
      "------------------------\n",
      "train Loss: 0.0225 Acc: 99.5180\n",
      "test Loss: 0.1130 Acc: 98.0078\n",
      "\n",
      "Epoch 1143/1199\n",
      "------------------------\n",
      "train Loss: 0.0221 Acc: 99.5208\n",
      "test Loss: 0.1110 Acc: 98.0075\n",
      "\n",
      "Epoch 1144/1199\n",
      "------------------------\n",
      "train Loss: 0.0214 Acc: 99.5377\n",
      "test Loss: 0.1123 Acc: 98.0106\n",
      "\n",
      "Epoch 1145/1199\n",
      "------------------------\n",
      "train Loss: 0.0216 Acc: 99.5248\n",
      "test Loss: 0.1113 Acc: 98.0173\n",
      "\n",
      "Epoch 1146/1199\n",
      "------------------------\n",
      "train Loss: 0.0216 Acc: 99.5284\n",
      "test Loss: 0.1090 Acc: 98.0308\n",
      "\n",
      "Epoch 1147/1199\n",
      "------------------------\n",
      "train Loss: 0.0244 Acc: 99.4795\n",
      "test Loss: 0.1092 Acc: 98.0444\n",
      "\n",
      "Epoch 1148/1199\n",
      "------------------------\n",
      "train Loss: 0.0237 Acc: 99.4881\n",
      "test Loss: 0.1121 Acc: 97.9990\n",
      "\n",
      "Epoch 1149/1199\n",
      "------------------------\n",
      "train Loss: 0.0238 Acc: 99.4891\n",
      "test Loss: 0.1152 Acc: 97.9994\n",
      "\n",
      "Epoch 1150/1199\n",
      "------------------------\n",
      "train Loss: 0.0235 Acc: 99.4943\n",
      "test Loss: 0.1102 Acc: 98.0189\n",
      "\n",
      "Epoch 1151/1199\n",
      "------------------------\n",
      "train Loss: 0.0246 Acc: 99.4716\n",
      "test Loss: 0.1111 Acc: 98.0209\n",
      "\n",
      "Epoch 1152/1199\n",
      "------------------------\n",
      "train Loss: 0.0244 Acc: 99.4769\n",
      "test Loss: 0.1144 Acc: 97.9335\n",
      "\n",
      "Epoch 1153/1199\n",
      "------------------------\n",
      "train Loss: 0.0249 Acc: 99.4579\n",
      "test Loss: 0.1116 Acc: 98.0389\n",
      "\n",
      "Epoch 1154/1199\n",
      "------------------------\n",
      "train Loss: 0.0237 Acc: 99.5010\n",
      "test Loss: 0.1147 Acc: 98.0315\n",
      "\n",
      "Epoch 1155/1199\n",
      "------------------------\n",
      "train Loss: 0.0216 Acc: 99.5289\n",
      "test Loss: 0.1115 Acc: 98.0351\n",
      "\n",
      "Epoch 1156/1199\n",
      "------------------------\n",
      "train Loss: 0.0223 Acc: 99.5159\n",
      "test Loss: 0.1114 Acc: 98.0802\n",
      "\n",
      "Epoch 1157/1199\n",
      "------------------------\n",
      "train Loss: 0.0228 Acc: 99.4999\n",
      "test Loss: 0.1146 Acc: 98.0420\n",
      "\n",
      "Epoch 1158/1199\n",
      "------------------------\n",
      "train Loss: 0.0231 Acc: 99.5038\n",
      "test Loss: 0.1106 Acc: 98.0607\n",
      "\n",
      "Epoch 1159/1199\n",
      "------------------------\n",
      "train Loss: 0.0228 Acc: 99.5055\n",
      "test Loss: 0.1142 Acc: 98.0403\n",
      "\n",
      "Epoch 1160/1199\n",
      "------------------------\n",
      "train Loss: 0.0241 Acc: 99.4713\n",
      "test Loss: 0.1106 Acc: 98.0166\n",
      "\n",
      "Epoch 1161/1199\n",
      "------------------------\n",
      "train Loss: 0.0236 Acc: 99.4833\n",
      "test Loss: 0.1090 Acc: 98.0797\n",
      "\n",
      "Epoch 1162/1199\n",
      "------------------------\n",
      "train Loss: 0.0222 Acc: 99.5186\n",
      "test Loss: 0.1122 Acc: 97.9586\n",
      "\n",
      "Epoch 1163/1199\n",
      "------------------------\n",
      "train Loss: 0.0233 Acc: 99.4944\n",
      "test Loss: 0.1112 Acc: 98.0859\n",
      "\n",
      "Epoch 1164/1199\n",
      "------------------------\n",
      "train Loss: 0.0237 Acc: 99.4838\n",
      "test Loss: 0.1103 Acc: 98.0284\n",
      "\n",
      "Epoch 1165/1199\n",
      "------------------------\n",
      "train Loss: 0.0222 Acc: 99.5240\n",
      "test Loss: 0.1144 Acc: 98.0090\n",
      "\n",
      "Epoch 1166/1199\n",
      "------------------------\n",
      "train Loss: 0.0222 Acc: 99.5238\n",
      "test Loss: 0.1103 Acc: 98.0049\n",
      "\n",
      "Epoch 1167/1199\n",
      "------------------------\n",
      "train Loss: 0.0222 Acc: 99.5185\n",
      "test Loss: 0.1085 Acc: 98.0370\n",
      "\n",
      "Epoch 1168/1199\n",
      "------------------------\n",
      "train Loss: 0.0250 Acc: 99.4675\n",
      "test Loss: 0.1130 Acc: 97.9766\n",
      "\n",
      "Epoch 1169/1199\n",
      "------------------------\n",
      "train Loss: 0.0240 Acc: 99.4826\n",
      "test Loss: 0.1134 Acc: 98.0275\n",
      "\n",
      "Epoch 1170/1199\n",
      "------------------------\n",
      "train Loss: 0.0231 Acc: 99.4989\n",
      "test Loss: 0.1126 Acc: 98.0339\n",
      "\n",
      "Epoch 1171/1199\n",
      "------------------------\n",
      "train Loss: 0.0227 Acc: 99.5138\n",
      "test Loss: 0.1110 Acc: 98.0360\n",
      "\n",
      "Epoch 1172/1199\n",
      "------------------------\n",
      "train Loss: 0.0223 Acc: 99.5254\n",
      "test Loss: 0.1146 Acc: 97.9543\n",
      "\n",
      "Epoch 1173/1199\n",
      "------------------------\n",
      "train Loss: 0.0217 Acc: 99.5287\n",
      "test Loss: 0.1100 Acc: 97.9774\n",
      "\n",
      "Epoch 1174/1199\n",
      "------------------------\n",
      "train Loss: 0.0229 Acc: 99.5104\n",
      "test Loss: 0.1120 Acc: 97.9902\n",
      "\n",
      "Epoch 1175/1199\n",
      "------------------------\n",
      "train Loss: 0.0222 Acc: 99.5304\n",
      "test Loss: 0.1133 Acc: 98.0272\n",
      "\n",
      "Epoch 1176/1199\n",
      "------------------------\n",
      "train Loss: 0.0223 Acc: 99.5178\n",
      "test Loss: 0.1102 Acc: 97.9862\n",
      "\n",
      "Epoch 1177/1199\n",
      "------------------------\n",
      "train Loss: 0.0224 Acc: 99.5094\n",
      "test Loss: 0.1121 Acc: 98.0215\n",
      "\n",
      "Epoch 1178/1199\n",
      "------------------------\n",
      "train Loss: 0.0219 Acc: 99.5262\n",
      "test Loss: 0.1112 Acc: 98.0137\n",
      "\n",
      "Epoch 1179/1199\n",
      "------------------------\n",
      "train Loss: 0.0250 Acc: 99.4575\n",
      "test Loss: 0.1105 Acc: 98.0384\n",
      "\n",
      "Epoch 1180/1199\n",
      "------------------------\n",
      "train Loss: 0.0231 Acc: 99.4953\n",
      "test Loss: 0.1118 Acc: 98.0118\n",
      "\n",
      "Epoch 1181/1199\n",
      "------------------------\n",
      "train Loss: 0.0217 Acc: 99.5298\n",
      "test Loss: 0.1095 Acc: 98.0247\n",
      "\n",
      "Epoch 1182/1199\n",
      "------------------------\n",
      "train Loss: 0.0224 Acc: 99.5080\n",
      "test Loss: 0.1121 Acc: 98.0401\n",
      "\n",
      "Epoch 1183/1199\n",
      "------------------------\n",
      "train Loss: 0.0230 Acc: 99.5021\n",
      "test Loss: 0.1146 Acc: 98.0104\n",
      "\n",
      "Epoch 1184/1199\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0220 Acc: 99.5191\n",
      "test Loss: 0.1129 Acc: 98.0534\n",
      "\n",
      "Epoch 1185/1199\n",
      "------------------------\n",
      "train Loss: 0.0218 Acc: 99.5276\n",
      "test Loss: 0.1071 Acc: 98.0503\n",
      "\n",
      "Epoch 1186/1199\n",
      "------------------------\n",
      "train Loss: 0.0215 Acc: 99.5376\n",
      "test Loss: 0.1116 Acc: 98.0114\n",
      "\n",
      "Epoch 1187/1199\n",
      "------------------------\n",
      "train Loss: 0.0238 Acc: 99.4881\n",
      "test Loss: 0.1138 Acc: 98.0961\n",
      "\n",
      "Epoch 1188/1199\n",
      "------------------------\n",
      "train Loss: 0.0229 Acc: 99.5054\n",
      "test Loss: 0.1130 Acc: 97.9842\n",
      "\n",
      "Epoch 1189/1199\n",
      "------------------------\n",
      "train Loss: 0.0223 Acc: 99.5099\n",
      "test Loss: 0.1120 Acc: 97.9627\n",
      "\n",
      "Epoch 1190/1199\n",
      "------------------------\n",
      "train Loss: 0.0231 Acc: 99.4858\n",
      "test Loss: 0.1126 Acc: 97.9874\n",
      "\n",
      "Epoch 1191/1199\n",
      "------------------------\n",
      "train Loss: 0.0222 Acc: 99.5212\n",
      "test Loss: 0.1126 Acc: 97.9940\n",
      "\n",
      "Epoch 1192/1199\n",
      "------------------------\n",
      "train Loss: 0.0237 Acc: 99.4842\n",
      "test Loss: 0.1139 Acc: 98.0786\n",
      "\n",
      "Epoch 1193/1199\n",
      "------------------------\n",
      "train Loss: 0.0227 Acc: 99.5123\n",
      "test Loss: 0.1116 Acc: 97.9671\n",
      "\n",
      "Epoch 1194/1199\n",
      "------------------------\n",
      "train Loss: 0.0223 Acc: 99.5130\n",
      "test Loss: 0.1113 Acc: 97.9798\n",
      "\n",
      "Epoch 1195/1199\n",
      "------------------------\n",
      "train Loss: 0.0218 Acc: 99.5423\n",
      "test Loss: 0.1116 Acc: 98.0323\n",
      "\n",
      "Epoch 1196/1199\n",
      "------------------------\n",
      "train Loss: 0.0245 Acc: 99.4637\n",
      "test Loss: 0.1136 Acc: 98.0410\n",
      "\n",
      "Epoch 1197/1199\n",
      "------------------------\n",
      "train Loss: 0.0220 Acc: 99.5140\n",
      "test Loss: 0.1114 Acc: 97.9809\n",
      "\n",
      "Epoch 1198/1199\n",
      "------------------------\n",
      "train Loss: 0.0214 Acc: 99.5347\n",
      "test Loss: 0.1147 Acc: 97.9612\n",
      "\n",
      "Epoch 1199/1199\n",
      "------------------------\n",
      "train Loss: 0.0229 Acc: 99.4985\n",
      "test Loss: 0.1122 Acc: 98.0068\n",
      "\n",
      "Training complete in 170m 38s\n",
      "Best val Acc: 98.104012\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f046e676cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type Model1. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "final_model = mac_train_model(model=model_adam, criterion=criterion, optimizer=optimizer_adam, \n",
    "                          scheduler=exp_lr_scheduler, num_epochs=1200)\n",
    "torch.save(final_model, './data/model_adam_gru.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tk = english_tokenizer\n",
    "y_tk = french_tokenizer\n",
    "x_id_to_word = {value: key for key, value in x_tk.word_index.items()}\n",
    "y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "x_id_to_word[0] = '<PAD>'\n",
    "y_id_to_word[0] = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "your favorite fruit is the grapefruit but their favorite is the mango <PAD> <PAD> <PAD>\n",
      "votre fruit préféré est le pamplemousse mais leur favori est la mangue <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "correct French sentence\n",
      "votre fruit préféré est le pamplemousse mais leur favori est la mangue <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "index = 11\n",
    "pred_input = input[index].view(1,-1) \n",
    "#print(pred_input.shape)\n",
    "pred_input = pred_input.numpy()\n",
    "pred_input.reshape(-1)\n",
    "\n",
    "#model_adam = torch.load('./data/model_adam.pt')\n",
    "#model_adam = model_adam.to(device)\n",
    "output = model_adam(input[index].view(1,-1).to(device))\n",
    "\n",
    "pred_output = output.data.view(french_vocab_size+1, max_french_sequence_length).cpu().numpy()\n",
    "#print(pred_output.shape)\n",
    "pred_output[0]\n",
    "\n",
    "target_output = label[index].numpy()\n",
    "\n",
    "x_tk = english_tokenizer\n",
    "y_tk = french_tokenizer\n",
    "x_id_to_word = {value: key for key, value in x_tk.word_index.items()}\n",
    "y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "x_id_to_word[0] = '<PAD>'\n",
    "y_id_to_word[0] = '<PAD>'\n",
    "x_id_to_word[pred_input.reshape(-1)[0]]\n",
    "print(' '.join([x_id_to_word[x] for x in pred_input.reshape(-1)]))\n",
    "print(' '.join([y_id_to_word[x] for x in np.argmax(pred_output, axis=0)]))\n",
    "print('correct French sentence')\n",
    "print(' '.join([y_id_to_word[x] for x in target_output]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate the html\n",
    "\n",
    "**Save your notebook before running the next cell to generate the HTML output.** Then submit your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save before you run this cell!\n",
    "!!jupyter nbconvert *.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Enhancements\n",
    "\n",
    "This project focuses on learning various network architectures for machine translation, but we don't evaluate the models according to best practices by splitting the data into separate test & training sets -- so the model accuracy is overstated. Use the [`sklearn.model_selection.train_test_split()`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to create separate training & test datasets, then retrain each of the models using only the training set and evaluate the prediction accuracy using the hold out test set. Does the \"best\" model change?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
