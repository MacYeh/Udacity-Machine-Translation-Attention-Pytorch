{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Translation Project (PyTorch Framework)\n",
    "\n",
    "## Introduction\n",
    "In this notebook, the machine translation end2end pipeline is implemented; two DL models are implemented with **PyTorch** Frameworks; the goal is to translate from English to French\n",
    "\n",
    "- **Preprocess Pipeline** - Convert text to sequence of integers.\n",
    "- **Model 1** Bi-Directional RNN with (GRU/LSTM) cells; the neural network includes word embedding, encoder/decoder\n",
    "- **Model 2** Implement Attention Model with Bi-Directional RNN cells \n",
    "- **Prediction** Run the model on English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport helper, tests\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "import project_tests as tests\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logger\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify access to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We begin by investigating the dataset that will be used to train and evaluate your pipeline.  The most common datasets used for machine translation are from [WMT](http://www.statmt.org/).  \n",
    "### Load Data\n",
    "The data is located in `data/small_vocab_en` and `data/small_vocab_fr`. The `small_vocab_en` file contains English sentences with their French translations in the `small_vocab_fr` file. Load the English and French data from these files from running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load English data\n",
    "english_sentences = helper.load_data('data\\small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = helper.load_data('data\\small_vocab_fr')\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files\n",
    "Each line in `small_vocab_en` contains an English sentence with the respective translation in each line of `small_vocab_fr`.  View the first two lines from each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(english_sentences), len(french_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "Three steps in the text preprocess\n",
    "\n",
    "- 1. **Vocabulary Creation**\n",
    "- 2. **Tokenize** Implemented with Keras\n",
    "- 3. **Padding to the same length** Implemented with Keras\n",
    "\n",
    "The complexity of the problem is determined by the complexity of the vocabulary.  A more complex vocabulary is a more complex problem.  Let's look at the complexity of the dataset we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    text_tokenizer = Tokenizer()\n",
    "    text_tokenizer.fit_on_texts(x)\n",
    "    text_tokenized = text_tokenizer.texts_to_sequences(x)\n",
    "    \n",
    "    return text_tokenized, text_tokenizer\n",
    "\n",
    "tests.test_tokenize(tokenize)\n",
    "\n",
    "# Tokenize Example output\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    max_length = 0\n",
    "    \n",
    "    if length!=None:\n",
    "        max_length = length\n",
    "    else:\n",
    "        for i in x:\n",
    "            if len(i) > max_length:\n",
    "                max_length = len(i)\n",
    "                \n",
    "    return pad_sequences(x, maxlen=max_length, padding='post')\n",
    "\n",
    "tests.test_pad(pad)\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders\n",
    "\n",
    "To write the dataloaders format in Pytorch\n",
    "- Split Dataset to Train and Validation and Test\n",
    "- Applied into customized dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split(x_data, y_data, split_ratio=0.2):\n",
    "    \n",
    "    assert(x_data.shape[0] == y_data.shape[0])\n",
    "    data_length = x_data.shape[0]\n",
    "    index = np.random.permutation(data_length).tolist()\n",
    "    train_data_x = x_data[index[0:int(data_length*(1-split_ratio))], :]\n",
    "    train_data_y = y_data[index[0:int(data_length*(1-split_ratio))], :]\n",
    "    test_data_x = x_data[index[int(data_length*(1-split_ratio)):-1], :]\n",
    "    test_data_y = y_data[index[int(data_length*(1-split_ratio)):-1], :]\n",
    "    train_data = (train_data_x, train_data_y)\n",
    "    test_data = (test_data_x, test_data_y)\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    ### Sequence Dataset\n",
    "    \n",
    "    def __init__(self, sequences_in, sequences_out):\n",
    "        super().__init__()\n",
    "        self.len = sequences_in.shape[0]\n",
    "        self.x_data = torch.from_numpy(sequences_in).long()\n",
    "        self.y_data = torch.from_numpy(sequences_out).long()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_valid_split(preproc_english_sentences, preproc_french_sentences, split_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train_data[1].reshape(-1, 21)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TimeSeriesDataset(train_data[0], train_data[1].reshape(-1, max_french_sequence_length))\n",
    "test_dataset = TimeSeriesDataset(test_data[0], test_data[1].reshape(-1, max_french_sequence_length))\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = {'train':len(train_dataset), 'test':len(test_dataset)}\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = dict()\n",
    "dataloaders['train'] = train_loader\n",
    "dataloaders['test'] = test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "- **Model 1** Bi-Directional RNN with (GRU/LSTM) cells; the neural network includes word embedding, encoder/decoder\n",
    "- **Model 2** Implement Attention Model with Bi-Directional RNN cells \n",
    "\n",
    "After experimenting with the four simple architectures, you will construct a deeper architecture that is designed to outperform all four models.\n",
    "### Ids Back to Text\n",
    "The neural network will be translating the input to words ids, which isn't the final form we want.  We want the French translation.  The function `logits_to_text` will bridge the gab between the logits from the neural network to the French translation.  You'll be using this function to better understand the output of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define help Model (Customized TimeDistributed Model)\n",
    "class TimeDistributed(nn.Module):\n",
    "    \n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size)\n",
    "\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().reshape(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Feedthrough Model\n",
    "class Model1(nn.Module):\n",
    "    \n",
    "    def __init__(self, english_vocab_size, french_vocab_size, max_french_sequence_length, \n",
    "                 embedding_dim=200, hidden_dim=100, rnn_module = nn.LSTM):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.emb_vector = nn.Embedding(english_vocab_size+1\n",
    "                                       , embedding_dim)\n",
    "        self.enc_rnn_1 = rnn_module(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=1, \n",
    "                                    batch_first=True, bidirectional=True)\n",
    "        self.dec_rnn_1 = rnn_module(input_size=2*hidden_dim, hidden_size=hidden_dim, num_layers=1, \n",
    "                                    batch_first=True, bidirectional=True)\n",
    "        #self.bn = nn.BatchNorm1d(2*hidden_dim)\n",
    "        self.fc = nn.Linear(2*hidden_dim, french_vocab_size+1)\n",
    "        #self.time_series = TimeDistributed(self.fc, batch_first=True)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        embd_inputs = self.emb_vector(inputs)\n",
    "        en_out, en_hn = self.enc_rnn_1(embd_inputs)\n",
    "        en_out_end = en_out[:, -1]\n",
    "        decode_inputs = en_out_end.view(en_out_end.size()[0], 1, -1)\n",
    "        decode_inputs = decode_inputs.repeat(1, max_french_sequence_length, 1)\n",
    "        de_out, dn_hn = self.dec_rnn_1(decode_inputs)\n",
    "        # Add Batch Norm \n",
    "        bn_1 = nn.BatchNorm1d(de_out.shape[2])\n",
    "        b_de_out = bn_1(de_out.contiguous().view(de_out.shape[0], de_out.shape[2], de_out.shape[1])) \n",
    "        #shape=(batch, catogories, time-series-length)\n",
    "        # softmax along the catogories axis\n",
    "        outputs = F.softmax(self.fc(b_de_out.view(de_out.shape[0], de_out.shape[1], de_out.shape[2])), \n",
    "                            dim=2) #shape=(batch, time-series-length, catogories)\n",
    "        #outputs = F.softmax(self.time_series(b_de_out.contiguous().\n",
    "        #                                    reshape(de_out.shape[0], de_out.shape[1], de_out.shape[2])), dim=2)\n",
    "        #outputs = F.softmax(TimeDistributed(self.fc, batch_first=True)(de_out), dim=2)\n",
    "        \n",
    "        return outputs.view(-1, outputs.shape[2], outputs.shape[1]) #shape=(batch, catogories, time-series-length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define help Model (Customized TimeDistributed Model)\n",
    "class TimeDistributed(nn.Module):\n",
    "    \n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size)\n",
    "\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Scoring Function\n",
    "def score_multiply(hx, enc_h):\n",
    "    score = F.softmax(torch.matmul(enc_h, \n",
    "                                   hx.view(hx.shape[0], hx.shape[1], 1)),\n",
    "                      dim=1)\n",
    "    batch = score.shape[0]\n",
    "    seq_length = score.shape[1]\n",
    "    enc_h_new = torch.mul(enc_h, score)\n",
    "    atten_vec = torch.sum(enc_h_new, dim=1)\n",
    "    \n",
    "    return atten_vec\n",
    "\n",
    "## Define Attention_Decoder_Model\n",
    "class Attention_Decode(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_h, input_dim, hidden_dim, rnn_module=nn.LSTMCell):\n",
    "        \n",
    "        # enc_h is encoded hidden tensor \n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.enc_h = enc_h\n",
    "        self.dec_rnn = rnn_module(input_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, ini_x, ini_hc):\n",
    "        \n",
    "        # ini_x is the tensor for the initial input \n",
    "        # ini_hc is the tensor for the initial hidden layer; LSTMcell will be h and c states\n",
    "        # hx, cx = LSTMcell(ini_x, ini_hc)\n",
    "        # atten_vec = score(hx, enc_h) \n",
    "        # Glued atten_vec to hx => cat(atten_vec, hx)\n",
    "        # x_next = tanh(wc[Glued_vector])\n",
    "        # hc_next = (hx, cx)\n",
    "        # foward (x_next, h_next) to create the next layer (repeat length times)\n",
    "        \n",
    "        hx, cx = self.dec_rnn(ini_x, (ini_hc[0], ini_hc[1]))\n",
    "        atten_vec = score_multiply(hx, self.enc_h)\n",
    "        glued_vector = torch.cat((atten_vec, hx), dim=1)\n",
    "        x_next = F.tanh(nn.Linear(glued_vector.shape[1], self.input_dim)(glued_vector))\n",
    "        hc_next = (hx, cx)\n",
    "  \n",
    "        return x_next, hc_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Feedthrough Model\n",
    "class Model2(nn.Module):\n",
    "    \n",
    "    def __init__(self, english_vocab_size, french_vocab_size, max_french_sequence_length, \n",
    "                 embedding_dim=200, hidden_dim=100, rnn_encode_module=nn.LSTM, rnn_decode_cell=nn.LSTMCell):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_vector = nn.Embedding(english_vocab_size+1, embedding_dim)\n",
    "        self.enc_rnn_1 = rnn_encode_module(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=1, \n",
    "                                    batch_first=True, bidirectional=True)\n",
    "        self.rnn_decode_cell = rnn_decode_cell\n",
    "        self.fc = nn.Linear(embedding_dim, french_vocab_size+1)\n",
    "        #self.time_series = TimeDistributed(self.fc, batch_first=True)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        embd_inputs = self.emb_vector(inputs)\n",
    "        en_out, en_hn = self.enc_rnn_1(embd_inputs)\n",
    "        enc_h = en_out # Treat the output as the hidden state\n",
    "        #Initiate Attention Decode Module        \n",
    "        Decode_process = nn.ModuleList([Attention_Decode(enc_h, input_dim=self.embedding_dim, hidden_dim=self.hidden_dim*2, \n",
    "                                                         rnn_module=nn.LSTMCell) for i in range(max_french_sequence_length)])\n",
    "        # 1st x_next and h_next\n",
    "        # x_next is <END>\n",
    "        # hc_next is zero tensor with the correct dimension \n",
    "        h_0 = torch.zeros((inputs.shape[0], self.hidden_dim*2))\n",
    "        c_0 = torch.zeros((inputs.shape[0], self.hidden_dim*2))\n",
    "        # i_0 should implement with the <END> embed matrix (batch, embd_dim)\n",
    "        i_0 = self.emb_vector(torch.zeros((inputs.shape[0])).type(torch.LongTensor))\n",
    "        x_next = i_0\n",
    "        hc_next = (h_0, c_0)\n",
    "        x_sequence = list()\n",
    "        hc_sequence = list() # h contains (h, c)\n",
    "        \n",
    "        for i, decode in enumerate(Decode_process):\n",
    "            x_next, hc_next = decode(x_next, hc_next)\n",
    "            x_sequence.append(x_next)\n",
    "            hc_sequence.append(hc_next)\n",
    "        \n",
    "        # stack x_sequence and hc_sequence\n",
    "        x_sequence = [i.view(i.shape[0], 1, -1) for i in x_sequence]\n",
    "        outputs_c = torch.cat(x_sequence, dim=1)\n",
    "        # BatchNorm\n",
    "        bn_1 = nn.BatchNorm1d(outputs_c.shape[2])\n",
    "        outputs_c_bn = bn_1(outputs_c.contiguous().view(outputs_c.shape[0], outputs_c.shape[2], outputs_c.shape[1])) \n",
    "        # softmax along the final matrix\n",
    "        #\n",
    "        outputs = F.softmax(self.fc(outputs_c_bn.view\n",
    "                                    (outputs_c.shape[0], outputs_c.shape[1], outputs_c.shape[2])), dim=2)    \n",
    "        #outputs = F.softmax(self.time_series(outputs_c_bn.view\n",
    "                                             #(outputs_c.shape[0], outputs_c.shape[1], outputs_c.shape[2])), dim=2)\n",
    "        \n",
    "        return outputs.view(-1, outputs.shape[2], outputs.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loss/Accuracy and Optimization/LRrate Function Setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model2(english_vocab_size, french_vocab_size, max_french_sequence_length, \n",
    "                 embedding_dim=200, hidden_dim=100, rnn_encode_module=nn.LSTM, rnn_decode_cell=nn.LSTMCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_decay(epoch):\n",
    "    lr_matrix = np.ones(18)\n",
    "    lr_matrix[0:12] = 5e-3\n",
    "    lr_matrix[12:16] = 2e-3\n",
    "    lr_matrix[16:] = 1e-3\n",
    "    #lr_matrix[17:] = 0.8e-3\n",
    "    \n",
    "    return lr_matrix[epoch]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_lr_scheduler = lr_scheduler.LambdaLR(optimizer=optimizer_ft, lr_lambda=lr_decay, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Validation Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Input and Output Tensor Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(dataloaders['train'])\n",
    "b = list(dataloaders['test'])\n",
    "print(len(a), len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, label = a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input[0].view(1,-1).shape)\n",
    "print(label[0].view(1,-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(output, label)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mac_train_model(model, criterion, optimizer, scheduler, num_epochs=18):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 24)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode; won't alter to different dropout and BatchNorm weights\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # turn on the tracking history in train and turn off the tracking history in others\n",
    "                with torch.set_grad_enabled(phase == 'train'): #set gradient calculation enabled\n",
    "                    outputs = model(inputs)\n",
    "                    max_tensor, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.shape[0]\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_size[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_size[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_1 = mac_train_model(model=model, criterion=criterion, optimizer=optimizer_ft, \n",
    "                          scheduler=exp_lr_scheduler, num_epochs=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tk = english_tokenizer\n",
    "y_tk = french_tokenizer\n",
    "x_id_to_word = {value: key for key, value in x_tk.word_index.items()}\n",
    "y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "x_id_to_word[0] = '<PAD>'\n",
    "y_id_to_word[0] = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "pred_input = input[index].view(1,-1) \n",
    "#print(pred_input.shape)\n",
    "pred_input = pred_input.numpy()\n",
    "pred_input.reshape(-1)\n",
    "\n",
    "output = test_1(input[index].view(1,-1))\n",
    "\n",
    "pred_output = output.data.view(french_vocab_size, max_french_sequence_length).numpy()\n",
    "#print(pred_output.shape)\n",
    "pred_output[0]\n",
    "\n",
    "target_output = label[index].numpy()\n",
    "\n",
    "x_tk = english_tokenizer\n",
    "y_tk = french_tokenizer\n",
    "x_id_to_word = {value: key for key, value in x_tk.word_index.items()}\n",
    "y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "x_id_to_word[0] = '<PAD>'\n",
    "y_id_to_word[0] = '<PAD>'\n",
    "x_id_to_word[pred_input.reshape(-1)[0]]\n",
    "print(' '.join([x_id_to_word[x] for x in pred_input.reshape(-1)]))\n",
    "print(' '.join([y_id_to_word[x] for x in np.argmax(pred_output, axis=0)]))\n",
    "print('correct French sentence')\n",
    "print(' '.join([x_id_to_word[x] for x in target_output]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the html\n",
    "\n",
    "**Save your notebook before running the next cell to generate the HTML output.** Then submit your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save before you run this cell!\n",
    "!!jupyter nbconvert *.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Enhancements\n",
    "\n",
    "This project focuses on learning various network architectures for machine translation, but we don't evaluate the models according to best practices by splitting the data into separate test & training sets -- so the model accuracy is overstated. Use the [`sklearn.model_selection.train_test_split()`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to create separate training & test datasets, then retrain each of the models using only the training set and evaluate the prediction accuracy using the hold out test set. Does the \"best\" model change?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
