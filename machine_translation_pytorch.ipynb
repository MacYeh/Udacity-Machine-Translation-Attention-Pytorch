{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Machine Translation Project (PyTorch Framework)\n",
    "\n",
    "## Introduction\n",
    "In this notebook, the machine translation end2end pipeline is implemented; two DL models are implemented with **PyTorch** Frameworks; the goal is to translate from English to French\n",
    "\n",
    "- **Preprocess Pipeline** - Convert text to sequence of integers.\n",
    "- **Model 1** Bi-Directional RNN with (GRU/LSTM) cells; the neural network includes word embedding, encoder/decoder\n",
    "- **Model 2** Implement Attention Model with Bi-Directional RNN cells \n",
    "- **Prediction** Run the model on English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport helper, tests\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "import project_tests as tests\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import logger\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify access to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "We begin by investigating the dataset that will be used to train and evaluate your pipeline.  The most common datasets used for machine translation are from [WMT](http://www.statmt.org/).  \n",
    "### Load Data\n",
    "The data is located in `data/small_vocab_en` and `data/small_vocab_fr`. The `small_vocab_en` file contains English sentences with their French translations in the `small_vocab_fr` file. Load the English and French data from these files from running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n"
     ]
    }
   ],
   "source": [
    "# Load English data\n",
    "english_sentences = helper.load_data('data/small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = helper.load_data('data/small_vocab_fr')\n",
    "\n",
    "print('Dataset Loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files\n",
    "Each line in `small_vocab_en` contains an English sentence with the respective translation in each line of `small_vocab_fr`.  View the first two lines from each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "small_vocab_fr Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "small_vocab_en Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "small_vocab_fr Line 2:  les états-unis est généralement froid en juillet , et il gèle habituellement en novembre .\n"
     ]
    }
   ],
   "source": [
    "for sample_i in range(2):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137861, 137861)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_sentences), len(french_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "Three steps in the text preprocess\n",
    "\n",
    "- 1. **Vocabulary Creation**\n",
    "- 2. **Tokenize** Implemented with Keras\n",
    "- 3. **Padding to the same length** Implemented with Keras\n",
    "\n",
    "The complexity of the problem is determined by the complexity of the vocabulary.  A more complex vocabulary is a more complex problem.  Let's look at the complexity of the dataset we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    text_tokenizer = Tokenizer()\n",
    "    text_tokenizer.fit_on_texts(x)\n",
    "    text_tokenized = text_tokenizer.texts_to_sequences(x)\n",
    "    \n",
    "    return text_tokenized, text_tokenizer\n",
    "\n",
    "tests.test_tokenize(tokenize)\n",
    "\n",
    "# Tokenize Example output\n",
    "text_sentences = [\n",
    "    'The quick brown fox jumps over the lazy dog .',\n",
    "    'By Jove , my quick study of lexicography won a prize .',\n",
    "    'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    max_length = 0\n",
    "    \n",
    "    if length!=None:\n",
    "        max_length = length\n",
    "    else:\n",
    "        for i in x:\n",
    "            if len(i) > max_length:\n",
    "                max_length = len(i)\n",
    "                \n",
    "    return pad_sequences(x, maxlen=max_length, padding='post')\n",
    "\n",
    "tests.test_pad(pad)\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "Max English sentence length: 15\n",
      "Max French sentence length: 21\n",
      "English vocabulary size: 199\n",
      "French vocabulary size: 344\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "    \n",
    "max_english_sequence_length = preproc_english_sentences.shape[1]\n",
    "max_french_sequence_length = preproc_french_sentences.shape[1]\n",
    "english_vocab_size = len(english_tokenizer.word_index)\n",
    "french_vocab_size = len(french_tokenizer.word_index)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "print(\"Max English sentence length:\", max_english_sequence_length)\n",
    "print(\"Max French sentence length:\", max_french_sequence_length)\n",
    "print(\"English vocabulary size:\", english_vocab_size)\n",
    "print(\"French vocabulary size:\", french_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders\n",
    "\n",
    "To write the dataloaders format in Pytorch\n",
    "- Split Dataset to Train and Validation and Test\n",
    "- Applied into customized dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_valid_split(x_data, y_data, split_ratio=0.2):\n",
    "    \n",
    "    assert(x_data.shape[0] == y_data.shape[0])\n",
    "    data_length = x_data.shape[0]\n",
    "    index = np.random.permutation(data_length).tolist()\n",
    "    train_data_x = x_data[index[0:int(data_length*(1-split_ratio))], :]\n",
    "    train_data_y = y_data[index[0:int(data_length*(1-split_ratio))], :]\n",
    "    test_data_x = x_data[index[int(data_length*(1-split_ratio)):-1], :]\n",
    "    test_data_y = y_data[index[int(data_length*(1-split_ratio)):-1], :]\n",
    "    train_data = (train_data_x, train_data_y)\n",
    "    test_data = (test_data_x, test_data_y)\n",
    "    \n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    ### Sequence Dataset\n",
    "    \n",
    "    def __init__(self, sequences_in, sequences_out):\n",
    "        super().__init__()\n",
    "        self.len = sequences_in.shape[0]\n",
    "        self.x_data = torch.from_numpy(sequences_in).long()\n",
    "        self.y_data = torch.from_numpy(sequences_out).long()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_valid_split(preproc_english_sentences, preproc_french_sentences, split_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110288, 21)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = train_data[1].reshape(-1, 21)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110288\n",
      "27572\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TimeSeriesDataset(train_data[0], train_data[1].reshape(-1, max_french_sequence_length))\n",
    "test_dataset = TimeSeriesDataset(test_data[0], test_data[1].reshape(-1, max_french_sequence_length))\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': 27572, 'train': 110288}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_size = {'train':len(train_dataset), 'test':len(test_dataset)}\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = dict()\n",
    "dataloaders['train'] = train_loader\n",
    "dataloaders['test'] = test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "- **Model 1** Bi-Directional RNN with (GRU/LSTM) cells; the neural network includes word embedding, encoder/decoder\n",
    "- **Model 2** Implement Attention Model with Bi-Directional RNN cells \n",
    "\n",
    "After experimenting with the four simple architectures, you will construct a deeper architecture that is designed to outperform all four models.\n",
    "### Ids Back to Text\n",
    "The neural network will be translating the input to words ids, which isn't the final form we want.  We want the French translation.  The function `logits_to_text` will bridge the gab between the logits from the neural network to the French translation.  You'll be using this function to better understand the output of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define help Model (Customized TimeDistributed Model)\n",
    "class TimeDistributed(nn.Module):\n",
    "    \n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size)\n",
    "\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().reshape(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Feedthrough Model\n",
    "class Model1(nn.Module):\n",
    "    \n",
    "    def __init__(self, english_vocab_size, french_vocab_size, max_french_sequence_length, \n",
    "                 embedding_dim=200, hidden_dim=100, rnn_module = nn.LSTM):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.emb_vector = nn.Embedding(english_vocab_size+1\n",
    "                                       , embedding_dim)\n",
    "        self.enc_rnn_1 = rnn_module(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=1, \n",
    "                                    batch_first=True, bidirectional=True)\n",
    "        self.dec_rnn_1 = rnn_module(input_size=2*hidden_dim, hidden_size=hidden_dim, num_layers=1, \n",
    "                                    batch_first=True, bidirectional=True)\n",
    "        #self.bn = nn.BatchNorm1d(2*hidden_dim)\n",
    "        self.fc = nn.Linear(2*hidden_dim, french_vocab_size+1)\n",
    "        #self.time_series = TimeDistributed(self.fc, batch_first=True)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        embd_inputs = self.emb_vector(inputs)\n",
    "        en_out, en_hn = self.enc_rnn_1(embd_inputs)\n",
    "        en_out_end = en_out[:, -1]\n",
    "        decode_inputs = en_out_end.view(en_out_end.size()[0], 1, -1)\n",
    "        decode_inputs = decode_inputs.repeat(1, max_french_sequence_length, 1)\n",
    "        de_out, dn_hn = self.dec_rnn_1(decode_inputs)\n",
    "        # Add Batch Norm \n",
    "        bn_1 = nn.BatchNorm1d(de_out.shape[2]).to(device)\n",
    "        b_de_out = bn_1(de_out.contiguous().view(de_out.shape[0], de_out.shape[2], de_out.shape[1])) \n",
    "        #shape=(batch, catogories, time-series-length)\n",
    "        # softmax along the catogories axis\n",
    "        outputs = self.fc(b_de_out.view(de_out.shape[0], de_out.shape[1], de_out.shape[2]))\n",
    "        #outputs = F.softmax(self.fc(b_de_out.view(de_out.shape[0], de_out.shape[1], de_out.shape[2])), \n",
    "                            #dim=2) #shape=(batch, time-series-length, catogories)\n",
    "        #outputs = F.softmax(self.time_series(b_de_out.contiguous().\n",
    "        #                                    reshape(de_out.shape[0], de_out.shape[1], de_out.shape[2])), dim=2)\n",
    "        #outputs = F.softmax(TimeDistributed(self.fc, batch_first=True)(de_out), dim=2)\n",
    "        #outputs = torch.exp(outputs)\n",
    "        \n",
    "        return outputs.view(-1, outputs.shape[2], outputs.shape[1]) #shape=(batch, catogories, time-series-length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Model 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define help Model (Customized TimeDistributed Model)\n",
    "class TimeDistributed(nn.Module):\n",
    "    \n",
    "    def __init__(self, module, batch_first=False):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if len(x.size()) <= 2:\n",
    "            return self.module(x)\n",
    "\n",
    "        # Squash samples and timesteps into a single axis\n",
    "        x_reshape = x.contiguous().view(-1, x.size(-1))  # (samples * timesteps, input_size)\n",
    "\n",
    "        y = self.module(x_reshape)\n",
    "\n",
    "        # We have to reshape Y\n",
    "        if self.batch_first:\n",
    "            y = y.contiguous().view(x.size(0), -1, y.size(-1))  # (samples, timesteps, output_size)\n",
    "        else:\n",
    "            y = y.view(-1, x.size(1), y.size(-1))  # (timesteps, samples, output_size)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define Scoring Function\n",
    "def score_multiply(hx, enc_h):\n",
    "    score = F.softmax(torch.matmul(enc_h, \n",
    "                                   hx.view(hx.shape[0], hx.shape[1], 1)),\n",
    "                      dim=1)\n",
    "    batch = score.shape[0]\n",
    "    seq_length = score.shape[1]\n",
    "    enc_h_new = torch.mul(enc_h, score)\n",
    "    atten_vec = torch.sum(enc_h_new, dim=1)\n",
    "    \n",
    "    return atten_vec\n",
    "\n",
    "## Define Attention_Decoder_Model\n",
    "class Attention_Decode(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_h, input_dim, hidden_dim, rnn_module=nn.LSTMCell):\n",
    "        \n",
    "        # enc_h is encoded hidden tensor \n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.enc_h = enc_h\n",
    "        self.dec_rnn = rnn_module(input_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, ini_x, ini_hc):\n",
    "        \n",
    "        # ini_x is the tensor for the initial input \n",
    "        # ini_hc is the tensor for the initial hidden layer; LSTMcell will be h and c states\n",
    "        # hx, cx = LSTMcell(ini_x, ini_hc)\n",
    "        # atten_vec = score(hx, enc_h) \n",
    "        # Glued atten_vec to hx => cat(atten_vec, hx)\n",
    "        # x_next = tanh(wc[Glued_vector])\n",
    "        # hc_next = (hx, cx)\n",
    "        # foward (x_next, h_next) to create the next layer (repeat length times)\n",
    "        \n",
    "        hx, cx = self.dec_rnn(ini_x, (ini_hc[0], ini_hc[1]))\n",
    "        atten_vec = score_multiply(hx, self.enc_h)\n",
    "        glued_vector = torch.cat((atten_vec, hx), dim=1)\n",
    "        x_next = F.tanh(nn.Linear(glued_vector.shape[1], self.input_dim)(glued_vector))\n",
    "        hc_next = (hx, cx)\n",
    "  \n",
    "        return x_next, hc_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Feedthrough Model\n",
    "class Model2(nn.Module):\n",
    "    \n",
    "    def __init__(self, english_vocab_size, french_vocab_size, max_french_sequence_length, \n",
    "                 embedding_dim=200, hidden_dim=100, rnn_encode_module=nn.LSTM, rnn_decode_cell=nn.LSTMCell):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_vector = nn.Embedding(english_vocab_size+1, embedding_dim)\n",
    "        self.enc_rnn_1 = rnn_encode_module(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=1, \n",
    "                                    batch_first=True, bidirectional=True)\n",
    "        self.rnn_decode_cell = rnn_decode_cell\n",
    "        self.fc = nn.Linear(embedding_dim, french_vocab_size+1)\n",
    "        #self.time_series = TimeDistributed(self.fc, batch_first=True)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        embd_inputs = self.emb_vector(inputs)\n",
    "        en_out, en_hn = self.enc_rnn_1(embd_inputs)\n",
    "        enc_h = en_out # Treat the output as the hidden state\n",
    "        #Initiate Attention Decode Module        \n",
    "        Decode_process = nn.ModuleList([Attention_Decode(enc_h, input_dim=self.embedding_dim, hidden_dim=self.hidden_dim*2, \n",
    "                                                         rnn_module=nn.LSTMCell).to(device) \n",
    "                                        for i in range(max_french_sequence_length)])\n",
    "        # 1st x_next and h_next\n",
    "        # x_next is <END>\n",
    "        # hc_next is zero tensor with the correct dimension \n",
    "        h_0 = torch.zeros((inputs.shape[0], self.hidden_dim*2))\n",
    "        c_0 = torch.zeros((inputs.shape[0], self.hidden_dim*2))\n",
    "        # i_0 should implement with the <END> embed matrix (batch, embd_dim)\n",
    "        i_0 = self.emb_vector(torch.zeros((inputs.shape[0])).type(torch.LongTensor))\n",
    "        x_next = i_0\n",
    "        hc_next = (h_0, c_0)\n",
    "        x_sequence = list()\n",
    "        hc_sequence = list() # h contains (h, c)\n",
    "        \n",
    "        for i, decode in enumerate(Decode_process):\n",
    "            x_next, hc_next = decode(x_next, hc_next)\n",
    "            x_sequence.append(x_next)\n",
    "            hc_sequence.append(hc_next)\n",
    "        \n",
    "        # stack x_sequence and hc_sequence\n",
    "        x_sequence = [i.view(i.shape[0], 1, -1) for i in x_sequence]\n",
    "        outputs_c = torch.cat(x_sequence, dim=1)\n",
    "        # BatchNorm\n",
    "        bn_1 = nn.BatchNorm1d(outputs_c.shape[2]).to(device)\n",
    "        outputs_c_bn = bn_1(outputs_c.contiguous().view(outputs_c.shape[0], outputs_c.shape[2], outputs_c.shape[1])) \n",
    "        # softmax along the final matrix\n",
    "        #\n",
    "        outputs = F.softmax(self.fc(outputs_c_bn.view\n",
    "                                    (outputs_c.shape[0], outputs_c.shape[1], outputs_c.shape[2])), dim=2)    \n",
    "        #outputs = F.softmax(self.time_series(outputs_c_bn.view\n",
    "                                             #(outputs_c.shape[0], outputs_c.shape[1], outputs_c.shape[2])), dim=2)\n",
    "        \n",
    "        return outputs.view(-1, outputs.shape[2], outputs.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loss/Accuracy and Optimization/LRrate Function Setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model1(english_vocab_size, french_vocab_size, max_french_sequence_length, \n",
    "                 embedding_dim=200, hidden_dim=100, rnn_module = nn.LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning_Rate Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mac_lr_finder(model, criterion, lr_logrange=[-5, 0]):\n",
    "    \n",
    "    # numbers of iterations\n",
    "    n_iter = len(list(dataloaders['train'])) #108\n",
    "    \n",
    "    # lr rate matrix \n",
    "    lr_matrix = np.logspace(lr_logrange[0], lr_logrange[1], n_iter)\n",
    "    loss_matrix = np.zeros(n_iter)\n",
    "    \n",
    "    for epoch in range(1):\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode; won't alter to different dropout and BatchNorm weights\n",
    "\n",
    "            idx = 0\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Set Optimizer\n",
    "                #optimizer = optim.SGD(model.parameters(), \n",
    "                 #                     lr=lr_matrix[idx], \n",
    "                  #                    momentum=1,\n",
    "                   #                   nesterov=True)\n",
    "                \n",
    "                optimizer = optim.Adam(model.parameters(), \n",
    "                                       amsgrad=True)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward\n",
    "                # turn on the tracking history in train and turn off the tracking history in others\n",
    "                with torch.set_grad_enabled(phase == 'train'): #set gradient calculation enabled\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss_matrix[idx] = loss.item()\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                idx = idx+1\n",
    "                        \n",
    "    lr_matrix = lr_matrix.reshape((lr_matrix.shape[0], 1))\n",
    "    loss_matrix = loss_matrix.reshape((loss_matrix.shape[0], 1))\n",
    "\n",
    "    return np.concatenate((lr_matrix, loss_matrix), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_loss_matrix = mac_lr_finder(model=model, criterion=criterion, lr_logrange=[-5, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdf572f0da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_loss_matrix[:, 0], lr_loss_matrix[:, 1])\n",
    "plt.xlabel('learning rate(log scale)')\n",
    "plt.ylabel('loss')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdf58317d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lr_loss_matrix[:, 0], lr_loss_matrix[:, 1])\n",
    "plt.xlabel('learning rate(log scale)')\n",
    "plt.ylabel('loss')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_adam = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_adam = optim.Adam(model.parameters(), lr=0.1, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sgd = model.to(device)\n",
    "optimizer_sgd = optim.SGD(model.parameters(), lr=0.5, momentum=1, nesterov=True)\n",
    "\n",
    "#exp_lr_scheduler_anneal = lr_scheduler.CosineAnnealingLR(optimizer=optimizer_sgd, \n",
    "#                                                         T_max=400, \n",
    "#                                                         eta_min=7.5e-3, \n",
    "#                                                         last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_decay(epoch):\n",
    "    lr_matrix = np.ones(1200)\n",
    "    lr_matrix[0:300] = 0.1\n",
    "    lr_matrix[300:600] = 5e-2\n",
    "    lr_matrix[600:900] = 1e-2\n",
    "    lr_matrix[900:] = 7.5e-3\n",
    "    \n",
    "    return lr_matrix[epoch]\n",
    "\n",
    "def lr_decay_sgd(epoch):\n",
    "    lr_matrix = np.ones(1200)\n",
    "    lr_matrix[0:300] = 0.5\n",
    "    lr_matrix[300:600] = 0.25\n",
    "    lr_matrix[600:900] = 0.1\n",
    "    lr_matrix[900:] = 0.05\n",
    "    \n",
    "    return lr_matrix[epoch]\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.LambdaLR(optimizer=optimizer_adam, lr_lambda=lr_decay, last_epoch=-1)\n",
    "exp_lr_scheduler_sgd = lr_scheduler.LambdaLR(optimizer=optimizer_sgd, lr_lambda=lr_decay_sgd, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Validation Loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Input and Output Tensor Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 27\n"
     ]
    }
   ],
   "source": [
    "a = list(dataloaders['train'])\n",
    "b = list(dataloaders['test'])\n",
    "print(len(a), len(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "input, label = a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15])\n",
      "torch.Size([1, 21])\n"
     ]
    }
   ],
   "source": [
    "print(input[0].view(1,-1).shape)\n",
    "print(label[0].view(1,-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_adam(input.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 345, 21])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mac_train_model(model, criterion, optimizer, scheduler, num_epochs=18):\n",
    "    since = time.time()\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 24)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode; won't alter to different dropout and BatchNorm weights\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # turn on the tracking history in train and turn off the tracking history in others\n",
    "                with torch.set_grad_enabled(phase == 'train'): #set gradient calculation enabled\n",
    "                    outputs = model(inputs)\n",
    "                    max_tensor, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.shape[0]\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_size[phase]\n",
    "            epoch_acc = running_corrects.double()*100 / (dataset_size[phase]*max_french_sequence_length)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1199\n",
      "------------------------\n",
      "train Loss: 2.2288 Acc: 53.8899\n",
      "test Loss: 1.5999 Acc: 62.0377\n",
      "\n",
      "Epoch 1/1199\n",
      "------------------------\n",
      "train Loss: 1.4043 Acc: 65.2022\n",
      "test Loss: 1.2285 Acc: 68.5834\n",
      "\n",
      "Epoch 2/1199\n",
      "------------------------\n",
      "train Loss: 1.1070 Acc: 71.2066\n",
      "test Loss: 0.9986 Acc: 73.5731\n",
      "\n",
      "Epoch 3/1199\n",
      "------------------------\n",
      "train Loss: 0.9117 Acc: 75.5856\n",
      "test Loss: 0.8345 Acc: 77.4329\n",
      "\n",
      "Epoch 4/1199\n",
      "------------------------\n",
      "train Loss: 0.7801 Acc: 78.7010\n",
      "test Loss: 0.7332 Acc: 80.0142\n",
      "\n",
      "Epoch 5/1199\n",
      "------------------------\n",
      "train Loss: 0.6824 Acc: 81.2853\n",
      "test Loss: 0.6331 Acc: 82.4719\n",
      "\n",
      "Epoch 6/1199\n",
      "------------------------\n",
      "train Loss: 0.5997 Acc: 83.5274\n",
      "test Loss: 0.5824 Acc: 84.0138\n",
      "\n",
      "Epoch 7/1199\n",
      "------------------------\n",
      "train Loss: 0.5445 Acc: 85.0181\n",
      "test Loss: 0.5300 Acc: 85.4801\n",
      "\n",
      "Epoch 8/1199\n",
      "------------------------\n",
      "train Loss: 0.4876 Acc: 86.5498\n",
      "test Loss: 0.4919 Acc: 86.6063\n",
      "\n",
      "Epoch 9/1199\n",
      "------------------------\n",
      "train Loss: 0.4519 Acc: 87.6395\n",
      "test Loss: 0.4487 Acc: 87.9016\n",
      "\n",
      "Epoch 10/1199\n",
      "------------------------\n",
      "train Loss: 0.4204 Acc: 88.4892\n",
      "test Loss: 0.4234 Acc: 88.5398\n",
      "\n",
      "Epoch 11/1199\n",
      "------------------------\n",
      "train Loss: 0.3881 Acc: 89.5006\n",
      "test Loss: 0.4026 Acc: 89.1016\n",
      "\n",
      "Epoch 12/1199\n",
      "------------------------\n",
      "train Loss: 0.3691 Acc: 89.9365\n",
      "test Loss: 0.3793 Acc: 89.7151\n",
      "\n",
      "Epoch 13/1199\n",
      "------------------------\n",
      "train Loss: 0.3494 Acc: 90.4890\n",
      "test Loss: 0.3638 Acc: 90.2527\n",
      "\n",
      "Epoch 14/1199\n",
      "------------------------\n",
      "train Loss: 0.3307 Acc: 90.9821\n",
      "test Loss: 0.3366 Acc: 90.9808\n",
      "\n",
      "Epoch 15/1199\n",
      "------------------------\n",
      "train Loss: 0.3157 Acc: 91.4280\n",
      "test Loss: 0.3353 Acc: 91.0636\n",
      "\n",
      "Epoch 16/1199\n",
      "------------------------\n",
      "train Loss: 0.3066 Acc: 91.7171\n",
      "test Loss: 0.3224 Acc: 91.3895\n",
      "\n",
      "Epoch 17/1199\n",
      "------------------------\n",
      "train Loss: 0.2939 Acc: 92.0983\n",
      "test Loss: 0.3084 Acc: 91.8682\n",
      "\n",
      "Epoch 18/1199\n",
      "------------------------\n",
      "train Loss: 0.2764 Acc: 92.6282\n",
      "test Loss: 0.2980 Acc: 92.2102\n",
      "\n",
      "Epoch 19/1199\n",
      "------------------------\n",
      "train Loss: 0.2594 Acc: 93.0480\n",
      "test Loss: 0.2840 Acc: 92.6102\n",
      "\n",
      "Epoch 20/1199\n",
      "------------------------\n",
      "train Loss: 0.2541 Acc: 93.2382\n",
      "test Loss: 0.2678 Acc: 93.0675\n",
      "\n",
      "Epoch 21/1199\n",
      "------------------------\n",
      "train Loss: 0.2444 Acc: 93.5081\n",
      "test Loss: 0.2704 Acc: 92.8796\n",
      "\n",
      "Epoch 22/1199\n",
      "------------------------\n",
      "train Loss: 0.2368 Acc: 93.6565\n",
      "test Loss: 0.2642 Acc: 93.1067\n",
      "\n",
      "Epoch 23/1199\n",
      "------------------------\n",
      "train Loss: 0.2363 Acc: 93.7121\n",
      "test Loss: 0.2586 Acc: 93.3292\n",
      "\n",
      "Epoch 24/1199\n",
      "------------------------\n",
      "train Loss: 0.2276 Acc: 93.9443\n",
      "test Loss: 0.2588 Acc: 93.2727\n",
      "\n",
      "Epoch 25/1199\n",
      "------------------------\n",
      "train Loss: 0.2215 Acc: 94.1050\n",
      "test Loss: 0.2525 Acc: 93.3703\n",
      "\n",
      "Epoch 26/1199\n",
      "------------------------\n",
      "train Loss: 0.2202 Acc: 94.1124\n",
      "test Loss: 0.2527 Acc: 93.3528\n",
      "\n",
      "Epoch 27/1199\n",
      "------------------------\n",
      "train Loss: 0.2204 Acc: 94.1075\n",
      "test Loss: 0.2366 Acc: 93.8701\n",
      "\n",
      "Epoch 28/1199\n",
      "------------------------\n",
      "train Loss: 0.2129 Acc: 94.3089\n",
      "test Loss: 0.2508 Acc: 93.4689\n",
      "\n",
      "Epoch 29/1199\n",
      "------------------------\n",
      "train Loss: 0.2126 Acc: 94.3272\n",
      "test Loss: 0.2250 Acc: 94.1756\n",
      "\n",
      "Epoch 30/1199\n",
      "------------------------\n",
      "train Loss: 0.1992 Acc: 94.6823\n",
      "test Loss: 0.2210 Acc: 94.2773\n",
      "\n",
      "Epoch 31/1199\n",
      "------------------------\n",
      "train Loss: 0.1954 Acc: 94.7843\n",
      "test Loss: 0.2232 Acc: 94.2113\n",
      "\n",
      "Epoch 32/1199\n",
      "------------------------\n",
      "train Loss: 0.1889 Acc: 94.9608\n",
      "test Loss: 0.2133 Acc: 94.5433\n",
      "\n",
      "Epoch 33/1199\n",
      "------------------------\n",
      "train Loss: 0.1797 Acc: 95.1926\n",
      "test Loss: 0.2219 Acc: 94.2896\n",
      "\n",
      "Epoch 34/1199\n",
      "------------------------\n",
      "train Loss: 0.1742 Acc: 95.3374\n",
      "test Loss: 0.2107 Acc: 94.6160\n",
      "\n",
      "Epoch 35/1199\n",
      "------------------------\n",
      "train Loss: 0.1710 Acc: 95.4418\n",
      "test Loss: 0.2165 Acc: 94.4424\n",
      "\n",
      "Epoch 36/1199\n",
      "------------------------\n",
      "train Loss: 0.1808 Acc: 95.1657\n",
      "test Loss: 0.2134 Acc: 94.5239\n",
      "\n",
      "Epoch 37/1199\n",
      "------------------------\n",
      "train Loss: 0.1794 Acc: 95.2018\n",
      "test Loss: 0.2267 Acc: 94.0716\n",
      "\n",
      "Epoch 38/1199\n",
      "------------------------\n",
      "train Loss: 0.2096 Acc: 94.3334\n",
      "test Loss: 0.2340 Acc: 93.9360\n",
      "\n",
      "Epoch 39/1199\n",
      "------------------------\n",
      "train Loss: 0.2042 Acc: 94.5101\n",
      "test Loss: 0.2188 Acc: 94.3378\n",
      "\n",
      "Epoch 40/1199\n",
      "------------------------\n",
      "train Loss: 0.1924 Acc: 94.8539\n",
      "test Loss: 0.2466 Acc: 93.4494\n",
      "\n",
      "Epoch 41/1199\n",
      "------------------------\n",
      "train Loss: 0.1889 Acc: 94.8973\n",
      "test Loss: 0.2181 Acc: 94.3652\n",
      "\n",
      "Epoch 42/1199\n",
      "------------------------\n",
      "train Loss: 0.1766 Acc: 95.2454\n",
      "test Loss: 0.2113 Acc: 94.4922\n",
      "\n",
      "Epoch 43/1199\n",
      "------------------------\n",
      "train Loss: 0.1710 Acc: 95.3890\n",
      "test Loss: 0.1995 Acc: 94.7927\n",
      "\n",
      "Epoch 44/1199\n",
      "------------------------\n",
      "train Loss: 0.1671 Acc: 95.5235\n",
      "test Loss: 0.2145 Acc: 94.5157\n",
      "\n",
      "Epoch 45/1199\n",
      "------------------------\n",
      "train Loss: 0.1708 Acc: 95.3574\n",
      "test Loss: 0.1965 Acc: 94.9536\n",
      "\n",
      "Epoch 46/1199\n",
      "------------------------\n",
      "train Loss: 0.1632 Acc: 95.6033\n",
      "test Loss: 0.1966 Acc: 94.9856\n",
      "\n",
      "Epoch 47/1199\n",
      "------------------------\n",
      "train Loss: 0.1548 Acc: 95.8246\n",
      "test Loss: 0.2007 Acc: 94.8174\n",
      "\n",
      "Epoch 48/1199\n",
      "------------------------\n",
      "train Loss: 0.1567 Acc: 95.7892\n",
      "test Loss: 0.1926 Acc: 95.0346\n",
      "\n",
      "Epoch 49/1199\n",
      "------------------------\n",
      "train Loss: 0.1479 Acc: 96.0054\n",
      "test Loss: 0.1844 Acc: 95.3030\n",
      "\n",
      "Epoch 50/1199\n",
      "------------------------\n",
      "train Loss: 0.1443 Acc: 96.0986\n",
      "test Loss: 0.1776 Acc: 95.4745\n",
      "\n",
      "Epoch 51/1199\n",
      "------------------------\n",
      "train Loss: 0.1476 Acc: 96.0416\n",
      "test Loss: 0.1885 Acc: 95.1621\n",
      "\n",
      "Epoch 52/1199\n",
      "------------------------\n",
      "train Loss: 0.1521 Acc: 95.9491\n",
      "test Loss: 0.1864 Acc: 95.2241\n",
      "\n",
      "Epoch 53/1199\n",
      "------------------------\n",
      "train Loss: 0.1464 Acc: 96.0486\n",
      "test Loss: 0.1867 Acc: 95.2740\n",
      "\n",
      "Epoch 54/1199\n",
      "------------------------\n",
      "train Loss: 0.1607 Acc: 95.6746\n",
      "test Loss: 0.1949 Acc: 95.0101\n",
      "\n",
      "Epoch 55/1199\n",
      "------------------------\n",
      "train Loss: 0.1549 Acc: 95.8130\n",
      "test Loss: 0.1965 Acc: 94.9920\n",
      "\n",
      "Epoch 56/1199\n",
      "------------------------\n",
      "train Loss: 0.1613 Acc: 95.6675\n",
      "test Loss: 0.1932 Acc: 94.9979\n",
      "\n",
      "Epoch 57/1199\n",
      "------------------------\n",
      "train Loss: 0.1536 Acc: 95.8561\n",
      "test Loss: 0.1957 Acc: 94.9941\n",
      "\n",
      "Epoch 58/1199\n",
      "------------------------\n",
      "train Loss: 0.1590 Acc: 95.6953\n",
      "test Loss: 0.1872 Acc: 95.1172\n",
      "\n",
      "Epoch 59/1199\n",
      "------------------------\n",
      "train Loss: 0.1594 Acc: 95.7079\n",
      "test Loss: 0.2043 Acc: 94.8288\n",
      "\n",
      "Epoch 60/1199\n",
      "------------------------\n",
      "train Loss: 0.1538 Acc: 95.8421\n",
      "test Loss: 0.1900 Acc: 95.1492\n",
      "\n",
      "Epoch 61/1199\n",
      "------------------------\n",
      "train Loss: 0.1457 Acc: 96.0658\n",
      "test Loss: 0.1732 Acc: 95.6174\n",
      "\n",
      "Epoch 62/1199\n",
      "------------------------\n",
      "train Loss: 0.1341 Acc: 96.3914\n",
      "test Loss: 0.1689 Acc: 95.7232\n",
      "\n",
      "Epoch 63/1199\n",
      "------------------------\n",
      "train Loss: 0.1330 Acc: 96.3920\n",
      "test Loss: 0.2006 Acc: 94.8091\n",
      "\n",
      "Epoch 64/1199\n",
      "------------------------\n",
      "train Loss: 0.1545 Acc: 95.8397\n",
      "test Loss: 0.2033 Acc: 94.8307\n",
      "\n",
      "Epoch 65/1199\n",
      "------------------------\n",
      "train Loss: 0.1548 Acc: 95.7760\n",
      "test Loss: 0.1861 Acc: 95.2277\n",
      "\n",
      "Epoch 66/1199\n",
      "------------------------\n",
      "train Loss: 0.1415 Acc: 96.1247\n",
      "test Loss: 0.1848 Acc: 95.2377\n",
      "\n",
      "Epoch 67/1199\n",
      "------------------------\n",
      "train Loss: 0.1392 Acc: 96.2439\n",
      "test Loss: 0.1804 Acc: 95.4010\n",
      "\n",
      "Epoch 68/1199\n",
      "------------------------\n",
      "train Loss: 0.1399 Acc: 96.1986\n",
      "test Loss: 0.1812 Acc: 95.3393\n",
      "\n",
      "Epoch 69/1199\n",
      "------------------------\n",
      "train Loss: 0.1360 Acc: 96.2902\n",
      "test Loss: 0.1822 Acc: 95.3785\n",
      "\n",
      "Epoch 70/1199\n",
      "------------------------\n",
      "train Loss: 0.1344 Acc: 96.3309\n",
      "test Loss: 0.1711 Acc: 95.6075\n",
      "\n",
      "Epoch 71/1199\n",
      "------------------------\n",
      "train Loss: 0.1337 Acc: 96.3904\n",
      "test Loss: 0.1798 Acc: 95.3889\n",
      "\n",
      "Epoch 72/1199\n",
      "------------------------\n",
      "train Loss: 0.1319 Acc: 96.4154\n",
      "test Loss: 0.1763 Acc: 95.5778\n",
      "\n",
      "Epoch 73/1199\n",
      "------------------------\n",
      "train Loss: 0.1237 Acc: 96.6331\n",
      "test Loss: 0.1755 Acc: 95.6170\n",
      "\n",
      "Epoch 74/1199\n",
      "------------------------\n",
      "train Loss: 0.1285 Acc: 96.4999\n",
      "test Loss: 0.1736 Acc: 95.6693\n",
      "\n",
      "Epoch 75/1199\n",
      "------------------------\n",
      "train Loss: 0.1324 Acc: 96.4294\n",
      "test Loss: 0.1810 Acc: 95.4034\n",
      "\n",
      "Epoch 76/1199\n",
      "------------------------\n",
      "train Loss: 0.1281 Acc: 96.4990\n",
      "test Loss: 0.1683 Acc: 95.7206\n",
      "\n",
      "Epoch 77/1199\n",
      "------------------------\n",
      "train Loss: 0.1350 Acc: 96.3376\n",
      "test Loss: 0.1746 Acc: 95.5545\n",
      "\n",
      "Epoch 78/1199\n",
      "------------------------\n",
      "train Loss: 0.1333 Acc: 96.3409\n",
      "test Loss: 0.1968 Acc: 94.9872\n",
      "\n",
      "Epoch 79/1199\n",
      "------------------------\n",
      "train Loss: 0.1881 Acc: 94.9195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.2224 Acc: 94.2188\n",
      "\n",
      "Epoch 80/1199\n",
      "------------------------\n",
      "train Loss: 0.1732 Acc: 95.2621\n",
      "test Loss: 0.2063 Acc: 94.6711\n",
      "\n",
      "Epoch 81/1199\n",
      "------------------------\n",
      "train Loss: 0.1577 Acc: 95.6544\n",
      "test Loss: 0.1884 Acc: 95.0564\n",
      "\n",
      "Epoch 82/1199\n",
      "------------------------\n",
      "train Loss: 0.1995 Acc: 94.4981\n",
      "test Loss: 0.2212 Acc: 94.1025\n",
      "\n",
      "Epoch 83/1199\n",
      "------------------------\n",
      "train Loss: 0.1722 Acc: 95.2346\n",
      "test Loss: 0.1883 Acc: 95.0488\n",
      "\n",
      "Epoch 84/1199\n",
      "------------------------\n",
      "train Loss: 0.1827 Acc: 95.0056\n",
      "test Loss: 0.2180 Acc: 94.2647\n",
      "\n",
      "Epoch 85/1199\n",
      "------------------------\n",
      "train Loss: 0.1627 Acc: 95.5013\n",
      "test Loss: 0.1936 Acc: 94.9452\n",
      "\n",
      "Epoch 86/1199\n",
      "------------------------\n",
      "train Loss: 0.1478 Acc: 95.9031\n",
      "test Loss: 0.1910 Acc: 95.0036\n",
      "\n",
      "Epoch 87/1199\n",
      "------------------------\n",
      "train Loss: 0.1454 Acc: 95.9846\n",
      "test Loss: 0.1761 Acc: 95.5143\n",
      "\n",
      "Epoch 88/1199\n",
      "------------------------\n",
      "train Loss: 0.1527 Acc: 95.7937\n",
      "test Loss: 0.2209 Acc: 94.2005\n",
      "\n",
      "Epoch 89/1199\n",
      "------------------------\n",
      "train Loss: 0.1590 Acc: 95.6294\n",
      "test Loss: 0.1861 Acc: 95.1374\n",
      "\n",
      "Epoch 90/1199\n",
      "------------------------\n",
      "train Loss: 0.1454 Acc: 95.9947\n",
      "test Loss: 0.1996 Acc: 94.9498\n",
      "\n",
      "Epoch 91/1199\n",
      "------------------------\n",
      "train Loss: 0.1522 Acc: 95.7954\n",
      "test Loss: 0.1797 Acc: 95.3863\n",
      "\n",
      "Epoch 92/1199\n",
      "------------------------\n",
      "train Loss: 0.1367 Acc: 96.2406\n",
      "test Loss: 0.1825 Acc: 95.3310\n",
      "\n",
      "Epoch 93/1199\n",
      "------------------------\n",
      "train Loss: 0.1371 Acc: 96.2463\n",
      "test Loss: 0.1666 Acc: 95.7067\n",
      "\n",
      "Epoch 94/1199\n",
      "------------------------\n",
      "train Loss: 0.1330 Acc: 96.3246\n",
      "test Loss: 0.1916 Acc: 94.8858\n",
      "\n",
      "Epoch 95/1199\n",
      "------------------------\n",
      "train Loss: 0.1375 Acc: 96.2066\n",
      "test Loss: 0.1666 Acc: 95.6725\n",
      "\n",
      "Epoch 96/1199\n",
      "------------------------\n",
      "train Loss: 0.1277 Acc: 96.4558\n",
      "test Loss: 0.1711 Acc: 95.6576\n",
      "\n",
      "Epoch 97/1199\n",
      "------------------------\n",
      "train Loss: 0.1202 Acc: 96.6749\n",
      "test Loss: 0.1776 Acc: 95.5636\n",
      "\n",
      "Epoch 98/1199\n",
      "------------------------\n",
      "train Loss: 0.1281 Acc: 96.5087\n",
      "test Loss: 0.1776 Acc: 95.4576\n",
      "\n",
      "Epoch 99/1199\n",
      "------------------------\n",
      "train Loss: 0.1263 Acc: 96.5279\n",
      "test Loss: 0.1691 Acc: 95.7548\n",
      "\n",
      "Epoch 100/1199\n",
      "------------------------\n",
      "train Loss: 0.1223 Acc: 96.6401\n",
      "test Loss: 0.1785 Acc: 95.4405\n",
      "\n",
      "Epoch 101/1199\n",
      "------------------------\n",
      "train Loss: 0.1275 Acc: 96.4929\n",
      "test Loss: 0.1718 Acc: 95.6374\n",
      "\n",
      "Epoch 102/1199\n",
      "------------------------\n",
      "train Loss: 0.1290 Acc: 96.4701\n",
      "test Loss: 0.1622 Acc: 95.8410\n",
      "\n",
      "Epoch 103/1199\n",
      "------------------------\n",
      "train Loss: 0.1181 Acc: 96.7553\n",
      "test Loss: 0.1694 Acc: 95.7205\n",
      "\n",
      "Epoch 104/1199\n",
      "------------------------\n",
      "train Loss: 0.1189 Acc: 96.6998\n",
      "test Loss: 0.1677 Acc: 95.7048\n",
      "\n",
      "Epoch 105/1199\n",
      "------------------------\n",
      "train Loss: 0.1179 Acc: 96.7665\n",
      "test Loss: 0.1673 Acc: 95.7877\n",
      "\n",
      "Epoch 106/1199\n",
      "------------------------\n",
      "train Loss: 0.1236 Acc: 96.6034\n",
      "test Loss: 0.1780 Acc: 95.4566\n",
      "\n",
      "Epoch 107/1199\n",
      "------------------------\n",
      "train Loss: 0.1297 Acc: 96.4579\n",
      "test Loss: 0.1620 Acc: 95.9035\n",
      "\n",
      "Epoch 108/1199\n",
      "------------------------\n",
      "train Loss: 0.1192 Acc: 96.7518\n",
      "test Loss: 0.1661 Acc: 95.8317\n",
      "\n",
      "Epoch 109/1199\n",
      "------------------------\n",
      "train Loss: 0.1139 Acc: 96.8966\n",
      "test Loss: 0.1588 Acc: 96.0662\n",
      "\n",
      "Epoch 110/1199\n",
      "------------------------\n",
      "train Loss: 0.1163 Acc: 96.8517\n",
      "test Loss: 0.1627 Acc: 95.8555\n",
      "\n",
      "Epoch 111/1199\n",
      "------------------------\n",
      "train Loss: 0.1321 Acc: 96.4063\n",
      "test Loss: 0.1845 Acc: 95.4255\n",
      "\n",
      "Epoch 112/1199\n",
      "------------------------\n",
      "train Loss: 0.1480 Acc: 95.9909\n",
      "test Loss: 0.1840 Acc: 95.3407\n",
      "\n",
      "Epoch 113/1199\n",
      "------------------------\n",
      "train Loss: 0.1357 Acc: 96.2901\n",
      "test Loss: 0.1740 Acc: 95.5761\n",
      "\n",
      "Epoch 114/1199\n",
      "------------------------\n",
      "train Loss: 0.1300 Acc: 96.4287\n",
      "test Loss: 0.1898 Acc: 95.2488\n",
      "\n",
      "Epoch 115/1199\n",
      "------------------------\n",
      "train Loss: 0.1333 Acc: 96.3404\n",
      "test Loss: 0.1641 Acc: 95.8790\n",
      "\n",
      "Epoch 116/1199\n",
      "------------------------\n",
      "train Loss: 0.1169 Acc: 96.7836\n",
      "test Loss: 0.1618 Acc: 95.9148\n",
      "\n",
      "Epoch 117/1199\n",
      "------------------------\n",
      "train Loss: 0.1220 Acc: 96.6643\n",
      "test Loss: 0.1719 Acc: 95.6445\n",
      "\n",
      "Epoch 118/1199\n",
      "------------------------\n",
      "train Loss: 0.1222 Acc: 96.6267\n",
      "test Loss: 0.1602 Acc: 95.9605\n",
      "\n",
      "Epoch 119/1199\n",
      "------------------------\n",
      "train Loss: 0.1270 Acc: 96.5126\n",
      "test Loss: 0.1680 Acc: 95.7414\n",
      "\n",
      "Epoch 120/1199\n",
      "------------------------\n",
      "train Loss: 0.1159 Acc: 96.8008\n",
      "test Loss: 0.1574 Acc: 96.0006\n",
      "\n",
      "Epoch 121/1199\n",
      "------------------------\n",
      "train Loss: 0.1061 Acc: 97.0656\n",
      "test Loss: 0.1548 Acc: 96.1160\n",
      "\n",
      "Epoch 122/1199\n",
      "------------------------\n",
      "train Loss: 0.1079 Acc: 97.0242\n",
      "test Loss: 0.1518 Acc: 96.2135\n",
      "\n",
      "Epoch 123/1199\n",
      "------------------------\n",
      "train Loss: 0.1245 Acc: 96.6196\n",
      "test Loss: 0.1775 Acc: 95.5276\n",
      "\n",
      "Epoch 124/1199\n",
      "------------------------\n",
      "train Loss: 0.1346 Acc: 96.3360\n",
      "test Loss: 0.2044 Acc: 95.0065\n",
      "\n",
      "Epoch 125/1199\n",
      "------------------------\n",
      "train Loss: 0.1484 Acc: 95.9781\n",
      "test Loss: 0.1856 Acc: 95.3937\n",
      "\n",
      "Epoch 126/1199\n",
      "------------------------\n",
      "train Loss: 0.1464 Acc: 95.9993\n",
      "test Loss: 0.2202 Acc: 94.4473\n",
      "\n",
      "Epoch 127/1199\n",
      "------------------------\n",
      "train Loss: 0.1498 Acc: 95.8366\n",
      "test Loss: 0.1735 Acc: 95.5096\n",
      "\n",
      "Epoch 128/1199\n",
      "------------------------\n",
      "train Loss: 0.1268 Acc: 96.4665\n",
      "test Loss: 0.1788 Acc: 95.4890\n",
      "\n",
      "Epoch 129/1199\n",
      "------------------------\n",
      "train Loss: 0.1309 Acc: 96.3999\n",
      "test Loss: 0.1785 Acc: 95.3965\n",
      "\n",
      "Epoch 130/1199\n",
      "------------------------\n",
      "train Loss: 0.1215 Acc: 96.6284\n",
      "test Loss: 0.1672 Acc: 95.7319\n",
      "\n",
      "Epoch 131/1199\n",
      "------------------------\n",
      "train Loss: 0.1237 Acc: 96.5546\n",
      "test Loss: 0.1744 Acc: 95.5802\n",
      "\n",
      "Epoch 132/1199\n",
      "------------------------\n",
      "train Loss: 0.1249 Acc: 96.5177\n",
      "test Loss: 0.1629 Acc: 95.8465\n",
      "\n",
      "Epoch 133/1199\n",
      "------------------------\n",
      "train Loss: 0.1225 Acc: 96.6153\n",
      "test Loss: 0.1837 Acc: 95.4229\n",
      "\n",
      "Epoch 134/1199\n",
      "------------------------\n",
      "train Loss: 0.1312 Acc: 96.3785\n",
      "test Loss: 0.1697 Acc: 95.6833\n",
      "\n",
      "Epoch 135/1199\n",
      "------------------------\n",
      "train Loss: 0.1197 Acc: 96.6558\n",
      "test Loss: 0.1637 Acc: 95.8367\n",
      "\n",
      "Epoch 136/1199\n",
      "------------------------\n",
      "train Loss: 0.1149 Acc: 96.8075\n",
      "test Loss: 0.1541 Acc: 96.1134\n",
      "\n",
      "Epoch 137/1199\n",
      "------------------------\n",
      "train Loss: 0.1081 Acc: 96.9989\n",
      "test Loss: 0.1615 Acc: 95.8942\n",
      "\n",
      "Epoch 138/1199\n",
      "------------------------\n",
      "train Loss: 0.1039 Acc: 97.1117\n",
      "test Loss: 0.1581 Acc: 95.9889\n",
      "\n",
      "Epoch 139/1199\n",
      "------------------------\n",
      "train Loss: 0.1278 Acc: 96.4724\n",
      "test Loss: 0.1749 Acc: 95.5241\n",
      "\n",
      "Epoch 140/1199\n",
      "------------------------\n",
      "train Loss: 0.1218 Acc: 96.6046\n",
      "test Loss: 0.1695 Acc: 95.6965\n",
      "\n",
      "Epoch 141/1199\n",
      "------------------------\n",
      "train Loss: 0.1251 Acc: 96.5679\n",
      "test Loss: 0.1772 Acc: 95.5733\n",
      "\n",
      "Epoch 142/1199\n",
      "------------------------\n",
      "train Loss: 0.1312 Acc: 96.4338\n",
      "test Loss: 0.1720 Acc: 95.6248\n",
      "\n",
      "Epoch 143/1199\n",
      "------------------------\n",
      "train Loss: 0.1188 Acc: 96.7293\n",
      "test Loss: 0.1601 Acc: 95.8624\n",
      "\n",
      "Epoch 144/1199\n",
      "------------------------\n",
      "train Loss: 0.1140 Acc: 96.8491\n",
      "test Loss: 0.1576 Acc: 95.9902\n",
      "\n",
      "Epoch 145/1199\n",
      "------------------------\n",
      "train Loss: 0.1091 Acc: 96.9805\n",
      "test Loss: 0.1570 Acc: 96.0419\n",
      "\n",
      "Epoch 146/1199\n",
      "------------------------\n",
      "train Loss: 0.1085 Acc: 96.9883\n",
      "test Loss: 0.1522 Acc: 96.1740\n",
      "\n",
      "Epoch 147/1199\n",
      "------------------------\n",
      "train Loss: 0.1337 Acc: 96.3868\n",
      "test Loss: 0.2214 Acc: 94.4124\n",
      "\n",
      "Epoch 148/1199\n",
      "------------------------\n",
      "train Loss: 0.1638 Acc: 95.5769\n",
      "test Loss: 0.2041 Acc: 94.8602\n",
      "\n",
      "Epoch 149/1199\n",
      "------------------------\n",
      "train Loss: 0.1432 Acc: 96.0656\n",
      "test Loss: 0.1800 Acc: 95.4586\n",
      "\n",
      "Epoch 150/1199\n",
      "------------------------\n",
      "train Loss: 0.1396 Acc: 96.1446\n",
      "test Loss: 0.1744 Acc: 95.5548\n",
      "\n",
      "Epoch 151/1199\n",
      "------------------------\n",
      "train Loss: 0.1368 Acc: 96.2409\n",
      "test Loss: 0.1986 Acc: 94.9288\n",
      "\n",
      "Epoch 152/1199\n",
      "------------------------\n",
      "train Loss: 0.1646 Acc: 95.5608\n",
      "test Loss: 0.2008 Acc: 94.7839\n",
      "\n",
      "Epoch 153/1199\n",
      "------------------------\n",
      "train Loss: 0.1373 Acc: 96.1759\n",
      "test Loss: 0.1688 Acc: 95.6058\n",
      "\n",
      "Epoch 154/1199\n",
      "------------------------\n",
      "train Loss: 0.1232 Acc: 96.5902\n",
      "test Loss: 0.1694 Acc: 95.5947\n",
      "\n",
      "Epoch 155/1199\n",
      "------------------------\n",
      "train Loss: 0.1140 Acc: 96.8144\n",
      "test Loss: 0.1704 Acc: 95.6908\n",
      "\n",
      "Epoch 156/1199\n",
      "------------------------\n",
      "train Loss: 0.1196 Acc: 96.6988\n",
      "test Loss: 0.1646 Acc: 95.8704\n",
      "\n",
      "Epoch 157/1199\n",
      "------------------------\n",
      "train Loss: 0.1142 Acc: 96.8514\n",
      "test Loss: 0.1580 Acc: 95.9913\n",
      "\n",
      "Epoch 158/1199\n",
      "------------------------\n",
      "train Loss: 0.1110 Acc: 96.9336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1557 Acc: 96.0750\n",
      "\n",
      "Epoch 159/1199\n",
      "------------------------\n",
      "train Loss: 0.1089 Acc: 96.9663\n",
      "test Loss: 0.1604 Acc: 95.9895\n",
      "\n",
      "Epoch 160/1199\n",
      "------------------------\n",
      "train Loss: 0.1054 Acc: 97.0767\n",
      "test Loss: 0.1564 Acc: 96.0797\n",
      "\n",
      "Epoch 161/1199\n",
      "------------------------\n",
      "train Loss: 0.1029 Acc: 97.1250\n",
      "test Loss: 0.1562 Acc: 96.1553\n",
      "\n",
      "Epoch 162/1199\n",
      "------------------------\n",
      "train Loss: 0.1056 Acc: 97.0820\n",
      "test Loss: 0.1694 Acc: 95.7092\n",
      "\n",
      "Epoch 163/1199\n",
      "------------------------\n",
      "train Loss: 0.1206 Acc: 96.7054\n",
      "test Loss: 0.1631 Acc: 95.8887\n",
      "\n",
      "Epoch 164/1199\n",
      "------------------------\n",
      "train Loss: 0.1081 Acc: 97.0036\n",
      "test Loss: 0.1589 Acc: 96.0080\n",
      "\n",
      "Epoch 165/1199\n",
      "------------------------\n",
      "train Loss: 0.1064 Acc: 97.0432\n",
      "test Loss: 0.1555 Acc: 96.1130\n",
      "\n",
      "Epoch 166/1199\n",
      "------------------------\n",
      "train Loss: 0.1063 Acc: 97.0617\n",
      "test Loss: 0.1648 Acc: 95.8650\n",
      "\n",
      "Epoch 167/1199\n",
      "------------------------\n",
      "train Loss: 0.1041 Acc: 97.1084\n",
      "test Loss: 0.1567 Acc: 96.1099\n",
      "\n",
      "Epoch 168/1199\n",
      "------------------------\n",
      "train Loss: 0.1292 Acc: 96.4398\n",
      "test Loss: 0.1926 Acc: 95.0612\n",
      "\n",
      "Epoch 169/1199\n",
      "------------------------\n",
      "train Loss: 0.1437 Acc: 96.0473\n",
      "test Loss: 0.1727 Acc: 95.7065\n",
      "\n",
      "Epoch 170/1199\n",
      "------------------------\n",
      "train Loss: 0.1168 Acc: 96.7324\n",
      "test Loss: 0.1605 Acc: 95.9155\n",
      "\n",
      "Epoch 171/1199\n",
      "------------------------\n",
      "train Loss: 0.1125 Acc: 96.8671\n",
      "test Loss: 0.1585 Acc: 96.0231\n",
      "\n",
      "Epoch 172/1199\n",
      "------------------------\n",
      "train Loss: 0.1042 Acc: 97.0919\n",
      "test Loss: 0.1673 Acc: 95.7514\n",
      "\n",
      "Epoch 173/1199\n",
      "------------------------\n",
      "train Loss: 0.1157 Acc: 96.7863\n",
      "test Loss: 0.1680 Acc: 95.7621\n",
      "\n",
      "Epoch 174/1199\n",
      "------------------------\n",
      "train Loss: 0.1091 Acc: 96.9662\n",
      "test Loss: 0.1642 Acc: 95.8952\n",
      "\n",
      "Epoch 175/1199\n",
      "------------------------\n",
      "train Loss: 0.1129 Acc: 96.8303\n",
      "test Loss: 0.1694 Acc: 95.7307\n",
      "\n",
      "Epoch 176/1199\n",
      "------------------------\n",
      "train Loss: 0.1050 Acc: 97.0746\n",
      "test Loss: 0.1582 Acc: 96.1213\n",
      "\n",
      "Epoch 177/1199\n",
      "------------------------\n",
      "train Loss: 0.0999 Acc: 97.2207\n",
      "test Loss: 0.1526 Acc: 96.2864\n",
      "\n",
      "Epoch 178/1199\n",
      "------------------------\n",
      "train Loss: 0.1144 Acc: 96.8247\n",
      "test Loss: 0.1678 Acc: 95.8529\n",
      "\n",
      "Epoch 179/1199\n",
      "------------------------\n",
      "train Loss: 0.1084 Acc: 96.9910\n",
      "test Loss: 0.1611 Acc: 96.0049\n",
      "\n",
      "Epoch 180/1199\n",
      "------------------------\n",
      "train Loss: 0.1198 Acc: 96.7121\n",
      "test Loss: 0.1765 Acc: 95.5927\n",
      "\n",
      "Epoch 181/1199\n",
      "------------------------\n",
      "train Loss: 0.1195 Acc: 96.7151\n",
      "test Loss: 0.1636 Acc: 95.8460\n",
      "\n",
      "Epoch 182/1199\n",
      "------------------------\n",
      "train Loss: 0.1070 Acc: 97.0002\n",
      "test Loss: 0.1595 Acc: 95.9326\n",
      "\n",
      "Epoch 183/1199\n",
      "------------------------\n",
      "train Loss: 0.0976 Acc: 97.2779\n",
      "test Loss: 0.1557 Acc: 96.1146\n",
      "\n",
      "Epoch 184/1199\n",
      "------------------------\n",
      "train Loss: 0.1071 Acc: 97.0291\n",
      "test Loss: 0.1576 Acc: 96.1041\n",
      "\n",
      "Epoch 185/1199\n",
      "------------------------\n",
      "train Loss: 0.1096 Acc: 96.9660\n",
      "test Loss: 0.1899 Acc: 95.4143\n",
      "\n",
      "Epoch 186/1199\n",
      "------------------------\n",
      "train Loss: 0.1511 Acc: 95.9303\n",
      "test Loss: 0.1936 Acc: 95.2626\n",
      "\n",
      "Epoch 187/1199\n",
      "------------------------\n",
      "train Loss: 0.2647 Acc: 93.1336\n",
      "test Loss: 0.5047 Acc: 87.6866\n",
      "\n",
      "Epoch 188/1199\n",
      "------------------------\n",
      "train Loss: 0.6678 Acc: 83.3927\n",
      "test Loss: 0.5117 Acc: 86.1713\n",
      "\n",
      "Epoch 189/1199\n",
      "------------------------\n",
      "train Loss: 0.4311 Acc: 87.8946\n",
      "test Loss: 0.4215 Acc: 88.2211\n",
      "\n",
      "Epoch 190/1199\n",
      "------------------------\n",
      "train Loss: 0.3599 Acc: 89.6435\n",
      "test Loss: 0.3479 Acc: 90.1954\n",
      "\n",
      "Epoch 191/1199\n",
      "------------------------\n",
      "train Loss: 0.3162 Acc: 90.8573\n",
      "test Loss: 0.3307 Acc: 90.7285\n",
      "\n",
      "Epoch 192/1199\n",
      "------------------------\n",
      "train Loss: 0.3007 Acc: 91.3173\n",
      "test Loss: 0.3201 Acc: 90.9485\n",
      "\n",
      "Epoch 193/1199\n",
      "------------------------\n",
      "train Loss: 0.2719 Acc: 92.1209\n",
      "test Loss: 0.2979 Acc: 91.6632\n",
      "\n",
      "Epoch 194/1199\n",
      "------------------------\n",
      "train Loss: 0.2531 Acc: 92.6926\n",
      "test Loss: 0.2794 Acc: 92.1765\n",
      "\n",
      "Epoch 195/1199\n",
      "------------------------\n",
      "train Loss: 0.2516 Acc: 92.7540\n",
      "test Loss: 0.2758 Acc: 92.2496\n",
      "\n",
      "Epoch 196/1199\n",
      "------------------------\n",
      "train Loss: 0.2324 Acc: 93.2883\n",
      "test Loss: 0.2660 Acc: 92.6362\n",
      "\n",
      "Epoch 197/1199\n",
      "------------------------\n",
      "train Loss: 0.2292 Acc: 93.3842\n",
      "test Loss: 0.2581 Acc: 92.8870\n",
      "\n",
      "Epoch 198/1199\n",
      "------------------------\n",
      "train Loss: 0.2132 Acc: 93.8565\n",
      "test Loss: 0.2458 Acc: 93.1015\n",
      "\n",
      "Epoch 199/1199\n",
      "------------------------\n",
      "train Loss: 0.2062 Acc: 94.0560\n",
      "test Loss: 0.2424 Acc: 93.2616\n",
      "\n",
      "Epoch 200/1199\n",
      "------------------------\n",
      "train Loss: 0.2000 Acc: 94.2753\n",
      "test Loss: 0.2560 Acc: 93.0611\n",
      "\n",
      "Epoch 201/1199\n",
      "------------------------\n",
      "train Loss: 0.2012 Acc: 94.2208\n",
      "test Loss: 0.2338 Acc: 93.5418\n",
      "\n",
      "Epoch 202/1199\n",
      "------------------------\n",
      "train Loss: 0.2229 Acc: 93.5326\n",
      "test Loss: 0.2455 Acc: 93.2354\n",
      "\n",
      "Epoch 203/1199\n",
      "------------------------\n",
      "train Loss: 0.1979 Acc: 94.3245\n",
      "test Loss: 0.2414 Acc: 93.3791\n",
      "\n",
      "Epoch 204/1199\n",
      "------------------------\n",
      "train Loss: 0.1983 Acc: 94.3503\n",
      "test Loss: 0.2323 Acc: 93.6367\n",
      "\n",
      "Epoch 205/1199\n",
      "------------------------\n",
      "train Loss: 0.2375 Acc: 93.1597\n",
      "test Loss: 0.3830 Acc: 89.8240\n",
      "\n",
      "Epoch 206/1199\n",
      "------------------------\n",
      "train Loss: 0.3726 Acc: 89.3076\n",
      "test Loss: 0.3287 Acc: 90.7618\n",
      "\n",
      "Epoch 207/1199\n",
      "------------------------\n",
      "train Loss: 0.2709 Acc: 92.1700\n",
      "test Loss: 0.2837 Acc: 92.0673\n",
      "\n",
      "Epoch 208/1199\n",
      "------------------------\n",
      "train Loss: 0.2549 Acc: 92.6684\n",
      "test Loss: 0.2786 Acc: 92.2387\n",
      "\n",
      "Epoch 209/1199\n",
      "------------------------\n",
      "train Loss: 0.2350 Acc: 93.2655\n",
      "test Loss: 0.2545 Acc: 93.0254\n",
      "\n",
      "Epoch 210/1199\n",
      "------------------------\n",
      "train Loss: 0.2186 Acc: 93.7496\n",
      "test Loss: 0.2504 Acc: 93.1338\n",
      "\n",
      "Epoch 211/1199\n",
      "------------------------\n",
      "train Loss: 0.2167 Acc: 93.8351\n",
      "test Loss: 0.2434 Acc: 93.3076\n",
      "\n",
      "Epoch 212/1199\n",
      "------------------------\n",
      "train Loss: 0.2120 Acc: 93.9723\n",
      "test Loss: 0.2415 Acc: 93.4621\n",
      "\n",
      "Epoch 213/1199\n",
      "------------------------\n",
      "train Loss: 0.2021 Acc: 94.2540\n",
      "test Loss: 0.2311 Acc: 93.7271\n",
      "\n",
      "Epoch 214/1199\n",
      "------------------------\n",
      "train Loss: 0.1884 Acc: 94.6633\n",
      "test Loss: 0.2261 Acc: 93.8651\n",
      "\n",
      "Epoch 215/1199\n",
      "------------------------\n",
      "train Loss: 0.1924 Acc: 94.5591\n",
      "test Loss: 0.2389 Acc: 93.5121\n",
      "\n",
      "Epoch 216/1199\n",
      "------------------------\n",
      "train Loss: 0.1887 Acc: 94.6263\n",
      "test Loss: 0.2263 Acc: 93.8794\n",
      "\n",
      "Epoch 217/1199\n",
      "------------------------\n",
      "train Loss: 0.1874 Acc: 94.6523\n",
      "test Loss: 0.2261 Acc: 93.8765\n",
      "\n",
      "Epoch 218/1199\n",
      "------------------------\n",
      "train Loss: 0.1845 Acc: 94.7865\n",
      "test Loss: 0.2322 Acc: 93.6972\n",
      "\n",
      "Epoch 219/1199\n",
      "------------------------\n",
      "train Loss: 0.1870 Acc: 94.7074\n",
      "test Loss: 0.2205 Acc: 94.0034\n",
      "\n",
      "Epoch 220/1199\n",
      "------------------------\n",
      "train Loss: 0.1728 Acc: 95.0981\n",
      "test Loss: 0.2176 Acc: 94.1333\n",
      "\n",
      "Epoch 221/1199\n",
      "------------------------\n",
      "train Loss: 0.1674 Acc: 95.2907\n",
      "test Loss: 0.2031 Acc: 94.5556\n",
      "\n",
      "Epoch 222/1199\n",
      "------------------------\n",
      "train Loss: 0.1682 Acc: 95.2582\n",
      "test Loss: 0.2135 Acc: 94.3481\n",
      "\n",
      "Epoch 223/1199\n",
      "------------------------\n",
      "train Loss: 0.1679 Acc: 95.2753\n",
      "test Loss: 0.2112 Acc: 94.3359\n",
      "\n",
      "Epoch 224/1199\n",
      "------------------------\n",
      "train Loss: 0.1624 Acc: 95.4246\n",
      "test Loss: 0.2103 Acc: 94.4433\n",
      "\n",
      "Epoch 225/1199\n",
      "------------------------\n",
      "train Loss: 0.1576 Acc: 95.5448\n",
      "test Loss: 0.2024 Acc: 94.5982\n",
      "\n",
      "Epoch 226/1199\n",
      "------------------------\n",
      "train Loss: 0.1574 Acc: 95.5658\n",
      "test Loss: 0.1961 Acc: 94.7858\n",
      "\n",
      "Epoch 227/1199\n",
      "------------------------\n",
      "train Loss: 0.1505 Acc: 95.7467\n",
      "test Loss: 0.1980 Acc: 94.7110\n",
      "\n",
      "Epoch 228/1199\n",
      "------------------------\n",
      "train Loss: 0.1553 Acc: 95.6183\n",
      "test Loss: 0.2223 Acc: 93.9827\n",
      "\n",
      "Epoch 229/1199\n",
      "------------------------\n",
      "train Loss: 0.1673 Acc: 95.2626\n",
      "test Loss: 0.1986 Acc: 94.7519\n",
      "\n",
      "Epoch 230/1199\n",
      "------------------------\n",
      "train Loss: 0.1593 Acc: 95.5236\n",
      "test Loss: 0.1992 Acc: 94.7108\n",
      "\n",
      "Epoch 231/1199\n",
      "------------------------\n",
      "train Loss: 0.1520 Acc: 95.6932\n",
      "test Loss: 0.1941 Acc: 94.8295\n",
      "\n",
      "Epoch 232/1199\n",
      "------------------------\n",
      "train Loss: 0.1494 Acc: 95.7939\n",
      "test Loss: 0.1949 Acc: 94.8321\n",
      "\n",
      "Epoch 233/1199\n",
      "------------------------\n",
      "train Loss: 0.1515 Acc: 95.7636\n",
      "test Loss: 0.1942 Acc: 94.8889\n",
      "\n",
      "Epoch 234/1199\n",
      "------------------------\n",
      "train Loss: 0.1486 Acc: 95.8351\n",
      "test Loss: 0.1910 Acc: 95.0122\n",
      "\n",
      "Epoch 235/1199\n",
      "------------------------\n",
      "train Loss: 0.1498 Acc: 95.8151\n",
      "test Loss: 0.1929 Acc: 94.9992\n",
      "\n",
      "Epoch 236/1199\n",
      "------------------------\n",
      "train Loss: 0.1485 Acc: 95.8361\n",
      "test Loss: 0.1961 Acc: 94.8203\n",
      "\n",
      "Epoch 237/1199\n",
      "------------------------\n",
      "train Loss: 0.1462 Acc: 95.9222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1851 Acc: 95.1455\n",
      "\n",
      "Epoch 238/1199\n",
      "------------------------\n",
      "train Loss: 0.1414 Acc: 96.0250\n",
      "test Loss: 0.1858 Acc: 95.0989\n",
      "\n",
      "Epoch 239/1199\n",
      "------------------------\n",
      "train Loss: 0.1402 Acc: 96.0813\n",
      "test Loss: 0.1875 Acc: 95.1329\n",
      "\n",
      "Epoch 240/1199\n",
      "------------------------\n",
      "train Loss: 0.1431 Acc: 96.0211\n",
      "test Loss: 0.1845 Acc: 95.1744\n",
      "\n",
      "Epoch 241/1199\n",
      "------------------------\n",
      "train Loss: 0.1450 Acc: 95.9612\n",
      "test Loss: 0.2056 Acc: 94.6383\n",
      "\n",
      "Epoch 242/1199\n",
      "------------------------\n",
      "train Loss: 0.1529 Acc: 95.7639\n",
      "test Loss: 0.1877 Acc: 95.1473\n",
      "\n",
      "Epoch 243/1199\n",
      "------------------------\n",
      "train Loss: 0.1487 Acc: 95.8623\n",
      "test Loss: 0.1869 Acc: 95.1700\n",
      "\n",
      "Epoch 244/1199\n",
      "------------------------\n",
      "train Loss: 0.1399 Acc: 96.0857\n",
      "test Loss: 0.1965 Acc: 94.8820\n",
      "\n",
      "Epoch 245/1199\n",
      "------------------------\n",
      "train Loss: 0.1345 Acc: 96.2055\n",
      "test Loss: 0.1909 Acc: 95.0692\n",
      "\n",
      "Epoch 246/1199\n",
      "------------------------\n",
      "train Loss: 0.1377 Acc: 96.1830\n",
      "test Loss: 0.1923 Acc: 95.0498\n",
      "\n",
      "Epoch 247/1199\n",
      "------------------------\n",
      "train Loss: 0.1352 Acc: 96.2406\n",
      "test Loss: 0.1889 Acc: 95.0583\n",
      "\n",
      "Epoch 248/1199\n",
      "------------------------\n",
      "train Loss: 0.1361 Acc: 96.2072\n",
      "test Loss: 0.1860 Acc: 95.1903\n",
      "\n",
      "Epoch 249/1199\n",
      "------------------------\n",
      "train Loss: 0.1423 Acc: 96.0874\n",
      "test Loss: 0.1971 Acc: 94.9013\n",
      "\n",
      "Epoch 250/1199\n",
      "------------------------\n",
      "train Loss: 0.1823 Acc: 94.9982\n",
      "test Loss: 0.2371 Acc: 93.7492\n",
      "\n",
      "Epoch 251/1199\n",
      "------------------------\n",
      "train Loss: 0.1761 Acc: 95.1177\n",
      "test Loss: 0.2060 Acc: 94.5849\n",
      "\n",
      "Epoch 252/1199\n",
      "------------------------\n",
      "train Loss: 0.1671 Acc: 95.3709\n",
      "test Loss: 0.2060 Acc: 94.5841\n",
      "\n",
      "Epoch 253/1199\n",
      "------------------------\n",
      "train Loss: 0.1606 Acc: 95.5142\n",
      "test Loss: 0.2026 Acc: 94.7535\n",
      "\n",
      "Epoch 254/1199\n",
      "------------------------\n",
      "train Loss: 0.1524 Acc: 95.7695\n",
      "test Loss: 0.1921 Acc: 94.9618\n",
      "\n",
      "Epoch 255/1199\n",
      "------------------------\n",
      "train Loss: 0.1676 Acc: 95.2843\n",
      "test Loss: 0.2080 Acc: 94.5913\n",
      "\n",
      "Epoch 256/1199\n",
      "------------------------\n",
      "train Loss: 0.1576 Acc: 95.5928\n",
      "test Loss: 0.1984 Acc: 94.7930\n",
      "\n",
      "Epoch 257/1199\n",
      "------------------------\n",
      "train Loss: 0.1679 Acc: 95.3727\n",
      "test Loss: 0.2173 Acc: 94.3873\n",
      "\n",
      "Epoch 258/1199\n",
      "------------------------\n",
      "train Loss: 0.1730 Acc: 95.1449\n",
      "test Loss: 0.2022 Acc: 94.7298\n",
      "\n",
      "Epoch 259/1199\n",
      "------------------------\n",
      "train Loss: 0.1577 Acc: 95.5723\n",
      "test Loss: 0.2008 Acc: 94.7770\n",
      "\n",
      "Epoch 260/1199\n",
      "------------------------\n",
      "train Loss: 0.1509 Acc: 95.7447\n",
      "test Loss: 0.1843 Acc: 95.1573\n",
      "\n",
      "Epoch 261/1199\n",
      "------------------------\n",
      "train Loss: 0.1473 Acc: 95.8813\n",
      "test Loss: 0.2082 Acc: 94.5545\n",
      "\n",
      "Epoch 262/1199\n",
      "------------------------\n",
      "train Loss: 0.1683 Acc: 95.3713\n",
      "test Loss: 0.2030 Acc: 94.8243\n",
      "\n",
      "Epoch 263/1199\n",
      "------------------------\n",
      "train Loss: 0.1479 Acc: 95.9033\n",
      "test Loss: 0.1931 Acc: 95.0167\n",
      "\n",
      "Epoch 264/1199\n",
      "------------------------\n",
      "train Loss: 0.1582 Acc: 95.6064\n",
      "test Loss: 0.2036 Acc: 94.6480\n",
      "\n",
      "Epoch 265/1199\n",
      "------------------------\n",
      "train Loss: 0.1486 Acc: 95.8703\n",
      "test Loss: 0.1917 Acc: 95.0189\n",
      "\n",
      "Epoch 266/1199\n",
      "------------------------\n",
      "train Loss: 0.1472 Acc: 95.9176\n",
      "test Loss: 0.2081 Acc: 94.5941\n",
      "\n",
      "Epoch 267/1199\n",
      "------------------------\n",
      "train Loss: 0.1601 Acc: 95.5834\n",
      "test Loss: 0.2082 Acc: 94.7234\n",
      "\n",
      "Epoch 268/1199\n",
      "------------------------\n",
      "train Loss: 0.1572 Acc: 95.6171\n",
      "test Loss: 0.1992 Acc: 94.8062\n",
      "\n",
      "Epoch 269/1199\n",
      "------------------------\n",
      "train Loss: 0.1495 Acc: 95.8168\n",
      "test Loss: 0.1919 Acc: 94.8932\n",
      "\n",
      "Epoch 270/1199\n",
      "------------------------\n",
      "train Loss: 0.1442 Acc: 95.9645\n",
      "test Loss: 0.1950 Acc: 94.9773\n",
      "\n",
      "Epoch 271/1199\n",
      "------------------------\n",
      "train Loss: 0.1409 Acc: 96.0519\n",
      "test Loss: 0.1907 Acc: 95.0833\n",
      "\n",
      "Epoch 272/1199\n",
      "------------------------\n",
      "train Loss: 0.1334 Acc: 96.2663\n",
      "test Loss: 0.1771 Acc: 95.4327\n",
      "\n",
      "Epoch 273/1199\n",
      "------------------------\n",
      "train Loss: 0.1302 Acc: 96.3650\n",
      "test Loss: 0.1808 Acc: 95.2512\n",
      "\n",
      "Epoch 274/1199\n",
      "------------------------\n",
      "train Loss: 0.1282 Acc: 96.4180\n",
      "test Loss: 0.1832 Acc: 95.2312\n",
      "\n",
      "Epoch 275/1199\n",
      "------------------------\n",
      "train Loss: 0.1332 Acc: 96.2914\n",
      "test Loss: 0.1844 Acc: 95.2516\n",
      "\n",
      "Epoch 276/1199\n",
      "------------------------\n",
      "train Loss: 0.1227 Acc: 96.5765\n",
      "test Loss: 0.1809 Acc: 95.3305\n",
      "\n",
      "Epoch 277/1199\n",
      "------------------------\n",
      "train Loss: 0.1214 Acc: 96.6205\n",
      "test Loss: 0.1719 Acc: 95.5754\n",
      "\n",
      "Epoch 278/1199\n",
      "------------------------\n",
      "train Loss: 0.1220 Acc: 96.5880\n",
      "test Loss: 0.1751 Acc: 95.5714\n",
      "\n",
      "Epoch 279/1199\n",
      "------------------------\n",
      "train Loss: 0.1252 Acc: 96.5009\n",
      "test Loss: 0.1759 Acc: 95.5232\n",
      "\n",
      "Epoch 280/1199\n",
      "------------------------\n",
      "train Loss: 0.1196 Acc: 96.6432\n",
      "test Loss: 0.1827 Acc: 95.4390\n",
      "\n",
      "Epoch 281/1199\n",
      "------------------------\n",
      "train Loss: 0.1444 Acc: 95.9984\n",
      "test Loss: 0.2020 Acc: 94.7844\n",
      "\n",
      "Epoch 282/1199\n",
      "------------------------\n",
      "train Loss: 0.1402 Acc: 96.1159\n",
      "test Loss: 0.2058 Acc: 94.8017\n",
      "\n",
      "Epoch 283/1199\n",
      "------------------------\n",
      "train Loss: 0.2294 Acc: 93.6516\n",
      "test Loss: 0.2375 Acc: 93.7412\n",
      "\n",
      "Epoch 284/1199\n",
      "------------------------\n",
      "train Loss: 0.1880 Acc: 94.7439\n",
      "test Loss: 0.2241 Acc: 94.1621\n",
      "\n",
      "Epoch 285/1199\n",
      "------------------------\n",
      "train Loss: 0.1721 Acc: 95.1833\n",
      "test Loss: 0.2009 Acc: 94.7557\n",
      "\n",
      "Epoch 286/1199\n",
      "------------------------\n",
      "train Loss: 0.1591 Acc: 95.5741\n",
      "test Loss: 0.1960 Acc: 94.8941\n",
      "\n",
      "Epoch 287/1199\n",
      "------------------------\n",
      "train Loss: 0.1513 Acc: 95.7482\n",
      "test Loss: 0.1922 Acc: 95.0274\n",
      "\n",
      "Epoch 288/1199\n",
      "------------------------\n",
      "train Loss: 0.1621 Acc: 95.5521\n",
      "test Loss: 0.2200 Acc: 94.4533\n",
      "\n",
      "Epoch 289/1199\n",
      "------------------------\n",
      "train Loss: 0.1670 Acc: 95.3817\n",
      "test Loss: 0.1956 Acc: 94.8362\n",
      "\n",
      "Epoch 290/1199\n",
      "------------------------\n",
      "train Loss: 0.1493 Acc: 95.8081\n",
      "test Loss: 0.1839 Acc: 95.1909\n",
      "\n",
      "Epoch 291/1199\n",
      "------------------------\n",
      "train Loss: 0.1417 Acc: 96.0198\n",
      "test Loss: 0.1913 Acc: 94.9690\n",
      "\n",
      "Epoch 292/1199\n",
      "------------------------\n",
      "train Loss: 0.1390 Acc: 96.1024\n",
      "test Loss: 0.1772 Acc: 95.3911\n",
      "\n",
      "Epoch 293/1199\n",
      "------------------------\n",
      "train Loss: 0.1363 Acc: 96.2025\n",
      "test Loss: 0.1990 Acc: 94.8065\n",
      "\n",
      "Epoch 294/1199\n",
      "------------------------\n",
      "train Loss: 0.1479 Acc: 95.8836\n",
      "test Loss: 0.1984 Acc: 94.8956\n",
      "\n",
      "Epoch 295/1199\n",
      "------------------------\n",
      "train Loss: 0.1382 Acc: 96.1053\n",
      "test Loss: 0.1795 Acc: 95.3156\n",
      "\n",
      "Epoch 296/1199\n",
      "------------------------\n",
      "train Loss: 0.1356 Acc: 96.2143\n",
      "test Loss: 0.1820 Acc: 95.2276\n",
      "\n",
      "Epoch 297/1199\n",
      "------------------------\n",
      "train Loss: 0.1341 Acc: 96.2395\n",
      "test Loss: 0.1867 Acc: 95.1792\n",
      "\n",
      "Epoch 298/1199\n",
      "------------------------\n",
      "train Loss: 0.1345 Acc: 96.2572\n",
      "test Loss: 0.2064 Acc: 94.7509\n",
      "\n",
      "Epoch 299/1199\n",
      "------------------------\n",
      "train Loss: 0.1372 Acc: 96.1688\n",
      "test Loss: 0.1787 Acc: 95.4445\n",
      "\n",
      "Epoch 300/1199\n",
      "------------------------\n",
      "train Loss: 0.1181 Acc: 96.7211\n",
      "test Loss: 0.1689 Acc: 95.7227\n",
      "\n",
      "Epoch 301/1199\n",
      "------------------------\n",
      "train Loss: 0.1067 Acc: 97.0423\n",
      "test Loss: 0.1640 Acc: 95.8029\n",
      "\n",
      "Epoch 302/1199\n",
      "------------------------\n",
      "train Loss: 0.1092 Acc: 96.9692\n",
      "test Loss: 0.1624 Acc: 95.9640\n",
      "\n",
      "Epoch 303/1199\n",
      "------------------------\n",
      "train Loss: 0.1059 Acc: 97.0681\n",
      "test Loss: 0.1556 Acc: 96.0483\n",
      "\n",
      "Epoch 304/1199\n",
      "------------------------\n",
      "train Loss: 0.1002 Acc: 97.2146\n",
      "test Loss: 0.1551 Acc: 96.0369\n",
      "\n",
      "Epoch 305/1199\n",
      "------------------------\n",
      "train Loss: 0.0980 Acc: 97.3008\n",
      "test Loss: 0.1545 Acc: 96.0460\n",
      "\n",
      "Epoch 306/1199\n",
      "------------------------\n",
      "train Loss: 0.1000 Acc: 97.2246\n",
      "test Loss: 0.1529 Acc: 96.1357\n",
      "\n",
      "Epoch 307/1199\n",
      "------------------------\n",
      "train Loss: 0.0966 Acc: 97.3190\n",
      "test Loss: 0.1528 Acc: 96.1816\n",
      "\n",
      "Epoch 308/1199\n",
      "------------------------\n",
      "train Loss: 0.0984 Acc: 97.2754\n",
      "test Loss: 0.1521 Acc: 96.1997\n",
      "\n",
      "Epoch 309/1199\n",
      "------------------------\n",
      "train Loss: 0.0957 Acc: 97.3721\n",
      "test Loss: 0.1505 Acc: 96.2334\n",
      "\n",
      "Epoch 310/1199\n",
      "------------------------\n",
      "train Loss: 0.0945 Acc: 97.3980\n",
      "test Loss: 0.1587 Acc: 96.0733\n",
      "\n",
      "Epoch 311/1199\n",
      "------------------------\n",
      "train Loss: 0.0989 Acc: 97.2778\n",
      "test Loss: 0.1455 Acc: 96.3553\n",
      "\n",
      "Epoch 312/1199\n",
      "------------------------\n",
      "train Loss: 0.0993 Acc: 97.2610\n",
      "test Loss: 0.1525 Acc: 96.1894\n",
      "\n",
      "Epoch 313/1199\n",
      "------------------------\n",
      "train Loss: 0.0946 Acc: 97.3867\n",
      "test Loss: 0.1526 Acc: 96.1607\n",
      "\n",
      "Epoch 314/1199\n",
      "------------------------\n",
      "train Loss: 0.0953 Acc: 97.3596\n",
      "test Loss: 0.1527 Acc: 96.2351\n",
      "\n",
      "Epoch 315/1199\n",
      "------------------------\n",
      "train Loss: 0.1126 Acc: 96.9299\n",
      "test Loss: 0.1890 Acc: 95.2094\n",
      "\n",
      "Epoch 316/1199\n",
      "------------------------\n",
      "train Loss: 0.1241 Acc: 96.5545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1657 Acc: 95.7168\n",
      "\n",
      "Epoch 317/1199\n",
      "------------------------\n",
      "train Loss: 0.1116 Acc: 96.8781\n",
      "test Loss: 0.1629 Acc: 95.8284\n",
      "\n",
      "Epoch 318/1199\n",
      "------------------------\n",
      "train Loss: 0.1080 Acc: 96.9874\n",
      "test Loss: 0.1673 Acc: 95.7464\n",
      "\n",
      "Epoch 319/1199\n",
      "------------------------\n",
      "train Loss: 0.1042 Acc: 97.1067\n",
      "test Loss: 0.1599 Acc: 95.9474\n",
      "\n",
      "Epoch 320/1199\n",
      "------------------------\n",
      "train Loss: 0.1016 Acc: 97.1911\n",
      "test Loss: 0.1605 Acc: 95.9573\n",
      "\n",
      "Epoch 321/1199\n",
      "------------------------\n",
      "train Loss: 0.0986 Acc: 97.2766\n",
      "test Loss: 0.1555 Acc: 96.0683\n",
      "\n",
      "Epoch 322/1199\n",
      "------------------------\n",
      "train Loss: 0.0970 Acc: 97.3197\n",
      "test Loss: 0.1540 Acc: 96.1491\n",
      "\n",
      "Epoch 323/1199\n",
      "------------------------\n",
      "train Loss: 0.0948 Acc: 97.3876\n",
      "test Loss: 0.1614 Acc: 96.0686\n",
      "\n",
      "Epoch 324/1199\n",
      "------------------------\n",
      "train Loss: 0.1050 Acc: 97.1126\n",
      "test Loss: 0.1587 Acc: 96.0809\n",
      "\n",
      "Epoch 325/1199\n",
      "------------------------\n",
      "train Loss: 0.0979 Acc: 97.2885\n",
      "test Loss: 0.1566 Acc: 96.0562\n",
      "\n",
      "Epoch 326/1199\n",
      "------------------------\n",
      "train Loss: 0.0957 Acc: 97.3533\n",
      "test Loss: 0.1519 Acc: 96.2239\n",
      "\n",
      "Epoch 327/1199\n",
      "------------------------\n",
      "train Loss: 0.0923 Acc: 97.4181\n",
      "test Loss: 0.1539 Acc: 96.1652\n",
      "\n",
      "Epoch 328/1199\n",
      "------------------------\n",
      "train Loss: 0.0907 Acc: 97.4651\n",
      "test Loss: 0.1527 Acc: 96.1464\n",
      "\n",
      "Epoch 329/1199\n",
      "------------------------\n",
      "train Loss: 0.0890 Acc: 97.5251\n",
      "test Loss: 0.1488 Acc: 96.2857\n",
      "\n",
      "Epoch 330/1199\n",
      "------------------------\n",
      "train Loss: 0.0913 Acc: 97.4593\n",
      "test Loss: 0.1499 Acc: 96.2968\n",
      "\n",
      "Epoch 331/1199\n",
      "------------------------\n",
      "train Loss: 0.0898 Acc: 97.5052\n",
      "test Loss: 0.1473 Acc: 96.4234\n",
      "\n",
      "Epoch 332/1199\n",
      "------------------------\n",
      "train Loss: 0.0913 Acc: 97.4871\n",
      "test Loss: 0.1495 Acc: 96.2426\n",
      "\n",
      "Epoch 333/1199\n",
      "------------------------\n",
      "train Loss: 0.0864 Acc: 97.6106\n",
      "test Loss: 0.1497 Acc: 96.3816\n",
      "\n",
      "Epoch 334/1199\n",
      "------------------------\n",
      "train Loss: 0.0876 Acc: 97.5623\n",
      "test Loss: 0.1534 Acc: 96.1840\n",
      "\n",
      "Epoch 335/1199\n",
      "------------------------\n",
      "train Loss: 0.0903 Acc: 97.5128\n",
      "test Loss: 0.1528 Acc: 96.3051\n",
      "\n",
      "Epoch 336/1199\n",
      "------------------------\n",
      "train Loss: 0.0861 Acc: 97.5956\n",
      "test Loss: 0.1486 Acc: 96.3585\n",
      "\n",
      "Epoch 337/1199\n",
      "------------------------\n",
      "train Loss: 0.0875 Acc: 97.5634\n",
      "test Loss: 0.1474 Acc: 96.4540\n",
      "\n",
      "Epoch 338/1199\n",
      "------------------------\n",
      "train Loss: 0.0865 Acc: 97.5882\n",
      "test Loss: 0.1521 Acc: 96.2873\n",
      "\n",
      "Epoch 339/1199\n",
      "------------------------\n",
      "train Loss: 0.0861 Acc: 97.6154\n",
      "test Loss: 0.1518 Acc: 96.3215\n",
      "\n",
      "Epoch 340/1199\n",
      "------------------------\n",
      "train Loss: 0.0872 Acc: 97.5792\n",
      "test Loss: 0.1526 Acc: 96.2512\n",
      "\n",
      "Epoch 341/1199\n",
      "------------------------\n",
      "train Loss: 0.0890 Acc: 97.5439\n",
      "test Loss: 0.1493 Acc: 96.3800\n",
      "\n",
      "Epoch 342/1199\n",
      "------------------------\n",
      "train Loss: 0.0872 Acc: 97.5548\n",
      "test Loss: 0.1491 Acc: 96.3985\n",
      "\n",
      "Epoch 343/1199\n",
      "------------------------\n",
      "train Loss: 0.0858 Acc: 97.6142\n",
      "test Loss: 0.1503 Acc: 96.3638\n",
      "\n",
      "Epoch 344/1199\n",
      "------------------------\n",
      "train Loss: 0.0874 Acc: 97.5778\n",
      "test Loss: 0.1596 Acc: 96.1533\n",
      "\n",
      "Epoch 345/1199\n",
      "------------------------\n",
      "train Loss: 0.0872 Acc: 97.5847\n",
      "test Loss: 0.1510 Acc: 96.3173\n",
      "\n",
      "Epoch 346/1199\n",
      "------------------------\n",
      "train Loss: 0.0859 Acc: 97.6105\n",
      "test Loss: 0.1605 Acc: 96.0305\n",
      "\n",
      "Epoch 347/1199\n",
      "------------------------\n",
      "train Loss: 0.0892 Acc: 97.5323\n",
      "test Loss: 0.1494 Acc: 96.4103\n",
      "\n",
      "Epoch 348/1199\n",
      "------------------------\n",
      "train Loss: 0.0869 Acc: 97.6012\n",
      "test Loss: 0.1519 Acc: 96.3263\n",
      "\n",
      "Epoch 349/1199\n",
      "------------------------\n",
      "train Loss: 0.0845 Acc: 97.6517\n",
      "test Loss: 0.1597 Acc: 96.1510\n",
      "\n",
      "Epoch 350/1199\n",
      "------------------------\n",
      "train Loss: 0.0818 Acc: 97.7153\n",
      "test Loss: 0.1460 Acc: 96.4983\n",
      "\n",
      "Epoch 351/1199\n",
      "------------------------\n",
      "train Loss: 0.0818 Acc: 97.7220\n",
      "test Loss: 0.1455 Acc: 96.4980\n",
      "\n",
      "Epoch 352/1199\n",
      "------------------------\n",
      "train Loss: 0.0795 Acc: 97.7881\n",
      "test Loss: 0.1504 Acc: 96.3355\n",
      "\n",
      "Epoch 353/1199\n",
      "------------------------\n",
      "train Loss: 0.0798 Acc: 97.7825\n",
      "test Loss: 0.1478 Acc: 96.3885\n",
      "\n",
      "Epoch 354/1199\n",
      "------------------------\n",
      "train Loss: 0.0844 Acc: 97.6569\n",
      "test Loss: 0.1506 Acc: 96.3436\n",
      "\n",
      "Epoch 355/1199\n",
      "------------------------\n",
      "train Loss: 0.0842 Acc: 97.6581\n",
      "test Loss: 0.1471 Acc: 96.4526\n",
      "\n",
      "Epoch 356/1199\n",
      "------------------------\n",
      "train Loss: 0.0820 Acc: 97.7157\n",
      "test Loss: 0.1452 Acc: 96.5344\n",
      "\n",
      "Epoch 357/1199\n",
      "------------------------\n",
      "train Loss: 0.0821 Acc: 97.7074\n",
      "test Loss: 0.1466 Acc: 96.5289\n",
      "\n",
      "Epoch 358/1199\n",
      "------------------------\n",
      "train Loss: 0.0804 Acc: 97.7431\n",
      "test Loss: 0.1457 Acc: 96.4774\n",
      "\n",
      "Epoch 359/1199\n",
      "------------------------\n",
      "train Loss: 0.0863 Acc: 97.6087\n",
      "test Loss: 0.1532 Acc: 96.2916\n",
      "\n",
      "Epoch 360/1199\n",
      "------------------------\n",
      "train Loss: 0.0856 Acc: 97.6276\n",
      "test Loss: 0.1516 Acc: 96.3398\n",
      "\n",
      "Epoch 361/1199\n",
      "------------------------\n",
      "train Loss: 0.0828 Acc: 97.6975\n",
      "test Loss: 0.1496 Acc: 96.3880\n",
      "\n",
      "Epoch 362/1199\n",
      "------------------------\n",
      "train Loss: 0.0882 Acc: 97.5545\n",
      "test Loss: 0.1495 Acc: 96.4179\n",
      "\n",
      "Epoch 363/1199\n",
      "------------------------\n",
      "train Loss: 0.0847 Acc: 97.6278\n",
      "test Loss: 0.1504 Acc: 96.3787\n",
      "\n",
      "Epoch 364/1199\n",
      "------------------------\n",
      "train Loss: 0.0872 Acc: 97.5826\n",
      "test Loss: 0.1466 Acc: 96.5507\n",
      "\n",
      "Epoch 365/1199\n",
      "------------------------\n",
      "train Loss: 0.0820 Acc: 97.7378\n",
      "test Loss: 0.1504 Acc: 96.3807\n",
      "\n",
      "Epoch 366/1199\n",
      "------------------------\n",
      "train Loss: 0.0815 Acc: 97.7397\n",
      "test Loss: 0.1504 Acc: 96.4236\n",
      "\n",
      "Epoch 367/1199\n",
      "------------------------\n",
      "train Loss: 0.0819 Acc: 97.7162\n",
      "test Loss: 0.1582 Acc: 96.3115\n",
      "\n",
      "Epoch 368/1199\n",
      "------------------------\n",
      "train Loss: 0.1231 Acc: 96.8081\n",
      "test Loss: 0.1758 Acc: 95.7723\n",
      "\n",
      "Epoch 369/1199\n",
      "------------------------\n",
      "train Loss: 0.1082 Acc: 97.0499\n",
      "test Loss: 0.1667 Acc: 95.9104\n",
      "\n",
      "Epoch 370/1199\n",
      "------------------------\n",
      "train Loss: 0.1008 Acc: 97.2358\n",
      "test Loss: 0.1567 Acc: 96.2334\n",
      "\n",
      "Epoch 371/1199\n",
      "------------------------\n",
      "train Loss: 0.1126 Acc: 96.9789\n",
      "test Loss: 0.1637 Acc: 96.0838\n",
      "\n",
      "Epoch 372/1199\n",
      "------------------------\n",
      "train Loss: 0.1022 Acc: 97.2033\n",
      "test Loss: 0.1681 Acc: 95.9762\n",
      "\n",
      "Epoch 373/1199\n",
      "------------------------\n",
      "train Loss: 0.0971 Acc: 97.3224\n",
      "test Loss: 0.1561 Acc: 96.1973\n",
      "\n",
      "Epoch 374/1199\n",
      "------------------------\n",
      "train Loss: 0.0948 Acc: 97.3577\n",
      "test Loss: 0.1553 Acc: 96.2604\n",
      "\n",
      "Epoch 375/1199\n",
      "------------------------\n",
      "train Loss: 0.0955 Acc: 97.3587\n",
      "test Loss: 0.1584 Acc: 96.1536\n",
      "\n",
      "Epoch 376/1199\n",
      "------------------------\n",
      "train Loss: 0.0983 Acc: 97.2657\n",
      "test Loss: 0.1573 Acc: 96.1956\n",
      "\n",
      "Epoch 377/1199\n",
      "------------------------\n",
      "train Loss: 0.0932 Acc: 97.4185\n",
      "test Loss: 0.1593 Acc: 96.1547\n",
      "\n",
      "Epoch 378/1199\n",
      "------------------------\n",
      "train Loss: 0.0870 Acc: 97.5634\n",
      "test Loss: 0.1640 Acc: 96.0396\n",
      "\n",
      "Epoch 379/1199\n",
      "------------------------\n",
      "train Loss: 0.0867 Acc: 97.5853\n",
      "test Loss: 0.1495 Acc: 96.4084\n",
      "\n",
      "Epoch 380/1199\n",
      "------------------------\n",
      "train Loss: 0.0850 Acc: 97.6358\n",
      "test Loss: 0.1495 Acc: 96.3937\n",
      "\n",
      "Epoch 381/1199\n",
      "------------------------\n",
      "train Loss: 0.0822 Acc: 97.7366\n",
      "test Loss: 0.1543 Acc: 96.3275\n",
      "\n",
      "Epoch 382/1199\n",
      "------------------------\n",
      "train Loss: 0.0797 Acc: 97.7801\n",
      "test Loss: 0.1476 Acc: 96.5344\n",
      "\n",
      "Epoch 383/1199\n",
      "------------------------\n",
      "train Loss: 0.0793 Acc: 97.7981\n",
      "test Loss: 0.1495 Acc: 96.4547\n",
      "\n",
      "Epoch 384/1199\n",
      "------------------------\n",
      "train Loss: 0.0819 Acc: 97.7161\n",
      "test Loss: 0.1544 Acc: 96.2904\n",
      "\n",
      "Epoch 385/1199\n",
      "------------------------\n",
      "train Loss: 0.0815 Acc: 97.7398\n",
      "test Loss: 0.1560 Acc: 96.3037\n",
      "\n",
      "Epoch 386/1199\n",
      "------------------------\n",
      "train Loss: 0.0829 Acc: 97.6994\n",
      "test Loss: 0.1497 Acc: 96.4358\n",
      "\n",
      "Epoch 387/1199\n",
      "------------------------\n",
      "train Loss: 0.0826 Acc: 97.7106\n",
      "test Loss: 0.1529 Acc: 96.3939\n",
      "\n",
      "Epoch 388/1199\n",
      "------------------------\n",
      "train Loss: 0.0787 Acc: 97.8092\n",
      "test Loss: 0.1496 Acc: 96.4621\n",
      "\n",
      "Epoch 389/1199\n",
      "------------------------\n",
      "train Loss: 0.0754 Acc: 97.9052\n",
      "test Loss: 0.1523 Acc: 96.4757\n",
      "\n",
      "Epoch 390/1199\n",
      "------------------------\n",
      "train Loss: 0.0769 Acc: 97.8716\n",
      "test Loss: 0.1468 Acc: 96.5565\n",
      "\n",
      "Epoch 391/1199\n",
      "------------------------\n",
      "train Loss: 0.0780 Acc: 97.8127\n",
      "test Loss: 0.1471 Acc: 96.5776\n",
      "\n",
      "Epoch 392/1199\n",
      "------------------------\n",
      "train Loss: 0.0769 Acc: 97.8557\n",
      "test Loss: 0.1491 Acc: 96.4985\n",
      "\n",
      "Epoch 393/1199\n",
      "------------------------\n",
      "train Loss: 0.0783 Acc: 97.8379\n",
      "test Loss: 0.1539 Acc: 96.3472\n",
      "\n",
      "Epoch 394/1199\n",
      "------------------------\n",
      "train Loss: 0.0806 Acc: 97.7736\n",
      "test Loss: 0.1472 Acc: 96.5263\n",
      "\n",
      "Epoch 395/1199\n",
      "------------------------\n",
      "train Loss: 0.0788 Acc: 97.8366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1537 Acc: 96.3113\n",
      "\n",
      "Epoch 396/1199\n",
      "------------------------\n",
      "train Loss: 0.0763 Acc: 97.8894\n",
      "test Loss: 0.1612 Acc: 96.1429\n",
      "\n",
      "Epoch 397/1199\n",
      "------------------------\n",
      "train Loss: 0.0899 Acc: 97.5403\n",
      "test Loss: 0.1568 Acc: 96.3215\n",
      "\n",
      "Epoch 398/1199\n",
      "------------------------\n",
      "train Loss: 0.0808 Acc: 97.7415\n",
      "test Loss: 0.1497 Acc: 96.3970\n",
      "\n",
      "Epoch 399/1199\n",
      "------------------------\n",
      "train Loss: 0.0826 Acc: 97.6949\n",
      "test Loss: 0.1554 Acc: 96.3308\n",
      "\n",
      "Epoch 400/1199\n",
      "------------------------\n",
      "train Loss: 0.0824 Acc: 97.7260\n",
      "test Loss: 0.1703 Acc: 96.1035\n",
      "\n",
      "Epoch 401/1199\n",
      "------------------------\n",
      "train Loss: 0.0935 Acc: 97.4456\n",
      "test Loss: 0.1639 Acc: 96.0996\n",
      "\n",
      "Epoch 402/1199\n",
      "------------------------\n",
      "train Loss: 0.1013 Acc: 97.2539\n",
      "test Loss: 0.1607 Acc: 96.1545\n",
      "\n",
      "Epoch 403/1199\n",
      "------------------------\n",
      "train Loss: 0.0971 Acc: 97.3232\n",
      "test Loss: 0.1618 Acc: 96.0220\n",
      "\n",
      "Epoch 404/1199\n",
      "------------------------\n",
      "train Loss: 0.0894 Acc: 97.4869\n",
      "test Loss: 0.1546 Acc: 96.1961\n",
      "\n",
      "Epoch 405/1199\n",
      "------------------------\n",
      "train Loss: 0.1017 Acc: 97.2045\n",
      "test Loss: 0.1583 Acc: 96.1422\n",
      "\n",
      "Epoch 406/1199\n",
      "------------------------\n",
      "train Loss: 0.0955 Acc: 97.3393\n",
      "test Loss: 0.1650 Acc: 96.0094\n",
      "\n",
      "Epoch 407/1199\n",
      "------------------------\n",
      "train Loss: 0.1003 Acc: 97.2114\n",
      "test Loss: 0.1619 Acc: 96.0942\n",
      "\n",
      "Epoch 408/1199\n",
      "------------------------\n",
      "train Loss: 0.0932 Acc: 97.4011\n",
      "test Loss: 0.1518 Acc: 96.4053\n",
      "\n",
      "Epoch 409/1199\n",
      "------------------------\n",
      "train Loss: 0.0858 Acc: 97.6051\n",
      "test Loss: 0.1494 Acc: 96.4120\n",
      "\n",
      "Epoch 410/1199\n",
      "------------------------\n",
      "train Loss: 0.0830 Acc: 97.6858\n",
      "test Loss: 0.1493 Acc: 96.4118\n",
      "\n",
      "Epoch 411/1199\n",
      "------------------------\n",
      "train Loss: 0.0809 Acc: 97.7349\n",
      "test Loss: 0.1482 Acc: 96.5063\n",
      "\n",
      "Epoch 412/1199\n",
      "------------------------\n",
      "train Loss: 0.0804 Acc: 97.7298\n",
      "test Loss: 0.1503 Acc: 96.3702\n",
      "\n",
      "Epoch 413/1199\n",
      "------------------------\n",
      "train Loss: 0.0871 Acc: 97.5822\n",
      "test Loss: 0.1595 Acc: 96.1531\n",
      "\n",
      "Epoch 414/1199\n",
      "------------------------\n",
      "train Loss: 0.0841 Acc: 97.6600\n",
      "test Loss: 0.1477 Acc: 96.4319\n",
      "\n",
      "Epoch 415/1199\n",
      "------------------------\n",
      "train Loss: 0.0836 Acc: 97.6784\n",
      "test Loss: 0.1460 Acc: 96.5047\n",
      "\n",
      "Epoch 416/1199\n",
      "------------------------\n",
      "train Loss: 0.0813 Acc: 97.7389\n",
      "test Loss: 0.1472 Acc: 96.5296\n",
      "\n",
      "Epoch 417/1199\n",
      "------------------------\n",
      "train Loss: 0.0797 Acc: 97.7696\n",
      "test Loss: 0.1524 Acc: 96.3312\n",
      "\n",
      "Epoch 418/1199\n",
      "------------------------\n",
      "train Loss: 0.1098 Acc: 97.0180\n",
      "test Loss: 0.1727 Acc: 95.7647\n",
      "\n",
      "Epoch 419/1199\n",
      "------------------------\n",
      "train Loss: 0.0981 Acc: 97.2473\n",
      "test Loss: 0.1575 Acc: 96.1788\n",
      "\n",
      "Epoch 420/1199\n",
      "------------------------\n",
      "train Loss: 0.0895 Acc: 97.5142\n",
      "test Loss: 0.1623 Acc: 96.0486\n",
      "\n",
      "Epoch 421/1199\n",
      "------------------------\n",
      "train Loss: 0.0931 Acc: 97.3949\n",
      "test Loss: 0.1534 Acc: 96.2382\n",
      "\n",
      "Epoch 422/1199\n",
      "------------------------\n",
      "train Loss: 0.0858 Acc: 97.6007\n",
      "test Loss: 0.1520 Acc: 96.2968\n",
      "\n",
      "Epoch 423/1199\n",
      "------------------------\n",
      "train Loss: 0.0846 Acc: 97.6084\n",
      "test Loss: 0.1547 Acc: 96.2970\n",
      "\n",
      "Epoch 424/1199\n",
      "------------------------\n",
      "train Loss: 0.0818 Acc: 97.7005\n",
      "test Loss: 0.1509 Acc: 96.3754\n",
      "\n",
      "Epoch 425/1199\n",
      "------------------------\n",
      "train Loss: 0.0782 Acc: 97.8056\n",
      "test Loss: 0.1502 Acc: 96.4034\n",
      "\n",
      "Epoch 426/1199\n",
      "------------------------\n",
      "train Loss: 0.0775 Acc: 97.8119\n",
      "test Loss: 0.1489 Acc: 96.5426\n",
      "\n",
      "Epoch 427/1199\n",
      "------------------------\n",
      "train Loss: 0.0756 Acc: 97.8676\n",
      "test Loss: 0.1470 Acc: 96.5365\n",
      "\n",
      "Epoch 428/1199\n",
      "------------------------\n",
      "train Loss: 0.0790 Acc: 97.7833\n",
      "test Loss: 0.1503 Acc: 96.4595\n",
      "\n",
      "Epoch 429/1199\n",
      "------------------------\n",
      "train Loss: 0.0744 Acc: 97.9006\n",
      "test Loss: 0.1464 Acc: 96.4992\n",
      "\n",
      "Epoch 430/1199\n",
      "------------------------\n",
      "train Loss: 0.0757 Acc: 97.8880\n",
      "test Loss: 0.1477 Acc: 96.5521\n",
      "\n",
      "Epoch 431/1199\n",
      "------------------------\n",
      "train Loss: 0.0735 Acc: 97.9398\n",
      "test Loss: 0.1491 Acc: 96.5552\n",
      "\n",
      "Epoch 432/1199\n",
      "------------------------\n",
      "train Loss: 0.0740 Acc: 97.9464\n",
      "test Loss: 0.1413 Acc: 96.7016\n",
      "\n",
      "Epoch 433/1199\n",
      "------------------------\n",
      "train Loss: 0.0700 Acc: 98.0321\n",
      "test Loss: 0.1520 Acc: 96.4876\n",
      "\n",
      "Epoch 434/1199\n",
      "------------------------\n",
      "train Loss: 0.0794 Acc: 97.7866\n",
      "test Loss: 0.1645 Acc: 96.1094\n",
      "\n",
      "Epoch 435/1199\n",
      "------------------------\n",
      "train Loss: 0.0842 Acc: 97.6554\n",
      "test Loss: 0.1545 Acc: 96.3847\n",
      "\n",
      "Epoch 436/1199\n",
      "------------------------\n",
      "train Loss: 0.0805 Acc: 97.7510\n",
      "test Loss: 0.1516 Acc: 96.4483\n",
      "\n",
      "Epoch 437/1199\n",
      "------------------------\n",
      "train Loss: 0.0755 Acc: 97.8860\n",
      "test Loss: 0.1478 Acc: 96.5270\n",
      "\n",
      "Epoch 438/1199\n",
      "------------------------\n",
      "train Loss: 0.0743 Acc: 97.9117\n",
      "test Loss: 0.1526 Acc: 96.4267\n",
      "\n",
      "Epoch 439/1199\n",
      "------------------------\n",
      "train Loss: 0.0773 Acc: 97.8484\n",
      "test Loss: 0.1536 Acc: 96.4002\n",
      "\n",
      "Epoch 440/1199\n",
      "------------------------\n",
      "train Loss: 0.0742 Acc: 97.9259\n",
      "test Loss: 0.1473 Acc: 96.5389\n",
      "\n",
      "Epoch 441/1199\n",
      "------------------------\n",
      "train Loss: 0.0729 Acc: 97.9556\n",
      "test Loss: 0.1453 Acc: 96.6118\n",
      "\n",
      "Epoch 442/1199\n",
      "------------------------\n",
      "train Loss: 0.0716 Acc: 97.9942\n",
      "test Loss: 0.1462 Acc: 96.6167\n",
      "\n",
      "Epoch 443/1199\n",
      "------------------------\n",
      "train Loss: 0.0728 Acc: 97.9618\n",
      "test Loss: 0.1477 Acc: 96.5192\n",
      "\n",
      "Epoch 444/1199\n",
      "------------------------\n",
      "train Loss: 0.0735 Acc: 97.9552\n",
      "test Loss: 0.1481 Acc: 96.5148\n",
      "\n",
      "Epoch 445/1199\n",
      "------------------------\n",
      "train Loss: 0.0729 Acc: 97.9561\n",
      "test Loss: 0.1541 Acc: 96.3712\n",
      "\n",
      "Epoch 446/1199\n",
      "------------------------\n",
      "train Loss: 0.0750 Acc: 97.9081\n",
      "test Loss: 0.1500 Acc: 96.5394\n",
      "\n",
      "Epoch 447/1199\n",
      "------------------------\n",
      "train Loss: 0.0771 Acc: 97.8529\n",
      "test Loss: 0.1502 Acc: 96.5215\n",
      "\n",
      "Epoch 448/1199\n",
      "------------------------\n",
      "train Loss: 0.0748 Acc: 97.9079\n",
      "test Loss: 0.1506 Acc: 96.6066\n",
      "\n",
      "Epoch 449/1199\n",
      "------------------------\n",
      "train Loss: 0.0726 Acc: 97.9405\n",
      "test Loss: 0.1484 Acc: 96.5476\n",
      "\n",
      "Epoch 450/1199\n",
      "------------------------\n",
      "train Loss: 0.0733 Acc: 97.9563\n",
      "test Loss: 0.1536 Acc: 96.3553\n",
      "\n",
      "Epoch 451/1199\n",
      "------------------------\n",
      "train Loss: 0.0779 Acc: 97.8413\n",
      "test Loss: 0.1495 Acc: 96.5700\n",
      "\n",
      "Epoch 452/1199\n",
      "------------------------\n",
      "train Loss: 0.0784 Acc: 97.8132\n",
      "test Loss: 0.1497 Acc: 96.5515\n",
      "\n",
      "Epoch 453/1199\n",
      "------------------------\n",
      "train Loss: 0.0766 Acc: 97.8660\n",
      "test Loss: 0.1633 Acc: 96.2070\n",
      "\n",
      "Epoch 454/1199\n",
      "------------------------\n",
      "train Loss: 0.0896 Acc: 97.5363\n",
      "test Loss: 0.1502 Acc: 96.5184\n",
      "\n",
      "Epoch 455/1199\n",
      "------------------------\n",
      "train Loss: 0.0814 Acc: 97.7455\n",
      "test Loss: 0.1539 Acc: 96.4735\n",
      "\n",
      "Epoch 456/1199\n",
      "------------------------\n",
      "train Loss: 0.0819 Acc: 97.7395\n",
      "test Loss: 0.1564 Acc: 96.3517\n",
      "\n",
      "Epoch 457/1199\n",
      "------------------------\n",
      "train Loss: 0.0832 Acc: 97.6659\n",
      "test Loss: 0.1602 Acc: 96.3092\n",
      "\n",
      "Epoch 458/1199\n",
      "------------------------\n",
      "train Loss: 0.0840 Acc: 97.6318\n",
      "test Loss: 0.1591 Acc: 96.2681\n",
      "\n",
      "Epoch 459/1199\n",
      "------------------------\n",
      "train Loss: 0.0758 Acc: 97.8674\n",
      "test Loss: 0.1521 Acc: 96.4144\n",
      "\n",
      "Epoch 460/1199\n",
      "------------------------\n",
      "train Loss: 0.0792 Acc: 97.7630\n",
      "test Loss: 0.1583 Acc: 96.3510\n",
      "\n",
      "Epoch 461/1199\n",
      "------------------------\n",
      "train Loss: 0.0790 Acc: 97.7672\n",
      "test Loss: 0.1528 Acc: 96.4306\n",
      "\n",
      "Epoch 462/1199\n",
      "------------------------\n",
      "train Loss: 0.0759 Acc: 97.8496\n",
      "test Loss: 0.1526 Acc: 96.4647\n",
      "\n",
      "Epoch 463/1199\n",
      "------------------------\n",
      "train Loss: 0.0791 Acc: 97.7932\n",
      "test Loss: 0.1518 Acc: 96.4736\n",
      "\n",
      "Epoch 464/1199\n",
      "------------------------\n",
      "train Loss: 0.0788 Acc: 97.7958\n",
      "test Loss: 0.1624 Acc: 96.1521\n",
      "\n",
      "Epoch 465/1199\n",
      "------------------------\n",
      "train Loss: 0.0846 Acc: 97.6190\n",
      "test Loss: 0.1539 Acc: 96.3930\n",
      "\n",
      "Epoch 466/1199\n",
      "------------------------\n",
      "train Loss: 0.0822 Acc: 97.6965\n",
      "test Loss: 0.1525 Acc: 96.4837\n",
      "\n",
      "Epoch 467/1199\n",
      "------------------------\n",
      "train Loss: 0.0792 Acc: 97.7984\n",
      "test Loss: 0.1553 Acc: 96.4049\n",
      "\n",
      "Epoch 468/1199\n",
      "------------------------\n",
      "train Loss: 0.0805 Acc: 97.7376\n",
      "test Loss: 0.1543 Acc: 96.4035\n",
      "\n",
      "Epoch 469/1199\n",
      "------------------------\n",
      "train Loss: 0.0787 Acc: 97.7665\n",
      "test Loss: 0.1502 Acc: 96.5113\n",
      "\n",
      "Epoch 470/1199\n",
      "------------------------\n",
      "train Loss: 0.0771 Acc: 97.8304\n",
      "test Loss: 0.1518 Acc: 96.5196\n",
      "\n",
      "Epoch 471/1199\n",
      "------------------------\n",
      "train Loss: 0.0735 Acc: 97.9293\n",
      "test Loss: 0.1534 Acc: 96.3731\n",
      "\n",
      "Epoch 472/1199\n",
      "------------------------\n",
      "train Loss: 0.0735 Acc: 97.9421\n",
      "test Loss: 0.1605 Acc: 96.1978\n",
      "\n",
      "Epoch 473/1199\n",
      "------------------------\n",
      "train Loss: 0.0746 Acc: 97.8781\n",
      "test Loss: 0.1498 Acc: 96.6500\n",
      "\n",
      "Epoch 474/1199\n",
      "------------------------\n",
      "train Loss: 0.0717 Acc: 97.9952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1547 Acc: 96.4099\n",
      "\n",
      "Epoch 475/1199\n",
      "------------------------\n",
      "train Loss: 0.0737 Acc: 97.9437\n",
      "test Loss: 0.1580 Acc: 96.2830\n",
      "\n",
      "Epoch 476/1199\n",
      "------------------------\n",
      "train Loss: 0.0892 Acc: 97.4652\n",
      "test Loss: 0.1766 Acc: 96.0013\n",
      "\n",
      "Epoch 477/1199\n",
      "------------------------\n",
      "train Loss: 0.0960 Acc: 97.3552\n",
      "test Loss: 0.1629 Acc: 96.2053\n",
      "\n",
      "Epoch 478/1199\n",
      "------------------------\n",
      "train Loss: 0.0978 Acc: 97.3267\n",
      "test Loss: 0.1678 Acc: 96.0666\n",
      "\n",
      "Epoch 479/1199\n",
      "------------------------\n",
      "train Loss: 0.0949 Acc: 97.4219\n",
      "test Loss: 0.1713 Acc: 96.0647\n",
      "\n",
      "Epoch 480/1199\n",
      "------------------------\n",
      "train Loss: 0.0893 Acc: 97.5121\n",
      "test Loss: 0.1575 Acc: 96.3424\n",
      "\n",
      "Epoch 481/1199\n",
      "------------------------\n",
      "train Loss: 0.1027 Acc: 97.1422\n",
      "test Loss: 0.1635 Acc: 96.0844\n",
      "\n",
      "Epoch 482/1199\n",
      "------------------------\n",
      "train Loss: 0.0911 Acc: 97.4619\n",
      "test Loss: 0.1612 Acc: 96.1811\n",
      "\n",
      "Epoch 483/1199\n",
      "------------------------\n",
      "train Loss: 0.0818 Acc: 97.7032\n",
      "test Loss: 0.1579 Acc: 96.2690\n",
      "\n",
      "Epoch 484/1199\n",
      "------------------------\n",
      "train Loss: 0.0794 Acc: 97.7541\n",
      "test Loss: 0.1601 Acc: 96.1685\n",
      "\n",
      "Epoch 485/1199\n",
      "------------------------\n",
      "train Loss: 0.0813 Acc: 97.7185\n",
      "test Loss: 0.1549 Acc: 96.3680\n",
      "\n",
      "Epoch 486/1199\n",
      "------------------------\n",
      "train Loss: 0.0767 Acc: 97.8430\n",
      "test Loss: 0.1565 Acc: 96.3750\n",
      "\n",
      "Epoch 487/1199\n",
      "------------------------\n",
      "train Loss: 0.0752 Acc: 97.8765\n",
      "test Loss: 0.1552 Acc: 96.3110\n",
      "\n",
      "Epoch 488/1199\n",
      "------------------------\n",
      "train Loss: 0.0768 Acc: 97.8383\n",
      "test Loss: 0.1526 Acc: 96.5224\n",
      "\n",
      "Epoch 489/1199\n",
      "------------------------\n",
      "train Loss: 0.0775 Acc: 97.8312\n",
      "test Loss: 0.1536 Acc: 96.4515\n",
      "\n",
      "Epoch 490/1199\n",
      "------------------------\n",
      "train Loss: 0.0742 Acc: 97.9114\n",
      "test Loss: 0.1468 Acc: 96.7099\n",
      "\n",
      "Epoch 491/1199\n",
      "------------------------\n",
      "train Loss: 0.0697 Acc: 98.0384\n",
      "test Loss: 0.1542 Acc: 96.4331\n",
      "\n",
      "Epoch 492/1199\n",
      "------------------------\n",
      "train Loss: 0.0741 Acc: 97.9147\n",
      "test Loss: 0.1493 Acc: 96.6049\n",
      "\n",
      "Epoch 493/1199\n",
      "------------------------\n",
      "train Loss: 0.0737 Acc: 97.9365\n",
      "test Loss: 0.1461 Acc: 96.6546\n",
      "\n",
      "Epoch 494/1199\n",
      "------------------------\n",
      "train Loss: 0.0711 Acc: 98.0171\n",
      "test Loss: 0.1489 Acc: 96.6173\n",
      "\n",
      "Epoch 495/1199\n",
      "------------------------\n",
      "train Loss: 0.0681 Acc: 98.0991\n",
      "test Loss: 0.1475 Acc: 96.6830\n",
      "\n",
      "Epoch 496/1199\n",
      "------------------------\n",
      "train Loss: 0.0704 Acc: 98.0377\n",
      "test Loss: 0.1521 Acc: 96.4850\n",
      "\n",
      "Epoch 497/1199\n",
      "------------------------\n",
      "train Loss: 0.0701 Acc: 98.0583\n",
      "test Loss: 0.1582 Acc: 96.4933\n",
      "\n",
      "Epoch 498/1199\n",
      "------------------------\n",
      "train Loss: 0.0771 Acc: 97.8551\n",
      "test Loss: 0.1463 Acc: 96.5709\n",
      "\n",
      "Epoch 499/1199\n",
      "------------------------\n",
      "train Loss: 0.0718 Acc: 97.9889\n",
      "test Loss: 0.1457 Acc: 96.6374\n",
      "\n",
      "Epoch 500/1199\n",
      "------------------------\n",
      "train Loss: 0.0726 Acc: 97.9572\n",
      "test Loss: 0.1447 Acc: 96.7358\n",
      "\n",
      "Epoch 501/1199\n",
      "------------------------\n",
      "train Loss: 0.0744 Acc: 97.9186\n",
      "test Loss: 0.1465 Acc: 96.6517\n",
      "\n",
      "Epoch 502/1199\n",
      "------------------------\n",
      "train Loss: 0.0733 Acc: 97.9335\n",
      "test Loss: 0.1562 Acc: 96.3831\n",
      "\n",
      "Epoch 503/1199\n",
      "------------------------\n",
      "train Loss: 0.0796 Acc: 97.7722\n",
      "test Loss: 0.1557 Acc: 96.3379\n",
      "\n",
      "Epoch 504/1199\n",
      "------------------------\n",
      "train Loss: 0.0757 Acc: 97.8877\n",
      "test Loss: 0.1635 Acc: 96.3433\n",
      "\n",
      "Epoch 505/1199\n",
      "------------------------\n",
      "train Loss: 0.0729 Acc: 97.9759\n",
      "test Loss: 0.1501 Acc: 96.6738\n",
      "\n",
      "Epoch 506/1199\n",
      "------------------------\n",
      "train Loss: 0.0695 Acc: 98.0456\n",
      "test Loss: 0.1592 Acc: 96.2883\n",
      "\n",
      "Epoch 507/1199\n",
      "------------------------\n",
      "train Loss: 0.0859 Acc: 97.6048\n",
      "test Loss: 0.1496 Acc: 96.5142\n",
      "\n",
      "Epoch 508/1199\n",
      "------------------------\n",
      "train Loss: 0.0743 Acc: 97.8924\n",
      "test Loss: 0.1541 Acc: 96.4351\n",
      "\n",
      "Epoch 509/1199\n",
      "------------------------\n",
      "train Loss: 0.0701 Acc: 98.0207\n",
      "test Loss: 0.1509 Acc: 96.5997\n",
      "\n",
      "Epoch 510/1199\n",
      "------------------------\n",
      "train Loss: 0.0709 Acc: 98.0007\n",
      "test Loss: 0.1557 Acc: 96.4353\n",
      "\n",
      "Epoch 511/1199\n",
      "------------------------\n",
      "train Loss: 0.0733 Acc: 97.9383\n",
      "test Loss: 0.1493 Acc: 96.5308\n",
      "\n",
      "Epoch 512/1199\n",
      "------------------------\n",
      "train Loss: 0.0682 Acc: 98.0782\n",
      "test Loss: 0.1483 Acc: 96.6037\n",
      "\n",
      "Epoch 513/1199\n",
      "------------------------\n",
      "train Loss: 0.0678 Acc: 98.1069\n",
      "test Loss: 0.1489 Acc: 96.6961\n",
      "\n",
      "Epoch 514/1199\n",
      "------------------------\n",
      "train Loss: 0.0661 Acc: 98.1436\n",
      "test Loss: 0.1506 Acc: 96.5821\n",
      "\n",
      "Epoch 515/1199\n",
      "------------------------\n",
      "train Loss: 0.0710 Acc: 98.0012\n",
      "test Loss: 0.1520 Acc: 96.5787\n",
      "\n",
      "Epoch 516/1199\n",
      "------------------------\n",
      "train Loss: 0.0706 Acc: 98.0152\n",
      "test Loss: 0.1496 Acc: 96.5970\n",
      "\n",
      "Epoch 517/1199\n",
      "------------------------\n",
      "train Loss: 0.0675 Acc: 98.1046\n",
      "test Loss: 0.1506 Acc: 96.6030\n",
      "\n",
      "Epoch 518/1199\n",
      "------------------------\n",
      "train Loss: 0.0681 Acc: 98.0981\n",
      "test Loss: 0.1499 Acc: 96.7144\n",
      "\n",
      "Epoch 519/1199\n",
      "------------------------\n",
      "train Loss: 0.0662 Acc: 98.1148\n",
      "test Loss: 0.1456 Acc: 96.7830\n",
      "\n",
      "Epoch 520/1199\n",
      "------------------------\n",
      "train Loss: 0.0641 Acc: 98.2106\n",
      "test Loss: 0.1454 Acc: 96.7802\n",
      "\n",
      "Epoch 521/1199\n",
      "------------------------\n",
      "train Loss: 0.0633 Acc: 98.2234\n",
      "test Loss: 0.1429 Acc: 96.8042\n",
      "\n",
      "Epoch 522/1199\n",
      "------------------------\n",
      "train Loss: 0.0611 Acc: 98.2865\n",
      "test Loss: 0.1443 Acc: 96.8296\n",
      "\n",
      "Epoch 523/1199\n",
      "------------------------\n",
      "train Loss: 0.1693 Acc: 96.1992\n",
      "test Loss: 0.3508 Acc: 92.4131\n",
      "\n",
      "Epoch 524/1199\n",
      "------------------------\n",
      "train Loss: 0.1844 Acc: 95.2086\n",
      "test Loss: 0.1942 Acc: 95.3783\n",
      "\n",
      "Epoch 525/1199\n",
      "------------------------\n",
      "train Loss: 0.1181 Acc: 96.7022\n",
      "test Loss: 0.1868 Acc: 95.6519\n",
      "\n",
      "Epoch 526/1199\n",
      "------------------------\n",
      "train Loss: 0.1007 Acc: 97.1601\n",
      "test Loss: 0.1635 Acc: 96.1811\n",
      "\n",
      "Epoch 527/1199\n",
      "------------------------\n",
      "train Loss: 0.0937 Acc: 97.3364\n",
      "test Loss: 0.1637 Acc: 96.1526\n",
      "\n",
      "Epoch 528/1199\n",
      "------------------------\n",
      "train Loss: 0.0881 Acc: 97.4859\n",
      "test Loss: 0.1561 Acc: 96.3681\n",
      "\n",
      "Epoch 529/1199\n",
      "------------------------\n",
      "train Loss: 0.0848 Acc: 97.5826\n",
      "test Loss: 0.1620 Acc: 96.2837\n",
      "\n",
      "Epoch 530/1199\n",
      "------------------------\n",
      "train Loss: 0.0792 Acc: 97.7447\n",
      "test Loss: 0.1633 Acc: 96.1840\n",
      "\n",
      "Epoch 531/1199\n",
      "------------------------\n",
      "train Loss: 0.0829 Acc: 97.6459\n",
      "test Loss: 0.1616 Acc: 96.2775\n",
      "\n",
      "Epoch 532/1199\n",
      "------------------------\n",
      "train Loss: 0.0807 Acc: 97.7281\n",
      "test Loss: 0.1570 Acc: 96.3408\n",
      "\n",
      "Epoch 533/1199\n",
      "------------------------\n",
      "train Loss: 0.0754 Acc: 97.8635\n",
      "test Loss: 0.1563 Acc: 96.4483\n",
      "\n",
      "Epoch 534/1199\n",
      "------------------------\n",
      "train Loss: 0.0738 Acc: 97.9057\n",
      "test Loss: 0.1602 Acc: 96.3082\n",
      "\n",
      "Epoch 535/1199\n",
      "------------------------\n",
      "train Loss: 0.0776 Acc: 97.8037\n",
      "test Loss: 0.1601 Acc: 96.2942\n",
      "\n",
      "Epoch 536/1199\n",
      "------------------------\n",
      "train Loss: 0.0796 Acc: 97.7596\n",
      "test Loss: 0.1547 Acc: 96.4464\n",
      "\n",
      "Epoch 537/1199\n",
      "------------------------\n",
      "train Loss: 0.0787 Acc: 97.7842\n",
      "test Loss: 0.1507 Acc: 96.5300\n",
      "\n",
      "Epoch 538/1199\n",
      "------------------------\n",
      "train Loss: 0.0780 Acc: 97.8197\n",
      "test Loss: 0.1560 Acc: 96.4249\n",
      "\n",
      "Epoch 539/1199\n",
      "------------------------\n",
      "train Loss: 0.0752 Acc: 97.8706\n",
      "test Loss: 0.1530 Acc: 96.5384\n",
      "\n",
      "Epoch 540/1199\n",
      "------------------------\n",
      "train Loss: 0.0805 Acc: 97.7623\n",
      "test Loss: 0.1572 Acc: 96.3341\n",
      "\n",
      "Epoch 541/1199\n",
      "------------------------\n",
      "train Loss: 0.0921 Acc: 97.4045\n",
      "test Loss: 0.1592 Acc: 96.1973\n",
      "\n",
      "Epoch 542/1199\n",
      "------------------------\n",
      "train Loss: 0.0808 Acc: 97.7088\n",
      "test Loss: 0.1553 Acc: 96.4515\n",
      "\n",
      "Epoch 543/1199\n",
      "------------------------\n",
      "train Loss: 0.0753 Acc: 97.8642\n",
      "test Loss: 0.1575 Acc: 96.3458\n",
      "\n",
      "Epoch 544/1199\n",
      "------------------------\n",
      "train Loss: 0.0737 Acc: 97.9141\n",
      "test Loss: 0.1524 Acc: 96.5163\n",
      "\n",
      "Epoch 545/1199\n",
      "------------------------\n",
      "train Loss: 0.0769 Acc: 97.8541\n",
      "test Loss: 0.1541 Acc: 96.4795\n",
      "\n",
      "Epoch 546/1199\n",
      "------------------------\n",
      "train Loss: 0.0742 Acc: 97.9036\n",
      "test Loss: 0.1506 Acc: 96.5524\n",
      "\n",
      "Epoch 547/1199\n",
      "------------------------\n",
      "train Loss: 0.0821 Acc: 97.7463\n",
      "test Loss: 0.1823 Acc: 95.8379\n",
      "\n",
      "Epoch 548/1199\n",
      "------------------------\n",
      "train Loss: 0.0883 Acc: 97.5713\n",
      "test Loss: 0.1599 Acc: 96.1878\n",
      "\n",
      "Epoch 549/1199\n",
      "------------------------\n",
      "train Loss: 0.0813 Acc: 97.7175\n",
      "test Loss: 0.1561 Acc: 96.4438\n",
      "\n",
      "Epoch 550/1199\n",
      "------------------------\n",
      "train Loss: 0.0785 Acc: 97.7887\n",
      "test Loss: 0.1543 Acc: 96.4628\n",
      "\n",
      "Epoch 551/1199\n",
      "------------------------\n",
      "train Loss: 0.0756 Acc: 97.8584\n",
      "test Loss: 0.1580 Acc: 96.3595\n",
      "\n",
      "Epoch 552/1199\n",
      "------------------------\n",
      "train Loss: 0.0755 Acc: 97.8506\n",
      "test Loss: 0.1524 Acc: 96.4698\n",
      "\n",
      "Epoch 553/1199\n",
      "------------------------\n",
      "train Loss: 0.0719 Acc: 97.9622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1538 Acc: 96.4809\n",
      "\n",
      "Epoch 554/1199\n",
      "------------------------\n",
      "train Loss: 0.0731 Acc: 97.9270\n",
      "test Loss: 0.1521 Acc: 96.5386\n",
      "\n",
      "Epoch 555/1199\n",
      "------------------------\n",
      "train Loss: 0.0761 Acc: 97.8582\n",
      "test Loss: 0.1495 Acc: 96.5350\n",
      "\n",
      "Epoch 556/1199\n",
      "------------------------\n",
      "train Loss: 0.0739 Acc: 97.9602\n",
      "test Loss: 0.1629 Acc: 96.3662\n",
      "\n",
      "Epoch 557/1199\n",
      "------------------------\n",
      "train Loss: 0.1032 Acc: 97.2984\n",
      "test Loss: 0.1765 Acc: 95.9702\n",
      "\n",
      "Epoch 558/1199\n",
      "------------------------\n",
      "train Loss: 0.1295 Acc: 96.7234\n",
      "test Loss: 0.1978 Acc: 95.5130\n",
      "\n",
      "Epoch 559/1199\n",
      "------------------------\n",
      "train Loss: 0.1165 Acc: 96.9194\n",
      "test Loss: 0.1706 Acc: 96.0816\n",
      "\n",
      "Epoch 560/1199\n",
      "------------------------\n",
      "train Loss: 0.0959 Acc: 97.3754\n",
      "test Loss: 0.1651 Acc: 96.2030\n",
      "\n",
      "Epoch 561/1199\n",
      "------------------------\n",
      "train Loss: 0.0903 Acc: 97.5276\n",
      "test Loss: 0.1650 Acc: 96.2165\n",
      "\n",
      "Epoch 562/1199\n",
      "------------------------\n",
      "train Loss: 0.1302 Acc: 96.5905\n",
      "test Loss: 0.1908 Acc: 95.5699\n",
      "\n",
      "Epoch 563/1199\n",
      "------------------------\n",
      "train Loss: 0.1068 Acc: 97.1149\n",
      "test Loss: 0.1787 Acc: 95.8320\n",
      "\n",
      "Epoch 564/1199\n",
      "------------------------\n",
      "train Loss: 0.1162 Acc: 96.8343\n",
      "test Loss: 0.1802 Acc: 95.6531\n",
      "\n",
      "Epoch 565/1199\n",
      "------------------------\n",
      "train Loss: 0.1016 Acc: 97.1814\n",
      "test Loss: 0.1656 Acc: 96.0882\n",
      "\n",
      "Epoch 566/1199\n",
      "------------------------\n",
      "train Loss: 0.0956 Acc: 97.3643\n",
      "test Loss: 0.1592 Acc: 96.3239\n",
      "\n",
      "Epoch 567/1199\n",
      "------------------------\n",
      "train Loss: 0.0898 Acc: 97.5009\n",
      "test Loss: 0.1604 Acc: 96.2400\n",
      "\n",
      "Epoch 568/1199\n",
      "------------------------\n",
      "train Loss: 0.0875 Acc: 97.5712\n",
      "test Loss: 0.1587 Acc: 96.2825\n",
      "\n",
      "Epoch 569/1199\n",
      "------------------------\n",
      "train Loss: 0.0979 Acc: 97.2886\n",
      "test Loss: 0.1676 Acc: 96.0061\n",
      "\n",
      "Epoch 570/1199\n",
      "------------------------\n",
      "train Loss: 0.0885 Acc: 97.5317\n",
      "test Loss: 0.1573 Acc: 96.2914\n",
      "\n",
      "Epoch 571/1199\n",
      "------------------------\n",
      "train Loss: 0.0848 Acc: 97.6283\n",
      "test Loss: 0.1515 Acc: 96.4004\n",
      "\n",
      "Epoch 572/1199\n",
      "------------------------\n",
      "train Loss: 0.0872 Acc: 97.5682\n",
      "test Loss: 0.1554 Acc: 96.3393\n",
      "\n",
      "Epoch 573/1199\n",
      "------------------------\n",
      "train Loss: 0.0801 Acc: 97.7595\n",
      "test Loss: 0.1567 Acc: 96.3217\n",
      "\n",
      "Epoch 574/1199\n",
      "------------------------\n",
      "train Loss: 0.0789 Acc: 97.7867\n",
      "test Loss: 0.1579 Acc: 96.2507\n",
      "\n",
      "Epoch 575/1199\n",
      "------------------------\n",
      "train Loss: 0.0791 Acc: 97.8009\n",
      "test Loss: 0.1580 Acc: 96.3438\n",
      "\n",
      "Epoch 576/1199\n",
      "------------------------\n",
      "train Loss: 0.0788 Acc: 97.8081\n",
      "test Loss: 0.1513 Acc: 96.5508\n",
      "\n",
      "Epoch 577/1199\n",
      "------------------------\n",
      "train Loss: 0.0764 Acc: 97.8654\n",
      "test Loss: 0.1543 Acc: 96.3705\n",
      "\n",
      "Epoch 578/1199\n",
      "------------------------\n",
      "train Loss: 0.0769 Acc: 97.8495\n",
      "test Loss: 0.1523 Acc: 96.4790\n",
      "\n",
      "Epoch 579/1199\n",
      "------------------------\n",
      "train Loss: 0.0750 Acc: 97.9116\n",
      "test Loss: 0.1502 Acc: 96.6085\n",
      "\n",
      "Epoch 580/1199\n",
      "------------------------\n",
      "train Loss: 0.0783 Acc: 97.8085\n",
      "test Loss: 0.1573 Acc: 96.3593\n",
      "\n",
      "Epoch 581/1199\n",
      "------------------------\n",
      "train Loss: 0.0767 Acc: 97.8537\n",
      "test Loss: 0.1572 Acc: 96.3707\n",
      "\n",
      "Epoch 582/1199\n",
      "------------------------\n",
      "train Loss: 0.0792 Acc: 97.8048\n",
      "test Loss: 0.1536 Acc: 96.4210\n",
      "\n",
      "Epoch 583/1199\n",
      "------------------------\n",
      "train Loss: 0.0756 Acc: 97.8685\n",
      "test Loss: 0.1650 Acc: 96.1388\n",
      "\n",
      "Epoch 584/1199\n",
      "------------------------\n",
      "train Loss: 0.0923 Acc: 97.4456\n",
      "test Loss: 0.1614 Acc: 96.3579\n",
      "\n",
      "Epoch 585/1199\n",
      "------------------------\n",
      "train Loss: 0.0780 Acc: 97.8138\n",
      "test Loss: 0.1570 Acc: 96.3816\n",
      "\n",
      "Epoch 586/1199\n",
      "------------------------\n",
      "train Loss: 0.0743 Acc: 97.9003\n",
      "test Loss: 0.1537 Acc: 96.5168\n",
      "\n",
      "Epoch 587/1199\n",
      "------------------------\n",
      "train Loss: 0.0743 Acc: 97.9136\n",
      "test Loss: 0.1506 Acc: 96.5141\n",
      "\n",
      "Epoch 588/1199\n",
      "------------------------\n",
      "train Loss: 0.0738 Acc: 97.9218\n",
      "test Loss: 0.1502 Acc: 96.5407\n",
      "\n",
      "Epoch 589/1199\n",
      "------------------------\n",
      "train Loss: 0.0703 Acc: 98.0337\n",
      "test Loss: 0.1557 Acc: 96.3890\n",
      "\n",
      "Epoch 590/1199\n",
      "------------------------\n",
      "train Loss: 0.0717 Acc: 97.9981\n",
      "test Loss: 0.1519 Acc: 96.5650\n",
      "\n",
      "Epoch 591/1199\n",
      "------------------------\n",
      "train Loss: 0.0683 Acc: 98.0855\n",
      "test Loss: 0.1511 Acc: 96.6540\n",
      "\n",
      "Epoch 592/1199\n",
      "------------------------\n",
      "train Loss: 0.0699 Acc: 98.0603\n",
      "test Loss: 0.1659 Acc: 96.3712\n",
      "\n",
      "Epoch 593/1199\n",
      "------------------------\n",
      "train Loss: 0.0846 Acc: 97.7144\n",
      "test Loss: 0.1619 Acc: 96.2730\n",
      "\n",
      "Epoch 594/1199\n",
      "------------------------\n",
      "train Loss: 0.1148 Acc: 96.9994\n",
      "test Loss: 0.1925 Acc: 95.5523\n",
      "\n",
      "Epoch 595/1199\n",
      "------------------------\n",
      "train Loss: 0.1162 Acc: 96.9186\n",
      "test Loss: 0.1839 Acc: 95.5595\n",
      "\n",
      "Epoch 596/1199\n",
      "------------------------\n",
      "train Loss: 0.1224 Acc: 96.7362\n",
      "test Loss: 0.1839 Acc: 95.5885\n",
      "\n",
      "Epoch 597/1199\n",
      "------------------------\n",
      "train Loss: 0.1069 Acc: 97.1084\n",
      "test Loss: 0.1730 Acc: 96.0345\n",
      "\n",
      "Epoch 598/1199\n",
      "------------------------\n",
      "train Loss: 0.1052 Acc: 97.1694\n",
      "test Loss: 0.1762 Acc: 95.8875\n",
      "\n",
      "Epoch 599/1199\n",
      "------------------------\n",
      "train Loss: 0.0974 Acc: 97.3285\n",
      "test Loss: 0.1605 Acc: 96.2047\n",
      "\n",
      "Epoch 600/1199\n",
      "------------------------\n",
      "train Loss: 0.0846 Acc: 97.6719\n",
      "test Loss: 0.1615 Acc: 96.3037\n",
      "\n",
      "Epoch 601/1199\n",
      "------------------------\n",
      "train Loss: 0.0825 Acc: 97.7239\n",
      "test Loss: 0.1575 Acc: 96.3246\n",
      "\n",
      "Epoch 602/1199\n",
      "------------------------\n",
      "train Loss: 0.0772 Acc: 97.8521\n",
      "test Loss: 0.1500 Acc: 96.4621\n",
      "\n",
      "Epoch 603/1199\n",
      "------------------------\n",
      "train Loss: 0.0756 Acc: 97.9023\n",
      "test Loss: 0.1482 Acc: 96.5901\n",
      "\n",
      "Epoch 604/1199\n",
      "------------------------\n",
      "train Loss: 0.0747 Acc: 97.9438\n",
      "test Loss: 0.1524 Acc: 96.4800\n",
      "\n",
      "Epoch 605/1199\n",
      "------------------------\n",
      "train Loss: 0.0775 Acc: 97.8591\n",
      "test Loss: 0.1502 Acc: 96.5306\n",
      "\n",
      "Epoch 606/1199\n",
      "------------------------\n",
      "train Loss: 0.0736 Acc: 97.9805\n",
      "test Loss: 0.1445 Acc: 96.5970\n",
      "\n",
      "Epoch 607/1199\n",
      "------------------------\n",
      "train Loss: 0.0731 Acc: 97.9695\n",
      "test Loss: 0.1486 Acc: 96.5327\n",
      "\n",
      "Epoch 608/1199\n",
      "------------------------\n",
      "train Loss: 0.0726 Acc: 98.0073\n",
      "test Loss: 0.1527 Acc: 96.5144\n",
      "\n",
      "Epoch 609/1199\n",
      "------------------------\n",
      "train Loss: 0.0712 Acc: 98.0456\n",
      "test Loss: 0.1497 Acc: 96.5460\n",
      "\n",
      "Epoch 610/1199\n",
      "------------------------\n",
      "train Loss: 0.0725 Acc: 97.9973\n",
      "test Loss: 0.1495 Acc: 96.5522\n",
      "\n",
      "Epoch 611/1199\n",
      "------------------------\n",
      "train Loss: 0.0698 Acc: 98.0762\n",
      "test Loss: 0.1499 Acc: 96.5842\n",
      "\n",
      "Epoch 612/1199\n",
      "------------------------\n",
      "train Loss: 0.0697 Acc: 98.0611\n",
      "test Loss: 0.1447 Acc: 96.6719\n",
      "\n",
      "Epoch 613/1199\n",
      "------------------------\n",
      "train Loss: 0.0703 Acc: 98.0661\n",
      "test Loss: 0.1470 Acc: 96.6334\n",
      "\n",
      "Epoch 614/1199\n",
      "------------------------\n",
      "train Loss: 0.0678 Acc: 98.1319\n",
      "test Loss: 0.1478 Acc: 96.5994\n",
      "\n",
      "Epoch 615/1199\n",
      "------------------------\n",
      "train Loss: 0.0693 Acc: 98.0819\n",
      "test Loss: 0.1484 Acc: 96.5332\n",
      "\n",
      "Epoch 616/1199\n",
      "------------------------\n",
      "train Loss: 0.0681 Acc: 98.1142\n",
      "test Loss: 0.1465 Acc: 96.6878\n",
      "\n",
      "Epoch 617/1199\n",
      "------------------------\n",
      "train Loss: 0.0683 Acc: 98.1212\n",
      "test Loss: 0.1474 Acc: 96.5688\n",
      "\n",
      "Epoch 618/1199\n",
      "------------------------\n",
      "train Loss: 0.0653 Acc: 98.1992\n",
      "test Loss: 0.1435 Acc: 96.6819\n",
      "\n",
      "Epoch 619/1199\n",
      "------------------------\n",
      "train Loss: 0.0687 Acc: 98.0825\n",
      "test Loss: 0.1469 Acc: 96.5904\n",
      "\n",
      "Epoch 620/1199\n",
      "------------------------\n",
      "train Loss: 0.0659 Acc: 98.1784\n",
      "test Loss: 0.1497 Acc: 96.6343\n",
      "\n",
      "Epoch 621/1199\n",
      "------------------------\n",
      "train Loss: 0.0661 Acc: 98.1753\n",
      "test Loss: 0.1459 Acc: 96.6002\n",
      "\n",
      "Epoch 622/1199\n",
      "------------------------\n",
      "train Loss: 0.0668 Acc: 98.1443\n",
      "test Loss: 0.1463 Acc: 96.6168\n",
      "\n",
      "Epoch 623/1199\n",
      "------------------------\n",
      "train Loss: 0.0678 Acc: 98.1458\n",
      "test Loss: 0.1534 Acc: 96.4626\n",
      "\n",
      "Epoch 624/1199\n",
      "------------------------\n",
      "train Loss: 0.0675 Acc: 98.1271\n",
      "test Loss: 0.1453 Acc: 96.6538\n",
      "\n",
      "Epoch 625/1199\n",
      "------------------------\n",
      "train Loss: 0.0649 Acc: 98.2321\n",
      "test Loss: 0.1462 Acc: 96.6861\n",
      "\n",
      "Epoch 626/1199\n",
      "------------------------\n",
      "train Loss: 0.0655 Acc: 98.1984\n",
      "test Loss: 0.1436 Acc: 96.7388\n",
      "\n",
      "Epoch 627/1199\n",
      "------------------------\n",
      "train Loss: 0.0637 Acc: 98.2481\n",
      "test Loss: 0.1485 Acc: 96.5940\n",
      "\n",
      "Epoch 628/1199\n",
      "------------------------\n",
      "train Loss: 0.0651 Acc: 98.1972\n",
      "test Loss: 0.1434 Acc: 96.7470\n",
      "\n",
      "Epoch 629/1199\n",
      "------------------------\n",
      "train Loss: 0.0619 Acc: 98.2970\n",
      "test Loss: 0.1466 Acc: 96.6783\n",
      "\n",
      "Epoch 630/1199\n",
      "------------------------\n",
      "train Loss: 0.0648 Acc: 98.2174\n",
      "test Loss: 0.1490 Acc: 96.6322\n",
      "\n",
      "Epoch 631/1199\n",
      "------------------------\n",
      "train Loss: 0.0641 Acc: 98.2254\n",
      "test Loss: 0.1467 Acc: 96.6690\n",
      "\n",
      "Epoch 632/1199\n",
      "------------------------\n",
      "train Loss: 0.0617 Acc: 98.3030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1464 Acc: 96.6954\n",
      "\n",
      "Epoch 633/1199\n",
      "------------------------\n",
      "train Loss: 0.0636 Acc: 98.2399\n",
      "test Loss: 0.1460 Acc: 96.6971\n",
      "\n",
      "Epoch 634/1199\n",
      "------------------------\n",
      "train Loss: 0.0608 Acc: 98.3262\n",
      "test Loss: 0.1447 Acc: 96.7329\n",
      "\n",
      "Epoch 635/1199\n",
      "------------------------\n",
      "train Loss: 0.0639 Acc: 98.2408\n",
      "test Loss: 0.1508 Acc: 96.5481\n",
      "\n",
      "Epoch 636/1199\n",
      "------------------------\n",
      "train Loss: 0.0622 Acc: 98.2920\n",
      "test Loss: 0.1479 Acc: 96.6438\n",
      "\n",
      "Epoch 637/1199\n",
      "------------------------\n",
      "train Loss: 0.0617 Acc: 98.2914\n",
      "test Loss: 0.1458 Acc: 96.7032\n",
      "\n",
      "Epoch 638/1199\n",
      "------------------------\n",
      "train Loss: 0.0632 Acc: 98.2574\n",
      "test Loss: 0.1440 Acc: 96.7381\n",
      "\n",
      "Epoch 639/1199\n",
      "------------------------\n",
      "train Loss: 0.0639 Acc: 98.2340\n",
      "test Loss: 0.1452 Acc: 96.7861\n",
      "\n",
      "Epoch 640/1199\n",
      "------------------------\n",
      "train Loss: 0.0630 Acc: 98.2674\n",
      "test Loss: 0.1442 Acc: 96.7859\n",
      "\n",
      "Epoch 641/1199\n",
      "------------------------\n",
      "train Loss: 0.0621 Acc: 98.3014\n",
      "test Loss: 0.1456 Acc: 96.7483\n",
      "\n",
      "Epoch 642/1199\n",
      "------------------------\n",
      "train Loss: 0.0619 Acc: 98.2928\n",
      "test Loss: 0.1455 Acc: 96.7201\n",
      "\n",
      "Epoch 643/1199\n",
      "------------------------\n",
      "train Loss: 0.0625 Acc: 98.2920\n",
      "test Loss: 0.1419 Acc: 96.7738\n",
      "\n",
      "Epoch 644/1199\n",
      "------------------------\n",
      "train Loss: 0.0635 Acc: 98.2591\n",
      "test Loss: 0.1445 Acc: 96.6854\n",
      "\n",
      "Epoch 645/1199\n",
      "------------------------\n",
      "train Loss: 0.0615 Acc: 98.3052\n",
      "test Loss: 0.1458 Acc: 96.6282\n",
      "\n",
      "Epoch 646/1199\n",
      "------------------------\n",
      "train Loss: 0.0650 Acc: 98.2257\n",
      "test Loss: 0.1462 Acc: 96.6578\n",
      "\n",
      "Epoch 647/1199\n",
      "------------------------\n",
      "train Loss: 0.0606 Acc: 98.3289\n",
      "test Loss: 0.1405 Acc: 96.8065\n",
      "\n",
      "Epoch 648/1199\n",
      "------------------------\n",
      "train Loss: 0.0603 Acc: 98.3421\n",
      "test Loss: 0.1447 Acc: 96.7707\n",
      "\n",
      "Epoch 649/1199\n",
      "------------------------\n",
      "train Loss: 0.0611 Acc: 98.3188\n",
      "test Loss: 0.1458 Acc: 96.7619\n",
      "\n",
      "Epoch 650/1199\n",
      "------------------------\n",
      "train Loss: 0.0614 Acc: 98.3153\n",
      "test Loss: 0.1422 Acc: 96.8149\n",
      "\n",
      "Epoch 651/1199\n",
      "------------------------\n",
      "train Loss: 0.0613 Acc: 98.3185\n",
      "test Loss: 0.1389 Acc: 96.8567\n",
      "\n",
      "Epoch 652/1199\n",
      "------------------------\n",
      "train Loss: 0.0604 Acc: 98.3486\n",
      "test Loss: 0.1497 Acc: 96.6379\n",
      "\n",
      "Epoch 653/1199\n",
      "------------------------\n",
      "train Loss: 0.0601 Acc: 98.3613\n",
      "test Loss: 0.1423 Acc: 96.8142\n",
      "\n",
      "Epoch 654/1199\n",
      "------------------------\n",
      "train Loss: 0.0603 Acc: 98.3423\n",
      "test Loss: 0.1493 Acc: 96.6892\n",
      "\n",
      "Epoch 655/1199\n",
      "------------------------\n",
      "train Loss: 0.0591 Acc: 98.3788\n",
      "test Loss: 0.1420 Acc: 96.8540\n",
      "\n",
      "Epoch 656/1199\n",
      "------------------------\n",
      "train Loss: 0.0578 Acc: 98.4027\n",
      "test Loss: 0.1458 Acc: 96.6553\n",
      "\n",
      "Epoch 657/1199\n",
      "------------------------\n",
      "train Loss: 0.0615 Acc: 98.3069\n",
      "test Loss: 0.1428 Acc: 96.7951\n",
      "\n",
      "Epoch 658/1199\n",
      "------------------------\n",
      "train Loss: 0.0595 Acc: 98.3739\n",
      "test Loss: 0.1434 Acc: 96.7678\n",
      "\n",
      "Epoch 659/1199\n",
      "------------------------\n",
      "train Loss: 0.0591 Acc: 98.3696\n",
      "test Loss: 0.1481 Acc: 96.7189\n",
      "\n",
      "Epoch 660/1199\n",
      "------------------------\n",
      "train Loss: 0.0628 Acc: 98.2875\n",
      "test Loss: 0.1511 Acc: 96.6096\n",
      "\n",
      "Epoch 661/1199\n",
      "------------------------\n",
      "train Loss: 0.0594 Acc: 98.3710\n",
      "test Loss: 0.1420 Acc: 96.8619\n",
      "\n",
      "Epoch 662/1199\n",
      "------------------------\n",
      "train Loss: 0.0601 Acc: 98.3475\n",
      "test Loss: 0.1451 Acc: 96.8071\n",
      "\n",
      "Epoch 663/1199\n",
      "------------------------\n",
      "train Loss: 0.0589 Acc: 98.3727\n",
      "test Loss: 0.1482 Acc: 96.6947\n",
      "\n",
      "Epoch 664/1199\n",
      "------------------------\n",
      "train Loss: 0.0616 Acc: 98.3027\n",
      "test Loss: 0.1483 Acc: 96.6697\n",
      "\n",
      "Epoch 665/1199\n",
      "------------------------\n",
      "train Loss: 0.0598 Acc: 98.3553\n",
      "test Loss: 0.1506 Acc: 96.6778\n",
      "\n",
      "Epoch 666/1199\n",
      "------------------------\n",
      "train Loss: 0.0594 Acc: 98.3527\n",
      "test Loss: 0.1446 Acc: 96.8325\n",
      "\n",
      "Epoch 667/1199\n",
      "------------------------\n",
      "train Loss: 0.0581 Acc: 98.4003\n",
      "test Loss: 0.1459 Acc: 96.7294\n",
      "\n",
      "Epoch 668/1199\n",
      "------------------------\n",
      "train Loss: 0.0586 Acc: 98.3896\n",
      "test Loss: 0.1468 Acc: 96.6933\n",
      "\n",
      "Epoch 669/1199\n",
      "------------------------\n",
      "train Loss: 0.0584 Acc: 98.3873\n",
      "test Loss: 0.1511 Acc: 96.6379\n",
      "\n",
      "Epoch 670/1199\n",
      "------------------------\n",
      "train Loss: 0.0590 Acc: 98.3761\n",
      "test Loss: 0.1400 Acc: 96.9109\n",
      "\n",
      "Epoch 671/1199\n",
      "------------------------\n",
      "train Loss: 0.0584 Acc: 98.3868\n",
      "test Loss: 0.1471 Acc: 96.7249\n",
      "\n",
      "Epoch 672/1199\n",
      "------------------------\n",
      "train Loss: 0.0597 Acc: 98.3598\n",
      "test Loss: 0.1471 Acc: 96.7477\n",
      "\n",
      "Epoch 673/1199\n",
      "------------------------\n",
      "train Loss: 0.0586 Acc: 98.3854\n",
      "test Loss: 0.1462 Acc: 96.8424\n",
      "\n",
      "Epoch 674/1199\n",
      "------------------------\n",
      "train Loss: 0.0581 Acc: 98.3968\n",
      "test Loss: 0.1467 Acc: 96.7942\n",
      "\n",
      "Epoch 675/1199\n",
      "------------------------\n",
      "train Loss: 0.0604 Acc: 98.3407\n",
      "test Loss: 0.1505 Acc: 96.6593\n",
      "\n",
      "Epoch 676/1199\n",
      "------------------------\n",
      "train Loss: 0.0588 Acc: 98.3708\n",
      "test Loss: 0.1431 Acc: 96.8460\n",
      "\n",
      "Epoch 677/1199\n",
      "------------------------\n",
      "train Loss: 0.0584 Acc: 98.3990\n",
      "test Loss: 0.1441 Acc: 96.8906\n",
      "\n",
      "Epoch 678/1199\n",
      "------------------------\n",
      "train Loss: 0.0570 Acc: 98.4282\n",
      "test Loss: 0.1474 Acc: 96.7709\n",
      "\n",
      "Epoch 679/1199\n",
      "------------------------\n",
      "train Loss: 0.0577 Acc: 98.4053\n",
      "test Loss: 0.1448 Acc: 96.7975\n",
      "\n",
      "Epoch 680/1199\n",
      "------------------------\n",
      "train Loss: 0.0577 Acc: 98.4208\n",
      "test Loss: 0.1491 Acc: 96.7258\n",
      "\n",
      "Epoch 681/1199\n",
      "------------------------\n",
      "train Loss: 0.0564 Acc: 98.4508\n",
      "test Loss: 0.1430 Acc: 96.9508\n",
      "\n",
      "Epoch 682/1199\n",
      "------------------------\n",
      "train Loss: 0.0571 Acc: 98.4328\n",
      "test Loss: 0.1476 Acc: 96.7426\n",
      "\n",
      "Epoch 683/1199\n",
      "------------------------\n",
      "train Loss: 0.0582 Acc: 98.3999\n",
      "test Loss: 0.1453 Acc: 96.8702\n",
      "\n",
      "Epoch 684/1199\n",
      "------------------------\n",
      "train Loss: 0.0565 Acc: 98.4464\n",
      "test Loss: 0.1476 Acc: 96.7919\n",
      "\n",
      "Epoch 685/1199\n",
      "------------------------\n",
      "train Loss: 0.0570 Acc: 98.4366\n",
      "test Loss: 0.1493 Acc: 96.7527\n",
      "\n",
      "Epoch 686/1199\n",
      "------------------------\n",
      "train Loss: 0.0595 Acc: 98.3449\n",
      "test Loss: 0.1460 Acc: 96.8073\n",
      "\n",
      "Epoch 687/1199\n",
      "------------------------\n",
      "train Loss: 0.0574 Acc: 98.4291\n",
      "test Loss: 0.1461 Acc: 96.7847\n",
      "\n",
      "Epoch 688/1199\n",
      "------------------------\n",
      "train Loss: 0.0582 Acc: 98.4101\n",
      "test Loss: 0.1449 Acc: 96.8239\n",
      "\n",
      "Epoch 689/1199\n",
      "------------------------\n",
      "train Loss: 0.0585 Acc: 98.3953\n",
      "test Loss: 0.1428 Acc: 96.8560\n",
      "\n",
      "Epoch 690/1199\n",
      "------------------------\n",
      "train Loss: 0.0563 Acc: 98.4524\n",
      "test Loss: 0.1517 Acc: 96.6975\n",
      "\n",
      "Epoch 691/1199\n",
      "------------------------\n",
      "train Loss: 0.0590 Acc: 98.3845\n",
      "test Loss: 0.1501 Acc: 96.6901\n",
      "\n",
      "Epoch 692/1199\n",
      "------------------------\n",
      "train Loss: 0.0603 Acc: 98.3374\n",
      "test Loss: 0.1496 Acc: 96.7619\n",
      "\n",
      "Epoch 693/1199\n",
      "------------------------\n",
      "train Loss: 0.0577 Acc: 98.3957\n",
      "test Loss: 0.1478 Acc: 96.7598\n",
      "\n",
      "Epoch 694/1199\n",
      "------------------------\n",
      "train Loss: 0.0563 Acc: 98.4530\n",
      "test Loss: 0.1488 Acc: 96.7524\n",
      "\n",
      "Epoch 695/1199\n",
      "------------------------\n",
      "train Loss: 0.0570 Acc: 98.4276\n",
      "test Loss: 0.1445 Acc: 96.8610\n",
      "\n",
      "Epoch 696/1199\n",
      "------------------------\n",
      "train Loss: 0.0572 Acc: 98.4254\n",
      "test Loss: 0.1450 Acc: 96.8766\n",
      "\n",
      "Epoch 697/1199\n",
      "------------------------\n",
      "train Loss: 0.0563 Acc: 98.4446\n",
      "test Loss: 0.1447 Acc: 96.9071\n",
      "\n",
      "Epoch 698/1199\n",
      "------------------------\n",
      "train Loss: 0.0589 Acc: 98.3718\n",
      "test Loss: 0.1447 Acc: 96.8645\n",
      "\n",
      "Epoch 699/1199\n",
      "------------------------\n",
      "train Loss: 0.0564 Acc: 98.4314\n",
      "test Loss: 0.1500 Acc: 96.6875\n",
      "\n",
      "Epoch 700/1199\n",
      "------------------------\n",
      "train Loss: 0.0609 Acc: 98.3281\n",
      "test Loss: 0.1495 Acc: 96.7913\n",
      "\n",
      "Epoch 701/1199\n",
      "------------------------\n",
      "train Loss: 0.0563 Acc: 98.4541\n",
      "test Loss: 0.1537 Acc: 96.6633\n",
      "\n",
      "Epoch 702/1199\n",
      "------------------------\n",
      "train Loss: 0.0566 Acc: 98.4440\n",
      "test Loss: 0.1488 Acc: 96.8830\n",
      "\n",
      "Epoch 703/1199\n",
      "------------------------\n",
      "train Loss: 0.0570 Acc: 98.4372\n",
      "test Loss: 0.1485 Acc: 96.8847\n",
      "\n",
      "Epoch 704/1199\n",
      "------------------------\n",
      "train Loss: 0.0558 Acc: 98.4701\n",
      "test Loss: 0.1450 Acc: 96.8781\n",
      "\n",
      "Epoch 705/1199\n",
      "------------------------\n",
      "train Loss: 0.0595 Acc: 98.3827\n",
      "test Loss: 0.1462 Acc: 96.7515\n",
      "\n",
      "Epoch 706/1199\n",
      "------------------------\n",
      "train Loss: 0.0578 Acc: 98.3935\n",
      "test Loss: 0.1446 Acc: 96.8717\n",
      "\n",
      "Epoch 707/1199\n",
      "------------------------\n",
      "train Loss: 0.0569 Acc: 98.4409\n",
      "test Loss: 0.1461 Acc: 96.8280\n",
      "\n",
      "Epoch 708/1199\n",
      "------------------------\n",
      "train Loss: 0.0568 Acc: 98.4388\n",
      "test Loss: 0.1449 Acc: 96.8006\n",
      "\n",
      "Epoch 709/1199\n",
      "------------------------\n",
      "train Loss: 0.0557 Acc: 98.4777\n",
      "test Loss: 0.1462 Acc: 96.8591\n",
      "\n",
      "Epoch 710/1199\n",
      "------------------------\n",
      "train Loss: 0.0562 Acc: 98.4524\n",
      "test Loss: 0.1484 Acc: 96.7355\n",
      "\n",
      "Epoch 711/1199\n",
      "------------------------\n",
      "train Loss: 0.0548 Acc: 98.4839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1479 Acc: 96.7502\n",
      "\n",
      "Epoch 712/1199\n",
      "------------------------\n",
      "train Loss: 0.0565 Acc: 98.4388\n",
      "test Loss: 0.1458 Acc: 96.8303\n",
      "\n",
      "Epoch 713/1199\n",
      "------------------------\n",
      "train Loss: 0.0553 Acc: 98.4760\n",
      "test Loss: 0.1491 Acc: 96.7574\n",
      "\n",
      "Epoch 714/1199\n",
      "------------------------\n",
      "train Loss: 0.0547 Acc: 98.4924\n",
      "test Loss: 0.1484 Acc: 96.7935\n",
      "\n",
      "Epoch 715/1199\n",
      "------------------------\n",
      "train Loss: 0.0564 Acc: 98.4455\n",
      "test Loss: 0.1437 Acc: 96.8954\n",
      "\n",
      "Epoch 716/1199\n",
      "------------------------\n",
      "train Loss: 0.0555 Acc: 98.4745\n",
      "test Loss: 0.1445 Acc: 96.8873\n",
      "\n",
      "Epoch 717/1199\n",
      "------------------------\n",
      "train Loss: 0.0563 Acc: 98.4437\n",
      "test Loss: 0.1514 Acc: 96.7426\n",
      "\n",
      "Epoch 718/1199\n",
      "------------------------\n",
      "train Loss: 0.0556 Acc: 98.4661\n",
      "test Loss: 0.1493 Acc: 96.8265\n",
      "\n",
      "Epoch 719/1199\n",
      "------------------------\n",
      "train Loss: 0.0532 Acc: 98.5431\n",
      "test Loss: 0.1450 Acc: 96.8631\n",
      "\n",
      "Epoch 720/1199\n",
      "------------------------\n",
      "train Loss: 0.0549 Acc: 98.4965\n",
      "test Loss: 0.1490 Acc: 96.7666\n",
      "\n",
      "Epoch 721/1199\n",
      "------------------------\n",
      "train Loss: 0.0542 Acc: 98.4993\n",
      "test Loss: 0.1491 Acc: 96.7693\n",
      "\n",
      "Epoch 722/1199\n",
      "------------------------\n",
      "train Loss: 0.0531 Acc: 98.5278\n",
      "test Loss: 0.1506 Acc: 96.7438\n",
      "\n",
      "Epoch 723/1199\n",
      "------------------------\n",
      "train Loss: 0.0565 Acc: 98.4576\n",
      "test Loss: 0.1470 Acc: 96.8472\n",
      "\n",
      "Epoch 724/1199\n",
      "------------------------\n",
      "train Loss: 0.0540 Acc: 98.5059\n",
      "test Loss: 0.1487 Acc: 96.8313\n",
      "\n",
      "Epoch 725/1199\n",
      "------------------------\n",
      "train Loss: 0.0550 Acc: 98.4869\n",
      "test Loss: 0.1469 Acc: 96.8842\n",
      "\n",
      "Epoch 726/1199\n",
      "------------------------\n",
      "train Loss: 0.0537 Acc: 98.5045\n",
      "test Loss: 0.1554 Acc: 96.6704\n",
      "\n",
      "Epoch 727/1199\n",
      "------------------------\n",
      "train Loss: 0.0531 Acc: 98.5260\n",
      "test Loss: 0.1480 Acc: 96.8111\n",
      "\n",
      "Epoch 728/1199\n",
      "------------------------\n",
      "train Loss: 0.0532 Acc: 98.5099\n",
      "test Loss: 0.1491 Acc: 96.7027\n",
      "\n",
      "Epoch 729/1199\n",
      "------------------------\n",
      "train Loss: 0.0541 Acc: 98.5142\n",
      "test Loss: 0.1478 Acc: 96.8805\n",
      "\n",
      "Epoch 730/1199\n",
      "------------------------\n",
      "train Loss: 0.0594 Acc: 98.3799\n",
      "test Loss: 0.1543 Acc: 96.6730\n",
      "\n",
      "Epoch 731/1199\n",
      "------------------------\n",
      "train Loss: 0.0574 Acc: 98.4306\n",
      "test Loss: 0.1485 Acc: 96.7712\n",
      "\n",
      "Epoch 732/1199\n",
      "------------------------\n",
      "train Loss: 0.0585 Acc: 98.4018\n",
      "test Loss: 0.1515 Acc: 96.7847\n",
      "\n",
      "Epoch 733/1199\n",
      "------------------------\n",
      "train Loss: 0.0563 Acc: 98.4411\n",
      "test Loss: 0.1535 Acc: 96.6949\n",
      "\n",
      "Epoch 734/1199\n",
      "------------------------\n",
      "train Loss: 0.0576 Acc: 98.3912\n",
      "test Loss: 0.1480 Acc: 96.8322\n",
      "\n",
      "Epoch 735/1199\n",
      "------------------------\n",
      "train Loss: 0.0549 Acc: 98.4891\n",
      "test Loss: 0.1490 Acc: 96.8170\n",
      "\n",
      "Epoch 736/1199\n",
      "------------------------\n",
      "train Loss: 0.0559 Acc: 98.4546\n",
      "test Loss: 0.1430 Acc: 96.8776\n",
      "\n",
      "Epoch 737/1199\n",
      "------------------------\n",
      "train Loss: 0.0533 Acc: 98.5116\n",
      "test Loss: 0.1479 Acc: 96.8455\n",
      "\n",
      "Epoch 738/1199\n",
      "------------------------\n",
      "train Loss: 0.0554 Acc: 98.4597\n",
      "test Loss: 0.1544 Acc: 96.6783\n",
      "\n",
      "Epoch 739/1199\n",
      "------------------------\n",
      "train Loss: 0.0540 Acc: 98.5179\n",
      "test Loss: 0.1481 Acc: 96.8458\n",
      "\n",
      "Epoch 740/1199\n",
      "------------------------\n",
      "train Loss: 0.0532 Acc: 98.5389\n",
      "test Loss: 0.1498 Acc: 96.7597\n",
      "\n",
      "Epoch 741/1199\n",
      "------------------------\n",
      "train Loss: 0.0562 Acc: 98.4446\n",
      "test Loss: 0.1479 Acc: 96.7723\n",
      "\n",
      "Epoch 742/1199\n",
      "------------------------\n",
      "train Loss: 0.0536 Acc: 98.5215\n",
      "test Loss: 0.1430 Acc: 96.9011\n",
      "\n",
      "Epoch 743/1199\n",
      "------------------------\n",
      "train Loss: 0.0542 Acc: 98.5043\n",
      "test Loss: 0.1484 Acc: 96.7845\n",
      "\n",
      "Epoch 744/1199\n",
      "------------------------\n",
      "train Loss: 0.0523 Acc: 98.5678\n",
      "test Loss: 0.1452 Acc: 96.8731\n",
      "\n",
      "Epoch 745/1199\n",
      "------------------------\n",
      "train Loss: 0.0550 Acc: 98.4946\n",
      "test Loss: 0.1459 Acc: 96.8693\n",
      "\n",
      "Epoch 746/1199\n",
      "------------------------\n",
      "train Loss: 0.0545 Acc: 98.4935\n",
      "test Loss: 0.1472 Acc: 96.8824\n",
      "\n",
      "Epoch 747/1199\n",
      "------------------------\n",
      "train Loss: 0.0532 Acc: 98.5237\n",
      "test Loss: 0.1470 Acc: 96.7716\n",
      "\n",
      "Epoch 748/1199\n",
      "------------------------\n",
      "train Loss: 0.0547 Acc: 98.4788\n",
      "test Loss: 0.1468 Acc: 96.8550\n",
      "\n",
      "Epoch 749/1199\n",
      "------------------------\n",
      "train Loss: 0.0537 Acc: 98.5104\n",
      "test Loss: 0.1508 Acc: 96.8194\n",
      "\n",
      "Epoch 750/1199\n",
      "------------------------\n",
      "train Loss: 0.0567 Acc: 98.4353\n",
      "test Loss: 0.1461 Acc: 96.8270\n",
      "\n",
      "Epoch 751/1199\n",
      "------------------------\n",
      "train Loss: 0.0550 Acc: 98.4794\n",
      "test Loss: 0.1448 Acc: 96.8995\n",
      "\n",
      "Epoch 752/1199\n",
      "------------------------\n",
      "train Loss: 0.0565 Acc: 98.4401\n",
      "test Loss: 0.1456 Acc: 96.8968\n",
      "\n",
      "Epoch 753/1199\n",
      "------------------------\n",
      "train Loss: 0.0523 Acc: 98.5475\n",
      "test Loss: 0.1496 Acc: 96.7999\n",
      "\n",
      "Epoch 754/1199\n",
      "------------------------\n",
      "train Loss: 0.0542 Acc: 98.5009\n",
      "test Loss: 0.1472 Acc: 96.8040\n",
      "\n",
      "Epoch 755/1199\n",
      "------------------------\n",
      "train Loss: 0.0544 Acc: 98.5008\n",
      "test Loss: 0.1503 Acc: 96.7783\n",
      "\n",
      "Epoch 756/1199\n",
      "------------------------\n",
      "train Loss: 0.0544 Acc: 98.5126\n",
      "test Loss: 0.1483 Acc: 96.8559\n",
      "\n",
      "Epoch 757/1199\n",
      "------------------------\n",
      "train Loss: 0.0511 Acc: 98.5798\n",
      "test Loss: 0.1464 Acc: 96.8966\n",
      "\n",
      "Epoch 758/1199\n",
      "------------------------\n",
      "train Loss: 0.0531 Acc: 98.5361\n",
      "test Loss: 0.1495 Acc: 96.7826\n",
      "\n",
      "Epoch 759/1199\n",
      "------------------------\n",
      "train Loss: 0.0536 Acc: 98.5155\n",
      "test Loss: 0.1466 Acc: 96.8495\n",
      "\n",
      "Epoch 760/1199\n",
      "------------------------\n",
      "train Loss: 0.0532 Acc: 98.5194\n",
      "test Loss: 0.1456 Acc: 96.9557\n",
      "\n",
      "Epoch 761/1199\n",
      "------------------------\n",
      "train Loss: 0.0529 Acc: 98.5306\n",
      "test Loss: 0.1502 Acc: 96.7018\n",
      "\n",
      "Epoch 762/1199\n",
      "------------------------\n",
      "train Loss: 0.0517 Acc: 98.5626\n",
      "test Loss: 0.1454 Acc: 96.8985\n",
      "\n",
      "Epoch 763/1199\n",
      "------------------------\n",
      "train Loss: 0.0512 Acc: 98.5816\n",
      "test Loss: 0.1453 Acc: 96.9035\n",
      "\n",
      "Epoch 764/1199\n",
      "------------------------\n",
      "train Loss: 0.0540 Acc: 98.5163\n",
      "test Loss: 0.1478 Acc: 96.7647\n",
      "\n",
      "Epoch 765/1199\n",
      "------------------------\n",
      "train Loss: 0.0541 Acc: 98.5078\n",
      "test Loss: 0.1459 Acc: 96.9370\n",
      "\n",
      "Epoch 766/1199\n",
      "------------------------\n",
      "train Loss: 0.0521 Acc: 98.5578\n",
      "test Loss: 0.1457 Acc: 96.8063\n",
      "\n",
      "Epoch 767/1199\n",
      "------------------------\n",
      "train Loss: 0.0551 Acc: 98.4873\n",
      "test Loss: 0.1467 Acc: 96.9141\n",
      "\n",
      "Epoch 768/1199\n",
      "------------------------\n",
      "train Loss: 0.0508 Acc: 98.5888\n",
      "test Loss: 0.1482 Acc: 96.7426\n",
      "\n",
      "Epoch 769/1199\n",
      "------------------------\n",
      "train Loss: 0.0541 Acc: 98.4895\n",
      "test Loss: 0.1484 Acc: 96.8401\n",
      "\n",
      "Epoch 770/1199\n",
      "------------------------\n",
      "train Loss: 0.0542 Acc: 98.4984\n",
      "test Loss: 0.1464 Acc: 96.8790\n",
      "\n",
      "Epoch 771/1199\n",
      "------------------------\n",
      "train Loss: 0.0545 Acc: 98.5091\n",
      "test Loss: 0.1424 Acc: 96.9584\n",
      "\n",
      "Epoch 772/1199\n",
      "------------------------\n",
      "train Loss: 0.0530 Acc: 98.5347\n",
      "test Loss: 0.1477 Acc: 96.9014\n",
      "\n",
      "Epoch 773/1199\n",
      "------------------------\n",
      "train Loss: 0.0541 Acc: 98.5117\n",
      "test Loss: 0.1503 Acc: 96.8419\n",
      "\n",
      "Epoch 774/1199\n",
      "------------------------\n",
      "train Loss: 0.0558 Acc: 98.4549\n",
      "test Loss: 0.1489 Acc: 96.8793\n",
      "\n",
      "Epoch 775/1199\n",
      "------------------------\n",
      "train Loss: 0.0532 Acc: 98.5265\n",
      "test Loss: 0.1519 Acc: 96.7453\n",
      "\n",
      "Epoch 776/1199\n",
      "------------------------\n",
      "train Loss: 0.0523 Acc: 98.5719\n",
      "test Loss: 0.1485 Acc: 96.7626\n",
      "\n",
      "Epoch 777/1199\n",
      "------------------------\n",
      "train Loss: 0.0515 Acc: 98.5689\n",
      "test Loss: 0.1461 Acc: 96.8776\n",
      "\n",
      "Epoch 778/1199\n",
      "------------------------\n",
      "train Loss: 0.0519 Acc: 98.5719\n",
      "test Loss: 0.1511 Acc: 96.7726\n",
      "\n",
      "Epoch 779/1199\n",
      "------------------------\n",
      "train Loss: 0.0529 Acc: 98.5390\n",
      "test Loss: 0.1475 Acc: 96.8331\n",
      "\n",
      "Epoch 780/1199\n",
      "------------------------\n",
      "train Loss: 0.0514 Acc: 98.5705\n",
      "test Loss: 0.1541 Acc: 96.7009\n",
      "\n",
      "Epoch 781/1199\n",
      "------------------------\n",
      "train Loss: 0.0531 Acc: 98.5281\n",
      "test Loss: 0.1441 Acc: 96.9652\n",
      "\n",
      "Epoch 782/1199\n",
      "------------------------\n",
      "train Loss: 0.0543 Acc: 98.4995\n",
      "test Loss: 0.1483 Acc: 96.9253\n",
      "\n",
      "Epoch 783/1199\n",
      "------------------------\n",
      "train Loss: 0.0507 Acc: 98.6067\n",
      "test Loss: 0.1501 Acc: 96.8122\n",
      "\n",
      "Epoch 784/1199\n",
      "------------------------\n",
      "train Loss: 0.0536 Acc: 98.5131\n",
      "test Loss: 0.1478 Acc: 96.9339\n",
      "\n",
      "Epoch 785/1199\n",
      "------------------------\n",
      "train Loss: 0.0538 Acc: 98.5252\n",
      "test Loss: 0.1517 Acc: 96.8995\n",
      "\n",
      "Epoch 786/1199\n",
      "------------------------\n",
      "train Loss: 0.0548 Acc: 98.4905\n",
      "test Loss: 0.1495 Acc: 96.7938\n",
      "\n",
      "Epoch 787/1199\n",
      "------------------------\n",
      "train Loss: 0.0516 Acc: 98.5778\n",
      "test Loss: 0.1475 Acc: 96.8667\n",
      "\n",
      "Epoch 788/1199\n",
      "------------------------\n",
      "train Loss: 0.0515 Acc: 98.5819\n",
      "test Loss: 0.1480 Acc: 96.8600\n",
      "\n",
      "Epoch 789/1199\n",
      "------------------------\n",
      "train Loss: 0.0553 Acc: 98.4801\n",
      "test Loss: 0.1513 Acc: 96.8913\n",
      "\n",
      "Epoch 790/1199\n",
      "------------------------\n",
      "train Loss: 0.0555 Acc: 98.4812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1506 Acc: 96.8830\n",
      "\n",
      "Epoch 791/1199\n",
      "------------------------\n",
      "train Loss: 0.0546 Acc: 98.4869\n",
      "test Loss: 0.1502 Acc: 96.8287\n",
      "\n",
      "Epoch 792/1199\n",
      "------------------------\n",
      "train Loss: 0.0553 Acc: 98.4698\n",
      "test Loss: 0.1474 Acc: 96.8294\n",
      "\n",
      "Epoch 793/1199\n",
      "------------------------\n",
      "train Loss: 0.0523 Acc: 98.5412\n",
      "test Loss: 0.1485 Acc: 96.7400\n",
      "\n",
      "Epoch 794/1199\n",
      "------------------------\n",
      "train Loss: 0.0518 Acc: 98.5740\n",
      "test Loss: 0.1508 Acc: 96.8241\n",
      "\n",
      "Epoch 795/1199\n",
      "------------------------\n",
      "train Loss: 0.0525 Acc: 98.5524\n",
      "test Loss: 0.1481 Acc: 96.8619\n",
      "\n",
      "Epoch 796/1199\n",
      "------------------------\n",
      "train Loss: 0.0513 Acc: 98.5803\n",
      "test Loss: 0.1480 Acc: 96.7952\n",
      "\n",
      "Epoch 797/1199\n",
      "------------------------\n",
      "train Loss: 0.0494 Acc: 98.6352\n",
      "test Loss: 0.1457 Acc: 96.9073\n",
      "\n",
      "Epoch 798/1199\n",
      "------------------------\n",
      "train Loss: 0.0519 Acc: 98.5477\n",
      "test Loss: 0.1474 Acc: 96.8420\n",
      "\n",
      "Epoch 799/1199\n",
      "------------------------\n",
      "train Loss: 0.0529 Acc: 98.5461\n",
      "test Loss: 0.1512 Acc: 96.7470\n",
      "\n",
      "Epoch 800/1199\n",
      "------------------------\n",
      "train Loss: 0.0525 Acc: 98.5506\n",
      "test Loss: 0.1463 Acc: 96.9317\n",
      "\n",
      "Epoch 801/1199\n",
      "------------------------\n",
      "train Loss: 0.0516 Acc: 98.5781\n",
      "test Loss: 0.1478 Acc: 96.8904\n",
      "\n",
      "Epoch 802/1199\n",
      "------------------------\n",
      "train Loss: 0.0517 Acc: 98.5703\n",
      "test Loss: 0.1482 Acc: 96.9533\n",
      "\n",
      "Epoch 803/1199\n",
      "------------------------\n",
      "train Loss: 0.0525 Acc: 98.5423\n",
      "test Loss: 0.1518 Acc: 96.7835\n",
      "\n",
      "Epoch 804/1199\n",
      "------------------------\n",
      "train Loss: 0.0489 Acc: 98.6457\n",
      "test Loss: 0.1484 Acc: 96.8184\n",
      "\n",
      "Epoch 805/1199\n",
      "------------------------\n",
      "train Loss: 0.0523 Acc: 98.5556\n",
      "test Loss: 0.1470 Acc: 96.8489\n",
      "\n",
      "Epoch 806/1199\n",
      "------------------------\n",
      "train Loss: 0.0505 Acc: 98.6007\n",
      "test Loss: 0.1492 Acc: 96.8130\n",
      "\n",
      "Epoch 807/1199\n",
      "------------------------\n",
      "train Loss: 0.0501 Acc: 98.6237\n",
      "test Loss: 0.1500 Acc: 96.7327\n",
      "\n",
      "Epoch 808/1199\n",
      "------------------------\n",
      "train Loss: 0.0504 Acc: 98.5985\n",
      "test Loss: 0.1460 Acc: 96.9291\n",
      "\n",
      "Epoch 809/1199\n",
      "------------------------\n",
      "train Loss: 0.0495 Acc: 98.6366\n",
      "test Loss: 0.1515 Acc: 96.7693\n",
      "\n",
      "Epoch 810/1199\n",
      "------------------------\n",
      "train Loss: 0.0524 Acc: 98.5479\n",
      "test Loss: 0.1454 Acc: 96.9097\n",
      "\n",
      "Epoch 811/1199\n",
      "------------------------\n",
      "train Loss: 0.0499 Acc: 98.6201\n",
      "test Loss: 0.1504 Acc: 96.7945\n",
      "\n",
      "Epoch 812/1199\n",
      "------------------------\n",
      "train Loss: 0.0532 Acc: 98.5160\n",
      "test Loss: 0.1498 Acc: 96.8303\n",
      "\n",
      "Epoch 813/1199\n",
      "------------------------\n",
      "train Loss: 0.0514 Acc: 98.5919\n",
      "test Loss: 0.1523 Acc: 96.7692\n",
      "\n",
      "Epoch 814/1199\n",
      "------------------------\n",
      "train Loss: 0.0508 Acc: 98.5873\n",
      "test Loss: 0.1486 Acc: 96.8626\n",
      "\n",
      "Epoch 815/1199\n",
      "------------------------\n",
      "train Loss: 0.0500 Acc: 98.6028\n",
      "test Loss: 0.1500 Acc: 96.8524\n",
      "\n",
      "Epoch 816/1199\n",
      "------------------------\n",
      "train Loss: 0.0512 Acc: 98.5787\n",
      "test Loss: 0.1458 Acc: 96.8828\n",
      "\n",
      "Epoch 817/1199\n",
      "------------------------\n",
      "train Loss: 0.0549 Acc: 98.4918\n",
      "test Loss: 0.1484 Acc: 96.8393\n",
      "\n",
      "Epoch 818/1199\n",
      "------------------------\n",
      "train Loss: 0.0518 Acc: 98.5527\n",
      "test Loss: 0.1481 Acc: 96.8363\n",
      "\n",
      "Epoch 819/1199\n",
      "------------------------\n",
      "train Loss: 0.0511 Acc: 98.5843\n",
      "test Loss: 0.1509 Acc: 96.8263\n",
      "\n",
      "Epoch 820/1199\n",
      "------------------------\n",
      "train Loss: 0.0507 Acc: 98.5990\n",
      "test Loss: 0.1473 Acc: 96.8970\n",
      "\n",
      "Epoch 821/1199\n",
      "------------------------\n",
      "train Loss: 0.0499 Acc: 98.6305\n",
      "test Loss: 0.1486 Acc: 96.9230\n",
      "\n",
      "Epoch 822/1199\n",
      "------------------------\n",
      "train Loss: 0.0518 Acc: 98.5620\n",
      "test Loss: 0.1454 Acc: 96.9272\n",
      "\n",
      "Epoch 823/1199\n",
      "------------------------\n",
      "train Loss: 0.0504 Acc: 98.5968\n",
      "test Loss: 0.1458 Acc: 96.9146\n",
      "\n",
      "Epoch 824/1199\n",
      "------------------------\n",
      "train Loss: 0.0518 Acc: 98.5700\n",
      "test Loss: 0.1510 Acc: 96.8367\n",
      "\n",
      "Epoch 825/1199\n",
      "------------------------\n",
      "train Loss: 0.0505 Acc: 98.5948\n",
      "test Loss: 0.1472 Acc: 96.8968\n",
      "\n",
      "Epoch 826/1199\n",
      "------------------------\n",
      "train Loss: 0.0506 Acc: 98.5969\n",
      "test Loss: 0.1502 Acc: 96.7973\n",
      "\n",
      "Epoch 827/1199\n",
      "------------------------\n",
      "train Loss: 0.0533 Acc: 98.5364\n",
      "test Loss: 0.1508 Acc: 96.8505\n",
      "\n",
      "Epoch 828/1199\n",
      "------------------------\n",
      "train Loss: 0.0529 Acc: 98.5407\n",
      "test Loss: 0.1480 Acc: 96.8619\n",
      "\n",
      "Epoch 829/1199\n",
      "------------------------\n",
      "train Loss: 0.0505 Acc: 98.6019\n",
      "test Loss: 0.1515 Acc: 96.8149\n",
      "\n",
      "Epoch 830/1199\n",
      "------------------------\n",
      "train Loss: 0.0511 Acc: 98.5793\n",
      "test Loss: 0.1508 Acc: 96.8436\n",
      "\n",
      "Epoch 831/1199\n",
      "------------------------\n",
      "train Loss: 0.0511 Acc: 98.5750\n",
      "test Loss: 0.1494 Acc: 96.8275\n",
      "\n",
      "Epoch 832/1199\n",
      "------------------------\n",
      "train Loss: 0.0520 Acc: 98.5542\n",
      "test Loss: 0.1491 Acc: 96.8761\n",
      "\n",
      "Epoch 833/1199\n",
      "------------------------\n",
      "train Loss: 0.0519 Acc: 98.5683\n",
      "test Loss: 0.1471 Acc: 96.9895\n",
      "\n",
      "Epoch 834/1199\n",
      "------------------------\n",
      "train Loss: 0.0500 Acc: 98.6124\n",
      "test Loss: 0.1497 Acc: 96.9018\n",
      "\n",
      "Epoch 835/1199\n",
      "------------------------\n",
      "train Loss: 0.0525 Acc: 98.5500\n",
      "test Loss: 0.1522 Acc: 96.7714\n",
      "\n",
      "Epoch 836/1199\n",
      "------------------------\n",
      "train Loss: 0.0491 Acc: 98.6315\n",
      "test Loss: 0.1480 Acc: 96.8838\n",
      "\n",
      "Epoch 837/1199\n",
      "------------------------\n",
      "train Loss: 0.0512 Acc: 98.5795\n",
      "test Loss: 0.1521 Acc: 96.8419\n",
      "\n",
      "Epoch 838/1199\n",
      "------------------------\n",
      "train Loss: 0.0519 Acc: 98.5499\n",
      "test Loss: 0.1503 Acc: 96.8921\n",
      "\n",
      "Epoch 839/1199\n",
      "------------------------\n",
      "train Loss: 0.0516 Acc: 98.5756\n",
      "test Loss: 0.1544 Acc: 96.6557\n",
      "\n",
      "Epoch 840/1199\n",
      "------------------------\n",
      "train Loss: 0.0504 Acc: 98.6184\n",
      "test Loss: 0.1458 Acc: 96.8679\n",
      "\n",
      "Epoch 841/1199\n",
      "------------------------\n",
      "train Loss: 0.0504 Acc: 98.6047\n",
      "test Loss: 0.1490 Acc: 96.9396\n",
      "\n",
      "Epoch 842/1199\n",
      "------------------------\n",
      "train Loss: 0.0484 Acc: 98.6602\n",
      "test Loss: 0.1474 Acc: 96.8282\n",
      "\n",
      "Epoch 843/1199\n",
      "------------------------\n",
      "train Loss: 0.0494 Acc: 98.6155\n",
      "test Loss: 0.1494 Acc: 96.9239\n",
      "\n",
      "Epoch 844/1199\n",
      "------------------------\n",
      "train Loss: 0.0491 Acc: 98.6429\n",
      "test Loss: 0.1453 Acc: 96.9501\n",
      "\n",
      "Epoch 845/1199\n",
      "------------------------\n",
      "train Loss: 0.0492 Acc: 98.6390\n",
      "test Loss: 0.1498 Acc: 96.9299\n",
      "\n",
      "Epoch 846/1199\n",
      "------------------------\n",
      "train Loss: 0.0514 Acc: 98.5834\n",
      "test Loss: 0.1519 Acc: 96.7783\n",
      "\n",
      "Epoch 847/1199\n",
      "------------------------\n",
      "train Loss: 0.0502 Acc: 98.6070\n",
      "test Loss: 0.1513 Acc: 96.8158\n",
      "\n",
      "Epoch 848/1199\n",
      "------------------------\n",
      "train Loss: 0.0516 Acc: 98.5846\n",
      "test Loss: 0.1543 Acc: 96.8056\n",
      "\n",
      "Epoch 849/1199\n",
      "------------------------\n",
      "train Loss: 0.0525 Acc: 98.5510\n",
      "test Loss: 0.1453 Acc: 96.9208\n",
      "\n",
      "Epoch 850/1199\n",
      "------------------------\n",
      "train Loss: 0.0513 Acc: 98.5920\n",
      "test Loss: 0.1512 Acc: 96.7933\n",
      "\n",
      "Epoch 851/1199\n",
      "------------------------\n",
      "train Loss: 0.0505 Acc: 98.5976\n",
      "test Loss: 0.1512 Acc: 96.7087\n",
      "\n",
      "Epoch 852/1199\n",
      "------------------------\n",
      "train Loss: 0.0502 Acc: 98.6069\n",
      "test Loss: 0.1469 Acc: 96.9429\n",
      "\n",
      "Epoch 853/1199\n",
      "------------------------\n",
      "train Loss: 0.0507 Acc: 98.6041\n",
      "test Loss: 0.1514 Acc: 96.8180\n",
      "\n",
      "Epoch 854/1199\n",
      "------------------------\n",
      "train Loss: 0.0497 Acc: 98.6290\n",
      "test Loss: 0.1490 Acc: 96.9070\n",
      "\n",
      "Epoch 855/1199\n",
      "------------------------\n",
      "train Loss: 0.0503 Acc: 98.5997\n",
      "test Loss: 0.1476 Acc: 96.8959\n",
      "\n",
      "Epoch 856/1199\n",
      "------------------------\n",
      "train Loss: 0.0479 Acc: 98.6755\n",
      "test Loss: 0.1482 Acc: 96.9533\n",
      "\n",
      "Epoch 857/1199\n",
      "------------------------\n",
      "train Loss: 0.0496 Acc: 98.6373\n",
      "test Loss: 0.1514 Acc: 96.8562\n",
      "\n",
      "Epoch 858/1199\n",
      "------------------------\n",
      "train Loss: 0.0535 Acc: 98.5296\n",
      "test Loss: 0.1512 Acc: 96.7557\n",
      "\n",
      "Epoch 859/1199\n",
      "------------------------\n",
      "train Loss: 0.0493 Acc: 98.6334\n",
      "test Loss: 0.1517 Acc: 96.7655\n",
      "\n",
      "Epoch 860/1199\n",
      "------------------------\n",
      "train Loss: 0.0494 Acc: 98.6286\n",
      "test Loss: 0.1516 Acc: 96.8838\n",
      "\n",
      "Epoch 861/1199\n",
      "------------------------\n",
      "train Loss: 0.0533 Acc: 98.5349\n",
      "test Loss: 0.1498 Acc: 96.9299\n",
      "\n",
      "Epoch 862/1199\n",
      "------------------------\n",
      "train Loss: 0.0507 Acc: 98.5948\n",
      "test Loss: 0.1533 Acc: 96.7963\n",
      "\n",
      "Epoch 863/1199\n",
      "------------------------\n",
      "train Loss: 0.0516 Acc: 98.5774\n",
      "test Loss: 0.1537 Acc: 96.8099\n",
      "\n",
      "Epoch 864/1199\n",
      "------------------------\n",
      "train Loss: 0.0513 Acc: 98.5868\n",
      "test Loss: 0.1477 Acc: 96.9035\n",
      "\n",
      "Epoch 865/1199\n",
      "------------------------\n",
      "train Loss: 0.0519 Acc: 98.5679\n",
      "test Loss: 0.1497 Acc: 96.8655\n",
      "\n",
      "Epoch 866/1199\n",
      "------------------------\n",
      "train Loss: 0.0504 Acc: 98.6156\n",
      "test Loss: 0.1562 Acc: 96.8286\n",
      "\n",
      "Epoch 867/1199\n",
      "------------------------\n",
      "train Loss: 0.0522 Acc: 98.5553\n",
      "test Loss: 0.1496 Acc: 96.7726\n",
      "\n",
      "Epoch 868/1199\n",
      "------------------------\n",
      "train Loss: 0.0492 Acc: 98.6165\n",
      "test Loss: 0.1531 Acc: 96.8367\n",
      "\n",
      "Epoch 869/1199\n",
      "------------------------\n",
      "train Loss: 0.0491 Acc: 98.6375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1526 Acc: 96.8246\n",
      "\n",
      "Epoch 870/1199\n",
      "------------------------\n",
      "train Loss: 0.0492 Acc: 98.6402\n",
      "test Loss: 0.1544 Acc: 96.8192\n",
      "\n",
      "Epoch 871/1199\n",
      "------------------------\n",
      "train Loss: 0.0514 Acc: 98.5868\n",
      "test Loss: 0.1485 Acc: 96.9128\n",
      "\n",
      "Epoch 872/1199\n",
      "------------------------\n",
      "train Loss: 0.0492 Acc: 98.6415\n",
      "test Loss: 0.1479 Acc: 97.0090\n",
      "\n",
      "Epoch 873/1199\n",
      "------------------------\n",
      "train Loss: 0.0510 Acc: 98.5899\n",
      "test Loss: 0.1515 Acc: 96.8629\n",
      "\n",
      "Epoch 874/1199\n",
      "------------------------\n",
      "train Loss: 0.0492 Acc: 98.6278\n",
      "test Loss: 0.1489 Acc: 96.8268\n",
      "\n",
      "Epoch 875/1199\n",
      "------------------------\n",
      "train Loss: 0.0504 Acc: 98.6236\n",
      "test Loss: 0.1490 Acc: 96.8975\n",
      "\n",
      "Epoch 876/1199\n",
      "------------------------\n",
      "train Loss: 0.0487 Acc: 98.6477\n",
      "test Loss: 0.1491 Acc: 96.8814\n",
      "\n",
      "Epoch 877/1199\n",
      "------------------------\n",
      "train Loss: 0.0495 Acc: 98.6252\n",
      "test Loss: 0.1510 Acc: 96.8334\n",
      "\n",
      "Epoch 878/1199\n",
      "------------------------\n",
      "train Loss: 0.0499 Acc: 98.6230\n",
      "test Loss: 0.1521 Acc: 96.9211\n",
      "\n",
      "Epoch 879/1199\n",
      "------------------------\n",
      "train Loss: 0.0477 Acc: 98.6675\n",
      "test Loss: 0.1499 Acc: 96.8318\n",
      "\n",
      "Epoch 880/1199\n",
      "------------------------\n",
      "train Loss: 0.0487 Acc: 98.6568\n",
      "test Loss: 0.1498 Acc: 96.8897\n",
      "\n",
      "Epoch 881/1199\n",
      "------------------------\n",
      "train Loss: 0.0492 Acc: 98.6362\n",
      "test Loss: 0.1517 Acc: 96.8234\n",
      "\n",
      "Epoch 882/1199\n",
      "------------------------\n",
      "train Loss: 0.0512 Acc: 98.5798\n",
      "test Loss: 0.1509 Acc: 96.8177\n",
      "\n",
      "Epoch 883/1199\n",
      "------------------------\n",
      "train Loss: 0.0487 Acc: 98.6483\n",
      "test Loss: 0.1483 Acc: 96.9042\n",
      "\n",
      "Epoch 884/1199\n",
      "------------------------\n",
      "train Loss: 0.0513 Acc: 98.5653\n",
      "test Loss: 0.1491 Acc: 96.9042\n",
      "\n",
      "Epoch 885/1199\n",
      "------------------------\n",
      "train Loss: 0.0480 Acc: 98.6722\n",
      "test Loss: 0.1519 Acc: 96.8491\n",
      "\n",
      "Epoch 886/1199\n",
      "------------------------\n",
      "train Loss: 0.0482 Acc: 98.6708\n",
      "test Loss: 0.1491 Acc: 96.9398\n",
      "\n",
      "Epoch 887/1199\n",
      "------------------------\n",
      "train Loss: 0.0471 Acc: 98.7043\n",
      "test Loss: 0.1491 Acc: 96.9539\n",
      "\n",
      "Epoch 888/1199\n",
      "------------------------\n",
      "train Loss: 0.0506 Acc: 98.6073\n",
      "test Loss: 0.1514 Acc: 96.9339\n",
      "\n",
      "Epoch 889/1199\n",
      "------------------------\n",
      "train Loss: 0.0511 Acc: 98.5895\n",
      "test Loss: 0.1483 Acc: 96.8913\n",
      "\n",
      "Epoch 890/1199\n",
      "------------------------\n",
      "train Loss: 0.0515 Acc: 98.5838\n",
      "test Loss: 0.1521 Acc: 96.8372\n",
      "\n",
      "Epoch 891/1199\n",
      "------------------------\n",
      "train Loss: 0.0505 Acc: 98.6180\n",
      "test Loss: 0.1511 Acc: 96.8065\n",
      "\n",
      "Epoch 892/1199\n",
      "------------------------\n",
      "train Loss: 0.0515 Acc: 98.5886\n",
      "test Loss: 0.1502 Acc: 96.9275\n",
      "\n",
      "Epoch 893/1199\n",
      "------------------------\n",
      "train Loss: 0.0481 Acc: 98.6662\n",
      "test Loss: 0.1489 Acc: 96.9021\n",
      "\n",
      "Epoch 894/1199\n",
      "------------------------\n",
      "train Loss: 0.0494 Acc: 98.6360\n",
      "test Loss: 0.1568 Acc: 96.7676\n",
      "\n",
      "Epoch 895/1199\n",
      "------------------------\n",
      "train Loss: 0.0603 Acc: 98.3906\n",
      "test Loss: 0.1486 Acc: 96.8990\n",
      "\n",
      "Epoch 896/1199\n",
      "------------------------\n",
      "train Loss: 0.0544 Acc: 98.5176\n",
      "test Loss: 0.1516 Acc: 96.8638\n",
      "\n",
      "Epoch 897/1199\n",
      "------------------------\n",
      "train Loss: 0.0523 Acc: 98.5777\n",
      "test Loss: 0.1545 Acc: 96.7785\n",
      "\n",
      "Epoch 898/1199\n",
      "------------------------\n",
      "train Loss: 0.0521 Acc: 98.5794\n",
      "test Loss: 0.1480 Acc: 96.9441\n",
      "\n",
      "Epoch 899/1199\n",
      "------------------------\n",
      "train Loss: 0.0510 Acc: 98.6109\n",
      "test Loss: 0.1506 Acc: 96.7733\n",
      "\n",
      "Epoch 900/1199\n",
      "------------------------\n",
      "train Loss: 0.0499 Acc: 98.6199\n",
      "test Loss: 0.1480 Acc: 96.9408\n",
      "\n",
      "Epoch 901/1199\n",
      "------------------------\n",
      "train Loss: 0.0505 Acc: 98.6254\n",
      "test Loss: 0.1519 Acc: 96.8614\n",
      "\n",
      "Epoch 902/1199\n",
      "------------------------\n",
      "train Loss: 0.0522 Acc: 98.5903\n",
      "test Loss: 0.1502 Acc: 96.8876\n",
      "\n",
      "Epoch 903/1199\n",
      "------------------------\n",
      "train Loss: 0.0505 Acc: 98.6172\n",
      "test Loss: 0.1483 Acc: 96.9151\n",
      "\n",
      "Epoch 904/1199\n",
      "------------------------\n",
      "train Loss: 0.0496 Acc: 98.6446\n",
      "test Loss: 0.1496 Acc: 96.9116\n",
      "\n",
      "Epoch 905/1199\n",
      "------------------------\n",
      "train Loss: 0.0493 Acc: 98.6509\n",
      "test Loss: 0.1506 Acc: 96.7885\n",
      "\n",
      "Epoch 906/1199\n",
      "------------------------\n",
      "train Loss: 0.0497 Acc: 98.6485\n",
      "test Loss: 0.1526 Acc: 96.7671\n",
      "\n",
      "Epoch 907/1199\n",
      "------------------------\n",
      "train Loss: 0.0471 Acc: 98.6997\n",
      "test Loss: 0.1530 Acc: 96.7780\n",
      "\n",
      "Epoch 908/1199\n",
      "------------------------\n",
      "train Loss: 0.0485 Acc: 98.6729\n",
      "test Loss: 0.1494 Acc: 96.9470\n",
      "\n",
      "Epoch 909/1199\n",
      "------------------------\n",
      "train Loss: 0.0503 Acc: 98.6246\n",
      "test Loss: 0.1523 Acc: 96.8362\n",
      "\n",
      "Epoch 910/1199\n",
      "------------------------\n",
      "train Loss: 0.0525 Acc: 98.5602\n",
      "test Loss: 0.1511 Acc: 96.8322\n",
      "\n",
      "Epoch 911/1199\n",
      "------------------------\n",
      "train Loss: 0.0478 Acc: 98.6845\n",
      "test Loss: 0.1468 Acc: 96.9249\n",
      "\n",
      "Epoch 912/1199\n",
      "------------------------\n",
      "train Loss: 0.0497 Acc: 98.6379\n",
      "test Loss: 0.1467 Acc: 97.0251\n",
      "\n",
      "Epoch 913/1199\n",
      "------------------------\n",
      "train Loss: 0.0495 Acc: 98.6358\n",
      "test Loss: 0.1484 Acc: 96.9579\n",
      "\n",
      "Epoch 914/1199\n",
      "------------------------\n",
      "train Loss: 0.0475 Acc: 98.7062\n",
      "test Loss: 0.1525 Acc: 96.8664\n",
      "\n",
      "Epoch 915/1199\n",
      "------------------------\n",
      "train Loss: 0.0496 Acc: 98.6532\n",
      "test Loss: 0.1482 Acc: 96.8393\n",
      "\n",
      "Epoch 916/1199\n",
      "------------------------\n",
      "train Loss: 0.0487 Acc: 98.6467\n",
      "test Loss: 0.1538 Acc: 96.7039\n",
      "\n",
      "Epoch 917/1199\n",
      "------------------------\n",
      "train Loss: 0.0491 Acc: 98.6519\n",
      "test Loss: 0.1487 Acc: 96.9698\n",
      "\n",
      "Epoch 918/1199\n",
      "------------------------\n",
      "train Loss: 0.0490 Acc: 98.6650\n",
      "test Loss: 0.1492 Acc: 96.8375\n",
      "\n",
      "Epoch 919/1199\n",
      "------------------------\n",
      "train Loss: 0.0489 Acc: 98.6562\n",
      "test Loss: 0.1525 Acc: 96.7976\n",
      "\n",
      "Epoch 920/1199\n",
      "------------------------\n",
      "train Loss: 0.0489 Acc: 98.6655\n",
      "test Loss: 0.1482 Acc: 96.9218\n",
      "\n",
      "Epoch 921/1199\n",
      "------------------------\n",
      "train Loss: 0.0475 Acc: 98.6932\n",
      "test Loss: 0.1496 Acc: 96.8989\n",
      "\n",
      "Epoch 922/1199\n",
      "------------------------\n",
      "train Loss: 0.0491 Acc: 98.6466\n",
      "test Loss: 0.1515 Acc: 96.8864\n",
      "\n",
      "Epoch 923/1199\n",
      "------------------------\n",
      "train Loss: 0.0487 Acc: 98.6549\n",
      "test Loss: 0.1512 Acc: 96.9336\n",
      "\n",
      "Epoch 924/1199\n",
      "------------------------\n",
      "train Loss: 0.0493 Acc: 98.6585\n",
      "test Loss: 0.1493 Acc: 96.9260\n",
      "\n",
      "Epoch 925/1199\n",
      "------------------------\n",
      "train Loss: 0.0495 Acc: 98.6364\n",
      "test Loss: 0.1466 Acc: 96.9363\n",
      "\n",
      "Epoch 926/1199\n",
      "------------------------\n",
      "train Loss: 0.0475 Acc: 98.6833\n",
      "test Loss: 0.1473 Acc: 96.9470\n",
      "\n",
      "Epoch 927/1199\n",
      "------------------------\n",
      "train Loss: 0.0467 Acc: 98.7097\n",
      "test Loss: 0.1481 Acc: 96.8443\n",
      "\n",
      "Epoch 928/1199\n",
      "------------------------\n",
      "train Loss: 0.0491 Acc: 98.6359\n",
      "test Loss: 0.1518 Acc: 96.8576\n",
      "\n",
      "Epoch 929/1199\n",
      "------------------------\n",
      "train Loss: 0.0480 Acc: 98.6720\n",
      "test Loss: 0.1483 Acc: 96.9334\n",
      "\n",
      "Epoch 930/1199\n",
      "------------------------\n",
      "train Loss: 0.0482 Acc: 98.6744\n",
      "test Loss: 0.1518 Acc: 96.8092\n",
      "\n",
      "Epoch 931/1199\n",
      "------------------------\n",
      "train Loss: 0.0468 Acc: 98.7089\n",
      "test Loss: 0.1519 Acc: 96.8021\n",
      "\n",
      "Epoch 932/1199\n",
      "------------------------\n",
      "train Loss: 0.0496 Acc: 98.6314\n",
      "test Loss: 0.1495 Acc: 96.9097\n",
      "\n",
      "Epoch 933/1199\n",
      "------------------------\n",
      "train Loss: 0.0486 Acc: 98.6439\n",
      "test Loss: 0.1513 Acc: 96.9027\n",
      "\n",
      "Epoch 934/1199\n",
      "------------------------\n",
      "train Loss: 0.0488 Acc: 98.6625\n",
      "test Loss: 0.1490 Acc: 96.9538\n",
      "\n",
      "Epoch 935/1199\n",
      "------------------------\n",
      "train Loss: 0.0503 Acc: 98.6255\n",
      "test Loss: 0.1460 Acc: 96.9536\n",
      "\n",
      "Epoch 936/1199\n",
      "------------------------\n",
      "train Loss: 0.0486 Acc: 98.6663\n",
      "test Loss: 0.1508 Acc: 96.8211\n",
      "\n",
      "Epoch 937/1199\n",
      "------------------------\n",
      "train Loss: 0.0497 Acc: 98.6357\n",
      "test Loss: 0.1472 Acc: 97.0522\n",
      "\n",
      "Epoch 938/1199\n",
      "------------------------\n",
      "train Loss: 0.0468 Acc: 98.7043\n",
      "test Loss: 0.1466 Acc: 96.9643\n",
      "\n",
      "Epoch 939/1199\n",
      "------------------------\n",
      "train Loss: 0.0473 Acc: 98.6922\n",
      "test Loss: 0.1448 Acc: 96.9457\n",
      "\n",
      "Epoch 940/1199\n",
      "------------------------\n",
      "train Loss: 0.0503 Acc: 98.6118\n",
      "test Loss: 0.1499 Acc: 96.9287\n",
      "\n",
      "Epoch 941/1199\n",
      "------------------------\n",
      "train Loss: 0.0480 Acc: 98.6816\n",
      "test Loss: 0.1513 Acc: 96.8603\n",
      "\n",
      "Epoch 942/1199\n",
      "------------------------\n",
      "train Loss: 0.0477 Acc: 98.6796\n",
      "test Loss: 0.1486 Acc: 96.9664\n",
      "\n",
      "Epoch 943/1199\n",
      "------------------------\n",
      "train Loss: 0.0478 Acc: 98.6830\n",
      "test Loss: 0.1528 Acc: 96.8738\n",
      "\n",
      "Epoch 944/1199\n",
      "------------------------\n",
      "train Loss: 0.0484 Acc: 98.6779\n",
      "test Loss: 0.1469 Acc: 96.9184\n",
      "\n",
      "Epoch 945/1199\n",
      "------------------------\n",
      "train Loss: 0.0480 Acc: 98.6741\n",
      "test Loss: 0.1491 Acc: 96.8647\n",
      "\n",
      "Epoch 946/1199\n",
      "------------------------\n",
      "train Loss: 0.0466 Acc: 98.7188\n",
      "test Loss: 0.1488 Acc: 96.9070\n",
      "\n",
      "Epoch 947/1199\n",
      "------------------------\n",
      "train Loss: 0.0476 Acc: 98.6796\n",
      "test Loss: 0.1460 Acc: 96.9476\n",
      "\n",
      "Epoch 948/1199\n",
      "------------------------\n",
      "train Loss: 0.0480 Acc: 98.6822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1490 Acc: 96.8802\n",
      "\n",
      "Epoch 949/1199\n",
      "------------------------\n",
      "train Loss: 0.0484 Acc: 98.6690\n",
      "test Loss: 0.1516 Acc: 96.8168\n",
      "\n",
      "Epoch 950/1199\n",
      "------------------------\n",
      "train Loss: 0.0495 Acc: 98.6510\n",
      "test Loss: 0.1486 Acc: 96.9355\n",
      "\n",
      "Epoch 951/1199\n",
      "------------------------\n",
      "train Loss: 0.0498 Acc: 98.6339\n",
      "test Loss: 0.1495 Acc: 96.8745\n",
      "\n",
      "Epoch 952/1199\n",
      "------------------------\n",
      "train Loss: 0.0471 Acc: 98.6922\n",
      "test Loss: 0.1550 Acc: 96.7985\n",
      "\n",
      "Epoch 953/1199\n",
      "------------------------\n",
      "train Loss: 0.0485 Acc: 98.6626\n",
      "test Loss: 0.1504 Acc: 96.8900\n",
      "\n",
      "Epoch 954/1199\n",
      "------------------------\n",
      "train Loss: 0.0491 Acc: 98.6461\n",
      "test Loss: 0.1520 Acc: 96.8997\n",
      "\n",
      "Epoch 955/1199\n",
      "------------------------\n",
      "train Loss: 0.0478 Acc: 98.6876\n",
      "test Loss: 0.1496 Acc: 96.9123\n",
      "\n",
      "Epoch 956/1199\n",
      "------------------------\n",
      "train Loss: 0.0477 Acc: 98.6845\n",
      "test Loss: 0.1486 Acc: 96.9873\n",
      "\n",
      "Epoch 957/1199\n",
      "------------------------\n",
      "train Loss: 0.0465 Acc: 98.7130\n",
      "test Loss: 0.1520 Acc: 96.8434\n",
      "\n",
      "Epoch 958/1199\n",
      "------------------------\n",
      "train Loss: 0.0464 Acc: 98.7214\n",
      "test Loss: 0.1517 Acc: 96.9558\n",
      "\n",
      "Epoch 959/1199\n",
      "------------------------\n",
      "train Loss: 0.0483 Acc: 98.6675\n",
      "test Loss: 0.1529 Acc: 96.8125\n",
      "\n",
      "Epoch 960/1199\n",
      "------------------------\n",
      "train Loss: 0.0463 Acc: 98.7110\n",
      "test Loss: 0.1515 Acc: 97.0118\n",
      "\n",
      "Epoch 961/1199\n",
      "------------------------\n",
      "train Loss: 0.0479 Acc: 98.6784\n",
      "test Loss: 0.1515 Acc: 96.8679\n",
      "\n",
      "Epoch 962/1199\n",
      "------------------------\n",
      "train Loss: 0.0492 Acc: 98.6465\n",
      "test Loss: 0.1491 Acc: 96.9071\n",
      "\n",
      "Epoch 963/1199\n",
      "------------------------\n",
      "train Loss: 0.0488 Acc: 98.6440\n",
      "test Loss: 0.1530 Acc: 96.8027\n",
      "\n",
      "Epoch 964/1199\n",
      "------------------------\n",
      "train Loss: 0.0489 Acc: 98.6444\n",
      "test Loss: 0.1554 Acc: 96.8705\n",
      "\n",
      "Epoch 965/1199\n",
      "------------------------\n",
      "train Loss: 0.0474 Acc: 98.6840\n",
      "test Loss: 0.1464 Acc: 96.9647\n",
      "\n",
      "Epoch 966/1199\n",
      "------------------------\n",
      "train Loss: 0.0499 Acc: 98.6213\n",
      "test Loss: 0.1471 Acc: 96.9697\n",
      "\n",
      "Epoch 967/1199\n",
      "------------------------\n",
      "train Loss: 0.0499 Acc: 98.6251\n",
      "test Loss: 0.1495 Acc: 96.9032\n",
      "\n",
      "Epoch 968/1199\n",
      "------------------------\n",
      "train Loss: 0.0468 Acc: 98.7091\n",
      "test Loss: 0.1548 Acc: 96.8096\n",
      "\n",
      "Epoch 969/1199\n",
      "------------------------\n",
      "train Loss: 0.0486 Acc: 98.6577\n",
      "test Loss: 0.1509 Acc: 96.9165\n",
      "\n",
      "Epoch 970/1199\n",
      "------------------------\n",
      "train Loss: 0.0468 Acc: 98.7064\n",
      "test Loss: 0.1516 Acc: 96.9274\n",
      "\n",
      "Epoch 971/1199\n",
      "------------------------\n",
      "train Loss: 0.0481 Acc: 98.6730\n",
      "test Loss: 0.1493 Acc: 96.9362\n",
      "\n",
      "Epoch 972/1199\n",
      "------------------------\n",
      "train Loss: 0.0461 Acc: 98.7167\n",
      "test Loss: 0.1530 Acc: 96.8550\n",
      "\n",
      "Epoch 973/1199\n",
      "------------------------\n",
      "train Loss: 0.0483 Acc: 98.6631\n",
      "test Loss: 0.1529 Acc: 96.8875\n",
      "\n",
      "Epoch 974/1199\n",
      "------------------------\n",
      "train Loss: 0.0466 Acc: 98.7065\n",
      "test Loss: 0.1484 Acc: 97.0070\n",
      "\n",
      "Epoch 975/1199\n",
      "------------------------\n",
      "train Loss: 0.0473 Acc: 98.7062\n",
      "test Loss: 0.1542 Acc: 96.8052\n",
      "\n",
      "Epoch 976/1199\n",
      "------------------------\n",
      "train Loss: 0.0494 Acc: 98.6391\n",
      "test Loss: 0.1499 Acc: 96.9662\n",
      "\n",
      "Epoch 977/1199\n",
      "------------------------\n",
      "train Loss: 0.0477 Acc: 98.6855\n",
      "test Loss: 0.1544 Acc: 96.9099\n",
      "\n",
      "Epoch 978/1199\n",
      "------------------------\n",
      "train Loss: 0.0474 Acc: 98.6856\n",
      "test Loss: 0.1578 Acc: 96.7412\n",
      "\n",
      "Epoch 979/1199\n",
      "------------------------\n",
      "train Loss: 0.0510 Acc: 98.5940\n",
      "test Loss: 0.1576 Acc: 96.8046\n",
      "\n",
      "Epoch 980/1199\n",
      "------------------------\n",
      "train Loss: 0.0468 Acc: 98.7093\n",
      "test Loss: 0.1504 Acc: 96.9648\n",
      "\n",
      "Epoch 981/1199\n",
      "------------------------\n",
      "train Loss: 0.0479 Acc: 98.6745\n",
      "test Loss: 0.1532 Acc: 96.8377\n",
      "\n",
      "Epoch 982/1199\n",
      "------------------------\n",
      "train Loss: 0.0460 Acc: 98.7304\n",
      "test Loss: 0.1498 Acc: 96.8790\n",
      "\n",
      "Epoch 983/1199\n",
      "------------------------\n",
      "train Loss: 0.0473 Acc: 98.7046\n",
      "test Loss: 0.1525 Acc: 96.8778\n",
      "\n",
      "Epoch 984/1199\n",
      "------------------------\n",
      "train Loss: 0.0492 Acc: 98.6359\n",
      "test Loss: 0.1503 Acc: 96.9396\n",
      "\n",
      "Epoch 985/1199\n",
      "------------------------\n",
      "train Loss: 0.0460 Acc: 98.7315\n",
      "test Loss: 0.1557 Acc: 96.7731\n",
      "\n",
      "Epoch 986/1199\n",
      "------------------------\n",
      "train Loss: 0.0470 Acc: 98.7059\n",
      "test Loss: 0.1480 Acc: 96.9833\n",
      "\n",
      "Epoch 987/1199\n",
      "------------------------\n",
      "train Loss: 0.0456 Acc: 98.7511\n",
      "test Loss: 0.1529 Acc: 96.8641\n",
      "\n",
      "Epoch 988/1199\n",
      "------------------------\n",
      "train Loss: 0.0471 Acc: 98.6975\n",
      "test Loss: 0.1490 Acc: 96.9638\n",
      "\n",
      "Epoch 989/1199\n",
      "------------------------\n",
      "train Loss: 0.0475 Acc: 98.6760\n",
      "test Loss: 0.1502 Acc: 96.9895\n",
      "\n",
      "Epoch 990/1199\n",
      "------------------------\n",
      "train Loss: 0.0459 Acc: 98.7405\n",
      "test Loss: 0.1489 Acc: 96.8432\n",
      "\n",
      "Epoch 991/1199\n",
      "------------------------\n",
      "train Loss: 0.0469 Acc: 98.6933\n",
      "test Loss: 0.1513 Acc: 96.9115\n",
      "\n",
      "Epoch 992/1199\n",
      "------------------------\n",
      "train Loss: 0.0452 Acc: 98.7419\n",
      "test Loss: 0.1590 Acc: 96.6959\n",
      "\n",
      "Epoch 993/1199\n",
      "------------------------\n",
      "train Loss: 0.0470 Acc: 98.7132\n",
      "test Loss: 0.1484 Acc: 96.9593\n",
      "\n",
      "Epoch 994/1199\n",
      "------------------------\n",
      "train Loss: 0.0492 Acc: 98.6383\n",
      "test Loss: 0.1499 Acc: 96.9061\n",
      "\n",
      "Epoch 995/1199\n",
      "------------------------\n",
      "train Loss: 0.0474 Acc: 98.6849\n",
      "test Loss: 0.1555 Acc: 96.8158\n",
      "\n",
      "Epoch 996/1199\n",
      "------------------------\n",
      "train Loss: 0.0487 Acc: 98.6720\n",
      "test Loss: 0.1507 Acc: 96.8742\n",
      "\n",
      "Epoch 997/1199\n",
      "------------------------\n",
      "train Loss: 0.0488 Acc: 98.6570\n",
      "test Loss: 0.1528 Acc: 96.9165\n",
      "\n",
      "Epoch 998/1199\n",
      "------------------------\n",
      "train Loss: 0.0449 Acc: 98.7528\n",
      "test Loss: 0.1505 Acc: 96.9161\n",
      "\n",
      "Epoch 999/1199\n",
      "------------------------\n",
      "train Loss: 0.0464 Acc: 98.7298\n",
      "test Loss: 0.1471 Acc: 97.0001\n",
      "\n",
      "Epoch 1000/1199\n",
      "------------------------\n",
      "train Loss: 0.0472 Acc: 98.6981\n",
      "test Loss: 0.1503 Acc: 96.9225\n",
      "\n",
      "Epoch 1001/1199\n",
      "------------------------\n",
      "train Loss: 0.0498 Acc: 98.6264\n",
      "test Loss: 0.1548 Acc: 96.8051\n",
      "\n",
      "Epoch 1002/1199\n",
      "------------------------\n",
      "train Loss: 0.0468 Acc: 98.7094\n",
      "test Loss: 0.1545 Acc: 96.8374\n",
      "\n",
      "Epoch 1003/1199\n",
      "------------------------\n",
      "train Loss: 0.0479 Acc: 98.6698\n",
      "test Loss: 0.1532 Acc: 96.9488\n",
      "\n",
      "Epoch 1004/1199\n",
      "------------------------\n",
      "train Loss: 0.0460 Acc: 98.7182\n",
      "test Loss: 0.1561 Acc: 96.7375\n",
      "\n",
      "Epoch 1005/1199\n",
      "------------------------\n",
      "train Loss: 0.0497 Acc: 98.6295\n",
      "test Loss: 0.1508 Acc: 96.9543\n",
      "\n",
      "Epoch 1006/1199\n",
      "------------------------\n",
      "train Loss: 0.0467 Acc: 98.7062\n",
      "test Loss: 0.1524 Acc: 96.9767\n",
      "\n",
      "Epoch 1007/1199\n",
      "------------------------\n",
      "train Loss: 0.0459 Acc: 98.7337\n",
      "test Loss: 0.1569 Acc: 96.7875\n",
      "\n",
      "Epoch 1008/1199\n",
      "------------------------\n",
      "train Loss: 0.0471 Acc: 98.6970\n",
      "test Loss: 0.1485 Acc: 96.9757\n",
      "\n",
      "Epoch 1009/1199\n",
      "------------------------\n",
      "train Loss: 0.0471 Acc: 98.7032\n",
      "test Loss: 0.1515 Acc: 96.8493\n",
      "\n",
      "Epoch 1010/1199\n",
      "------------------------\n",
      "train Loss: 0.0457 Acc: 98.7333\n",
      "test Loss: 0.1532 Acc: 96.8227\n",
      "\n",
      "Epoch 1011/1199\n",
      "------------------------\n",
      "train Loss: 0.0465 Acc: 98.7165\n",
      "test Loss: 0.1596 Acc: 96.7907\n",
      "\n",
      "Epoch 1012/1199\n",
      "------------------------\n",
      "train Loss: 0.0471 Acc: 98.7052\n",
      "test Loss: 0.1512 Acc: 96.9230\n",
      "\n",
      "Epoch 1013/1199\n",
      "------------------------\n",
      "train Loss: 0.0474 Acc: 98.6892\n",
      "test Loss: 0.1498 Acc: 96.9498\n",
      "\n",
      "Epoch 1014/1199\n",
      "------------------------\n",
      "train Loss: 0.0469 Acc: 98.7193\n",
      "test Loss: 0.1526 Acc: 96.9251\n",
      "\n",
      "Epoch 1015/1199\n",
      "------------------------\n",
      "train Loss: 0.0470 Acc: 98.6914\n",
      "test Loss: 0.1527 Acc: 96.8199\n",
      "\n",
      "Epoch 1016/1199\n",
      "------------------------\n",
      "train Loss: 0.0460 Acc: 98.7151\n",
      "test Loss: 0.1522 Acc: 97.0089\n",
      "\n",
      "Epoch 1017/1199\n",
      "------------------------\n",
      "train Loss: 0.0454 Acc: 98.7449\n",
      "test Loss: 0.1469 Acc: 96.9401\n",
      "\n",
      "Epoch 1018/1199\n",
      "------------------------\n",
      "train Loss: 0.0477 Acc: 98.6799\n",
      "test Loss: 0.1524 Acc: 96.9824\n",
      "\n",
      "Epoch 1019/1199\n",
      "------------------------\n",
      "train Loss: 0.0464 Acc: 98.7098\n",
      "test Loss: 0.1531 Acc: 96.7866\n",
      "\n",
      "Epoch 1020/1199\n",
      "------------------------\n",
      "train Loss: 0.0487 Acc: 98.6669\n",
      "test Loss: 0.1541 Acc: 96.9441\n",
      "\n",
      "Epoch 1021/1199\n",
      "------------------------\n",
      "train Loss: 0.0463 Acc: 98.7127\n",
      "test Loss: 0.1530 Acc: 96.8204\n",
      "\n",
      "Epoch 1022/1199\n",
      "------------------------\n",
      "train Loss: 0.0471 Acc: 98.6987\n",
      "test Loss: 0.1522 Acc: 96.8989\n",
      "\n",
      "Epoch 1023/1199\n",
      "------------------------\n",
      "train Loss: 0.0459 Acc: 98.7314\n",
      "test Loss: 0.1564 Acc: 96.8707\n",
      "\n",
      "Epoch 1024/1199\n",
      "------------------------\n",
      "train Loss: 0.0454 Acc: 98.7421\n",
      "test Loss: 0.1506 Acc: 96.9472\n",
      "\n",
      "Epoch 1025/1199\n",
      "------------------------\n",
      "train Loss: 0.0453 Acc: 98.7475\n",
      "test Loss: 0.1507 Acc: 97.0161\n",
      "\n",
      "Epoch 1026/1199\n",
      "------------------------\n",
      "train Loss: 0.0460 Acc: 98.7170\n",
      "test Loss: 0.1553 Acc: 96.9066\n",
      "\n",
      "Epoch 1027/1199\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0477 Acc: 98.6758\n",
      "test Loss: 0.1527 Acc: 96.9740\n",
      "\n",
      "Epoch 1028/1199\n",
      "------------------------\n",
      "train Loss: 0.0482 Acc: 98.6774\n",
      "test Loss: 0.1511 Acc: 96.9424\n",
      "\n",
      "Epoch 1029/1199\n",
      "------------------------\n",
      "train Loss: 0.0460 Acc: 98.7194\n",
      "test Loss: 0.1536 Acc: 96.8213\n",
      "\n",
      "Epoch 1030/1199\n",
      "------------------------\n",
      "train Loss: 0.0487 Acc: 98.6458\n",
      "test Loss: 0.1561 Acc: 96.8180\n",
      "\n",
      "Epoch 1031/1199\n",
      "------------------------\n",
      "train Loss: 0.0443 Acc: 98.7679\n",
      "test Loss: 0.1557 Acc: 96.9436\n",
      "\n",
      "Epoch 1032/1199\n",
      "------------------------\n",
      "train Loss: 0.0460 Acc: 98.7253\n",
      "test Loss: 0.1495 Acc: 96.9795\n",
      "\n",
      "Epoch 1033/1199\n",
      "------------------------\n",
      "train Loss: 0.0465 Acc: 98.7233\n",
      "test Loss: 0.1499 Acc: 96.9605\n",
      "\n",
      "Epoch 1034/1199\n",
      "------------------------\n",
      "train Loss: 0.0459 Acc: 98.7331\n",
      "test Loss: 0.1540 Acc: 96.8826\n",
      "\n",
      "Epoch 1035/1199\n",
      "------------------------\n",
      "train Loss: 0.0443 Acc: 98.7759\n",
      "test Loss: 0.1479 Acc: 96.9412\n",
      "\n",
      "Epoch 1036/1199\n",
      "------------------------\n",
      "train Loss: 0.0502 Acc: 98.6211\n",
      "test Loss: 0.1515 Acc: 96.9824\n",
      "\n",
      "Epoch 1037/1199\n",
      "------------------------\n",
      "train Loss: 0.0532 Acc: 98.5501\n",
      "test Loss: 0.1540 Acc: 96.8783\n",
      "\n",
      "Epoch 1038/1199\n",
      "------------------------\n",
      "train Loss: 0.0488 Acc: 98.6590\n",
      "test Loss: 0.1539 Acc: 96.8788\n",
      "\n",
      "Epoch 1039/1199\n",
      "------------------------\n",
      "train Loss: 0.0461 Acc: 98.7209\n",
      "test Loss: 0.1559 Acc: 96.8251\n",
      "\n",
      "Epoch 1040/1199\n",
      "------------------------\n",
      "train Loss: 0.0494 Acc: 98.6266\n",
      "test Loss: 0.1515 Acc: 96.9127\n",
      "\n",
      "Epoch 1041/1199\n",
      "------------------------\n",
      "train Loss: 0.0488 Acc: 98.6465\n",
      "test Loss: 0.1494 Acc: 96.9750\n",
      "\n",
      "Epoch 1042/1199\n",
      "------------------------\n",
      "train Loss: 0.0470 Acc: 98.7032\n",
      "test Loss: 0.1546 Acc: 96.8389\n",
      "\n",
      "Epoch 1043/1199\n",
      "------------------------\n",
      "train Loss: 0.0466 Acc: 98.7315\n",
      "test Loss: 0.1495 Acc: 96.9457\n",
      "\n",
      "Epoch 1044/1199\n",
      "------------------------\n",
      "train Loss: 0.0449 Acc: 98.7485\n",
      "test Loss: 0.1518 Acc: 96.9742\n",
      "\n",
      "Epoch 1045/1199\n",
      "------------------------\n",
      "train Loss: 0.0458 Acc: 98.7351\n",
      "test Loss: 0.1508 Acc: 96.8921\n",
      "\n",
      "Epoch 1046/1199\n",
      "------------------------\n",
      "train Loss: 0.0469 Acc: 98.7137\n",
      "test Loss: 0.1507 Acc: 96.9051\n",
      "\n",
      "Epoch 1047/1199\n",
      "------------------------\n",
      "train Loss: 0.0528 Acc: 98.5453\n",
      "test Loss: 0.1530 Acc: 96.9108\n",
      "\n",
      "Epoch 1048/1199\n",
      "------------------------\n",
      "train Loss: 0.0475 Acc: 98.6892\n",
      "test Loss: 0.1467 Acc: 96.9626\n",
      "\n",
      "Epoch 1049/1199\n",
      "------------------------\n",
      "train Loss: 0.0481 Acc: 98.6799\n",
      "test Loss: 0.1546 Acc: 96.8584\n",
      "\n",
      "Epoch 1050/1199\n",
      "------------------------\n",
      "train Loss: 0.0492 Acc: 98.6546\n",
      "test Loss: 0.1514 Acc: 96.9016\n",
      "\n",
      "Epoch 1051/1199\n",
      "------------------------\n",
      "train Loss: 0.0462 Acc: 98.7285\n",
      "test Loss: 0.1527 Acc: 96.8291\n",
      "\n",
      "Epoch 1052/1199\n",
      "------------------------\n",
      "train Loss: 0.0470 Acc: 98.7123\n",
      "test Loss: 0.1546 Acc: 96.9187\n",
      "\n",
      "Epoch 1053/1199\n",
      "------------------------\n",
      "train Loss: 0.0456 Acc: 98.7371\n",
      "test Loss: 0.1539 Acc: 96.8241\n",
      "\n",
      "Epoch 1054/1199\n",
      "------------------------\n",
      "train Loss: 0.0472 Acc: 98.6956\n",
      "test Loss: 0.1520 Acc: 96.8565\n",
      "\n",
      "Epoch 1055/1199\n",
      "------------------------\n",
      "train Loss: 0.0468 Acc: 98.7198\n",
      "test Loss: 0.1521 Acc: 96.9142\n",
      "\n",
      "Epoch 1056/1199\n",
      "------------------------\n",
      "train Loss: 0.0456 Acc: 98.7407\n",
      "test Loss: 0.1475 Acc: 96.9809\n",
      "\n",
      "Epoch 1057/1199\n",
      "------------------------\n",
      "train Loss: 0.0482 Acc: 98.6695\n",
      "test Loss: 0.1535 Acc: 96.9716\n",
      "\n",
      "Epoch 1058/1199\n",
      "------------------------\n",
      "train Loss: 0.0462 Acc: 98.7137\n",
      "test Loss: 0.1518 Acc: 96.8616\n",
      "\n",
      "Epoch 1059/1199\n",
      "------------------------\n",
      "train Loss: 0.0478 Acc: 98.6923\n",
      "test Loss: 0.1489 Acc: 96.8976\n",
      "\n",
      "Epoch 1060/1199\n",
      "------------------------\n",
      "train Loss: 0.0477 Acc: 98.6878\n",
      "test Loss: 0.1521 Acc: 96.9211\n",
      "\n",
      "Epoch 1061/1199\n",
      "------------------------\n",
      "train Loss: 0.0463 Acc: 98.7220\n",
      "test Loss: 0.1525 Acc: 96.9296\n",
      "\n",
      "Epoch 1062/1199\n",
      "------------------------\n",
      "train Loss: 0.0495 Acc: 98.6355\n",
      "test Loss: 0.1520 Acc: 96.8496\n",
      "\n",
      "Epoch 1063/1199\n",
      "------------------------\n",
      "train Loss: 0.0441 Acc: 98.7837\n",
      "test Loss: 0.1563 Acc: 96.7840\n",
      "\n",
      "Epoch 1064/1199\n",
      "------------------------\n",
      "train Loss: 0.0457 Acc: 98.7377\n",
      "test Loss: 0.1511 Acc: 96.9248\n",
      "\n",
      "Epoch 1065/1199\n",
      "------------------------\n",
      "train Loss: 0.0478 Acc: 98.6735\n",
      "test Loss: 0.1534 Acc: 96.9001\n",
      "\n",
      "Epoch 1066/1199\n",
      "------------------------\n",
      "train Loss: 0.0457 Acc: 98.7277\n",
      "test Loss: 0.1510 Acc: 96.9831\n",
      "\n",
      "Epoch 1067/1199\n",
      "------------------------\n",
      "train Loss: 0.0468 Acc: 98.7038\n",
      "test Loss: 0.1545 Acc: 96.8051\n",
      "\n",
      "Epoch 1068/1199\n",
      "------------------------\n",
      "train Loss: 0.0466 Acc: 98.7116\n",
      "test Loss: 0.1532 Acc: 96.8819\n",
      "\n",
      "Epoch 1069/1199\n",
      "------------------------\n",
      "train Loss: 0.0483 Acc: 98.6682\n",
      "test Loss: 0.1565 Acc: 96.9073\n",
      "\n",
      "Epoch 1070/1199\n",
      "------------------------\n",
      "train Loss: 0.0452 Acc: 98.7486\n",
      "test Loss: 0.1541 Acc: 96.8830\n",
      "\n",
      "Epoch 1071/1199\n",
      "------------------------\n",
      "train Loss: 0.0458 Acc: 98.7296\n",
      "test Loss: 0.1535 Acc: 96.8926\n",
      "\n",
      "Epoch 1072/1199\n",
      "------------------------\n",
      "train Loss: 0.0464 Acc: 98.7196\n",
      "test Loss: 0.1491 Acc: 96.9659\n",
      "\n",
      "Epoch 1073/1199\n",
      "------------------------\n",
      "train Loss: 0.0482 Acc: 98.6694\n",
      "test Loss: 0.1543 Acc: 96.8885\n",
      "\n",
      "Epoch 1074/1199\n",
      "------------------------\n",
      "train Loss: 0.0475 Acc: 98.6948\n",
      "test Loss: 0.1556 Acc: 96.8213\n",
      "\n",
      "Epoch 1075/1199\n",
      "------------------------\n",
      "train Loss: 0.0469 Acc: 98.6999\n",
      "test Loss: 0.1503 Acc: 96.9522\n",
      "\n",
      "Epoch 1076/1199\n",
      "------------------------\n",
      "train Loss: 0.0452 Acc: 98.7469\n",
      "test Loss: 0.1525 Acc: 96.9448\n",
      "\n",
      "Epoch 1077/1199\n",
      "------------------------\n",
      "train Loss: 0.0463 Acc: 98.7179\n",
      "test Loss: 0.1495 Acc: 96.9759\n",
      "\n",
      "Epoch 1078/1199\n",
      "------------------------\n",
      "train Loss: 0.0468 Acc: 98.6978\n",
      "test Loss: 0.1533 Acc: 97.0351\n",
      "\n",
      "Epoch 1079/1199\n",
      "------------------------\n",
      "train Loss: 0.0453 Acc: 98.7384\n",
      "test Loss: 0.1534 Acc: 96.9306\n",
      "\n",
      "Epoch 1080/1199\n",
      "------------------------\n",
      "train Loss: 0.0454 Acc: 98.7375\n",
      "test Loss: 0.1556 Acc: 96.8374\n",
      "\n",
      "Epoch 1081/1199\n",
      "------------------------\n",
      "train Loss: 0.0454 Acc: 98.7438\n",
      "test Loss: 0.1509 Acc: 96.8726\n",
      "\n",
      "Epoch 1082/1199\n",
      "------------------------\n",
      "train Loss: 0.0465 Acc: 98.7184\n",
      "test Loss: 0.1487 Acc: 96.8847\n",
      "\n",
      "Epoch 1083/1199\n",
      "------------------------\n",
      "train Loss: 0.0451 Acc: 98.7581\n",
      "test Loss: 0.1555 Acc: 96.8306\n",
      "\n",
      "Epoch 1084/1199\n",
      "------------------------\n",
      "train Loss: 0.0466 Acc: 98.7192\n",
      "test Loss: 0.1543 Acc: 96.8793\n",
      "\n",
      "Epoch 1085/1199\n",
      "------------------------\n",
      "train Loss: 0.0452 Acc: 98.7467\n",
      "test Loss: 0.1521 Acc: 96.8861\n",
      "\n",
      "Epoch 1086/1199\n",
      "------------------------\n",
      "train Loss: 0.0451 Acc: 98.7505\n",
      "test Loss: 0.1562 Acc: 96.9629\n",
      "\n",
      "Epoch 1087/1199\n",
      "------------------------\n",
      "train Loss: 0.0459 Acc: 98.7269\n",
      "test Loss: 0.1559 Acc: 96.8223\n",
      "\n",
      "Epoch 1088/1199\n",
      "------------------------\n",
      "train Loss: 0.0460 Acc: 98.7185\n",
      "test Loss: 0.1547 Acc: 96.9270\n",
      "\n",
      "Epoch 1089/1199\n",
      "------------------------\n",
      "train Loss: 0.0445 Acc: 98.7581\n",
      "test Loss: 0.1530 Acc: 96.9246\n",
      "\n",
      "Epoch 1090/1199\n",
      "------------------------\n",
      "train Loss: 0.0463 Acc: 98.7261\n",
      "test Loss: 0.1538 Acc: 96.9111\n",
      "\n",
      "Epoch 1091/1199\n",
      "------------------------\n",
      "train Loss: 0.0461 Acc: 98.7230\n",
      "test Loss: 0.1523 Acc: 96.9189\n",
      "\n",
      "Epoch 1092/1199\n",
      "------------------------\n",
      "train Loss: 0.0455 Acc: 98.7397\n",
      "test Loss: 0.1582 Acc: 96.7719\n",
      "\n",
      "Epoch 1093/1199\n",
      "------------------------\n",
      "train Loss: 0.0469 Acc: 98.7009\n",
      "test Loss: 0.1505 Acc: 96.9336\n",
      "\n",
      "Epoch 1094/1199\n",
      "------------------------\n",
      "train Loss: 0.0441 Acc: 98.7756\n",
      "test Loss: 0.1552 Acc: 96.8312\n",
      "\n",
      "Epoch 1095/1199\n",
      "------------------------\n",
      "train Loss: 0.0438 Acc: 98.7815\n",
      "test Loss: 0.1533 Acc: 96.9002\n",
      "\n",
      "Epoch 1096/1199\n",
      "------------------------\n",
      "train Loss: 0.0425 Acc: 98.8156\n",
      "test Loss: 0.1534 Acc: 96.9367\n",
      "\n",
      "Epoch 1097/1199\n",
      "------------------------\n",
      "train Loss: 0.0476 Acc: 98.6852\n",
      "test Loss: 0.1483 Acc: 97.0044\n",
      "\n",
      "Epoch 1098/1199\n",
      "------------------------\n",
      "train Loss: 0.0476 Acc: 98.6859\n",
      "test Loss: 0.1555 Acc: 96.8997\n",
      "\n",
      "Epoch 1099/1199\n",
      "------------------------\n",
      "train Loss: 0.0455 Acc: 98.7446\n",
      "test Loss: 0.1514 Acc: 96.8809\n",
      "\n",
      "Epoch 1100/1199\n",
      "------------------------\n",
      "train Loss: 0.0457 Acc: 98.7395\n",
      "test Loss: 0.1527 Acc: 96.9268\n",
      "\n",
      "Epoch 1101/1199\n",
      "------------------------\n",
      "train Loss: 0.0458 Acc: 98.7420\n",
      "test Loss: 0.1520 Acc: 96.9417\n",
      "\n",
      "Epoch 1102/1199\n",
      "------------------------\n",
      "train Loss: 0.0463 Acc: 98.7150\n",
      "test Loss: 0.1505 Acc: 96.9229\n",
      "\n",
      "Epoch 1103/1199\n",
      "------------------------\n",
      "train Loss: 0.0473 Acc: 98.6894\n",
      "test Loss: 0.1510 Acc: 96.9104\n",
      "\n",
      "Epoch 1104/1199\n",
      "------------------------\n",
      "train Loss: 0.0459 Acc: 98.7327\n",
      "test Loss: 0.1545 Acc: 96.9710\n",
      "\n",
      "Epoch 1105/1199\n",
      "------------------------\n",
      "train Loss: 0.0447 Acc: 98.7604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 0.1504 Acc: 96.9887\n",
      "\n",
      "Epoch 1106/1199\n",
      "------------------------\n",
      "train Loss: 0.0458 Acc: 98.7198\n",
      "test Loss: 0.1525 Acc: 96.8600\n",
      "\n",
      "Epoch 1107/1199\n",
      "------------------------\n",
      "train Loss: 0.0442 Acc: 98.7663\n",
      "test Loss: 0.1552 Acc: 96.9289\n",
      "\n",
      "Epoch 1108/1199\n",
      "------------------------\n",
      "train Loss: 0.0456 Acc: 98.7333\n",
      "test Loss: 0.1530 Acc: 96.9581\n",
      "\n",
      "Epoch 1109/1199\n",
      "------------------------\n",
      "train Loss: 0.0465 Acc: 98.7142\n",
      "test Loss: 0.1578 Acc: 96.7593\n",
      "\n",
      "Epoch 1110/1199\n",
      "------------------------\n",
      "train Loss: 0.0458 Acc: 98.7320\n",
      "test Loss: 0.1559 Acc: 96.9113\n",
      "\n",
      "Epoch 1111/1199\n",
      "------------------------\n",
      "train Loss: 0.0472 Acc: 98.6883\n",
      "test Loss: 0.1532 Acc: 96.8576\n",
      "\n",
      "Epoch 1112/1199\n",
      "------------------------\n",
      "train Loss: 0.0442 Acc: 98.7715\n",
      "test Loss: 0.1477 Acc: 97.0527\n",
      "\n",
      "Epoch 1113/1199\n",
      "------------------------\n",
      "train Loss: 0.0455 Acc: 98.7314\n",
      "test Loss: 0.1520 Acc: 96.9964\n",
      "\n",
      "Epoch 1114/1199\n",
      "------------------------\n",
      "train Loss: 0.0462 Acc: 98.6980\n",
      "test Loss: 0.1546 Acc: 96.9092\n",
      "\n",
      "Epoch 1115/1199\n",
      "------------------------\n",
      "train Loss: 0.0454 Acc: 98.7485\n",
      "test Loss: 0.1492 Acc: 96.9051\n",
      "\n",
      "Epoch 1116/1199\n",
      "------------------------\n",
      "train Loss: 0.0461 Acc: 98.7286\n",
      "test Loss: 0.1549 Acc: 96.9021\n",
      "\n",
      "Epoch 1117/1199\n",
      "------------------------\n",
      "train Loss: 0.0458 Acc: 98.7341\n",
      "test Loss: 0.1570 Acc: 96.9032\n",
      "\n",
      "Epoch 1118/1199\n",
      "------------------------\n",
      "train Loss: 0.0437 Acc: 98.7872\n",
      "test Loss: 0.1541 Acc: 96.8681\n",
      "\n",
      "Epoch 1119/1199\n",
      "------------------------\n",
      "train Loss: 0.0453 Acc: 98.7425\n",
      "test Loss: 0.1529 Acc: 96.9503\n",
      "\n",
      "Epoch 1120/1199\n",
      "------------------------\n",
      "train Loss: 0.0457 Acc: 98.7356\n",
      "test Loss: 0.1530 Acc: 96.9450\n",
      "\n",
      "Epoch 1121/1199\n",
      "------------------------\n",
      "train Loss: 0.0449 Acc: 98.7594\n",
      "test Loss: 0.1524 Acc: 96.8436\n",
      "\n",
      "Epoch 1122/1199\n",
      "------------------------\n",
      "train Loss: 0.0465 Acc: 98.7206\n",
      "test Loss: 0.1543 Acc: 96.9565\n",
      "\n",
      "Epoch 1123/1199\n",
      "------------------------\n",
      "train Loss: 0.0447 Acc: 98.7520\n",
      "test Loss: 0.1548 Acc: 96.8719\n",
      "\n",
      "Epoch 1124/1199\n",
      "------------------------\n",
      "train Loss: 0.0463 Acc: 98.7161\n",
      "test Loss: 0.1548 Acc: 96.8635\n",
      "\n",
      "Epoch 1125/1199\n",
      "------------------------\n",
      "train Loss: 0.0463 Acc: 98.7211\n",
      "test Loss: 0.1586 Acc: 96.8230\n",
      "\n",
      "Epoch 1126/1199\n",
      "------------------------\n",
      "train Loss: 0.0455 Acc: 98.7223\n",
      "test Loss: 0.1505 Acc: 96.9256\n",
      "\n",
      "Epoch 1127/1199\n",
      "------------------------\n",
      "train Loss: 0.0461 Acc: 98.7324\n",
      "test Loss: 0.1513 Acc: 96.8989\n",
      "\n",
      "Epoch 1128/1199\n",
      "------------------------\n",
      "train Loss: 0.0435 Acc: 98.7816\n",
      "test Loss: 0.1564 Acc: 96.8640\n",
      "\n",
      "Epoch 1129/1199\n",
      "------------------------\n",
      "train Loss: 0.0457 Acc: 98.7322\n",
      "test Loss: 0.1556 Acc: 96.8660\n",
      "\n",
      "Epoch 1130/1199\n",
      "------------------------\n",
      "train Loss: 0.0459 Acc: 98.7341\n",
      "test Loss: 0.1524 Acc: 96.9196\n",
      "\n",
      "Epoch 1131/1199\n",
      "------------------------\n",
      "train Loss: 0.0468 Acc: 98.7069\n",
      "test Loss: 0.1517 Acc: 96.9733\n",
      "\n",
      "Epoch 1132/1199\n",
      "------------------------\n",
      "train Loss: 0.0462 Acc: 98.7200\n",
      "test Loss: 0.1526 Acc: 96.9092\n",
      "\n",
      "Epoch 1133/1199\n",
      "------------------------\n",
      "train Loss: 0.0456 Acc: 98.7327\n",
      "test Loss: 0.1547 Acc: 96.8892\n",
      "\n",
      "Epoch 1134/1199\n",
      "------------------------\n",
      "train Loss: 0.0443 Acc: 98.7670\n",
      "test Loss: 0.1560 Acc: 96.8477\n",
      "\n",
      "Epoch 1135/1199\n",
      "------------------------\n",
      "train Loss: 0.0465 Acc: 98.7068\n",
      "test Loss: 0.1493 Acc: 96.9823\n",
      "\n",
      "Epoch 1136/1199\n",
      "------------------------\n",
      "train Loss: 0.0443 Acc: 98.7662\n",
      "test Loss: 0.1528 Acc: 96.9066\n",
      "\n",
      "Epoch 1137/1199\n",
      "------------------------\n",
      "train Loss: 0.0455 Acc: 98.7473\n",
      "test Loss: 0.1589 Acc: 96.8849\n",
      "\n",
      "Epoch 1138/1199\n",
      "------------------------\n",
      "train Loss: 0.0434 Acc: 98.7969\n",
      "test Loss: 0.1566 Acc: 96.8508\n",
      "\n",
      "Epoch 1139/1199\n",
      "------------------------\n",
      "train Loss: 0.0444 Acc: 98.7744\n",
      "test Loss: 0.1544 Acc: 96.9199\n",
      "\n",
      "Epoch 1140/1199\n",
      "------------------------\n",
      "train Loss: 0.0459 Acc: 98.7306\n",
      "test Loss: 0.1546 Acc: 96.8451\n",
      "\n",
      "Epoch 1141/1199\n",
      "------------------------\n",
      "train Loss: 0.0464 Acc: 98.7036\n",
      "test Loss: 0.1568 Acc: 96.8571\n",
      "\n",
      "Epoch 1142/1199\n",
      "------------------------\n",
      "train Loss: 0.0486 Acc: 98.6357\n",
      "test Loss: 0.1485 Acc: 97.0039\n",
      "\n",
      "Epoch 1143/1199\n",
      "------------------------\n",
      "train Loss: 0.0468 Acc: 98.6982\n",
      "test Loss: 0.1589 Acc: 96.8130\n",
      "\n",
      "Epoch 1144/1199\n",
      "------------------------\n",
      "train Loss: 0.0465 Acc: 98.7276\n",
      "test Loss: 0.1522 Acc: 97.0472\n",
      "\n",
      "Epoch 1145/1199\n",
      "------------------------\n",
      "train Loss: 0.0440 Acc: 98.7754\n",
      "test Loss: 0.1590 Acc: 96.7434\n",
      "\n",
      "Epoch 1146/1199\n",
      "------------------------\n",
      "train Loss: 0.0448 Acc: 98.7564\n",
      "test Loss: 0.1549 Acc: 96.9111\n",
      "\n",
      "Epoch 1147/1199\n",
      "------------------------\n",
      "train Loss: 0.0462 Acc: 98.7351\n",
      "test Loss: 0.1483 Acc: 96.9895\n",
      "\n",
      "Epoch 1148/1199\n",
      "------------------------\n",
      "train Loss: 0.0458 Acc: 98.7306\n",
      "test Loss: 0.1523 Acc: 96.9410\n",
      "\n",
      "Epoch 1149/1199\n",
      "------------------------\n",
      "train Loss: 0.0454 Acc: 98.7356\n",
      "test Loss: 0.1528 Acc: 96.9700\n",
      "\n",
      "Epoch 1150/1199\n",
      "------------------------\n",
      "train Loss: 0.0478 Acc: 98.6725\n",
      "test Loss: 0.1597 Acc: 96.8313\n",
      "\n",
      "Epoch 1151/1199\n",
      "------------------------\n",
      "train Loss: 0.0430 Acc: 98.8173\n",
      "test Loss: 0.1550 Acc: 96.9099\n",
      "\n",
      "Epoch 1152/1199\n",
      "------------------------\n",
      "train Loss: 0.0436 Acc: 98.7940\n",
      "test Loss: 0.1543 Acc: 96.9615\n",
      "\n",
      "Epoch 1153/1199\n",
      "------------------------\n",
      "train Loss: 0.0446 Acc: 98.7677\n",
      "test Loss: 0.1506 Acc: 96.9541\n",
      "\n",
      "Epoch 1154/1199\n",
      "------------------------\n",
      "train Loss: 0.0430 Acc: 98.7980\n",
      "test Loss: 0.1535 Acc: 96.8508\n",
      "\n",
      "Epoch 1155/1199\n",
      "------------------------\n",
      "train Loss: 0.0441 Acc: 98.7878\n",
      "test Loss: 0.1559 Acc: 96.9479\n",
      "\n",
      "Epoch 1156/1199\n",
      "------------------------\n",
      "train Loss: 0.0462 Acc: 98.7148\n",
      "test Loss: 0.1547 Acc: 96.9125\n",
      "\n",
      "Epoch 1157/1199\n",
      "------------------------\n",
      "train Loss: 0.0440 Acc: 98.7788\n",
      "test Loss: 0.1538 Acc: 96.9666\n",
      "\n",
      "Epoch 1158/1199\n",
      "------------------------\n",
      "train Loss: 0.0422 Acc: 98.8316\n",
      "test Loss: 0.1563 Acc: 96.8712\n",
      "\n",
      "Epoch 1159/1199\n",
      "------------------------\n",
      "train Loss: 0.0466 Acc: 98.7095\n",
      "test Loss: 0.1558 Acc: 96.8881\n",
      "\n",
      "Epoch 1160/1199\n",
      "------------------------\n",
      "train Loss: 0.0464 Acc: 98.7123\n",
      "test Loss: 0.1525 Acc: 96.9686\n",
      "\n",
      "Epoch 1161/1199\n",
      "------------------------\n",
      "train Loss: 0.0470 Acc: 98.7022\n",
      "test Loss: 0.1577 Acc: 96.9199\n",
      "\n",
      "Epoch 1162/1199\n",
      "------------------------\n",
      "train Loss: 0.0456 Acc: 98.7380\n",
      "test Loss: 0.1547 Acc: 96.8847\n",
      "\n",
      "Epoch 1163/1199\n",
      "------------------------\n",
      "train Loss: 0.0464 Acc: 98.7145\n",
      "test Loss: 0.1559 Acc: 96.8833\n",
      "\n",
      "Epoch 1164/1199\n",
      "------------------------\n",
      "train Loss: 0.0456 Acc: 98.7382\n",
      "test Loss: 0.1578 Acc: 96.8552\n",
      "\n",
      "Epoch 1165/1199\n",
      "------------------------\n",
      "train Loss: 0.0454 Acc: 98.7352\n",
      "test Loss: 0.1537 Acc: 97.0020\n",
      "\n",
      "Epoch 1166/1199\n",
      "------------------------\n",
      "train Loss: 0.0468 Acc: 98.7097\n",
      "test Loss: 0.1527 Acc: 96.9890\n",
      "\n",
      "Epoch 1167/1199\n",
      "------------------------\n",
      "train Loss: 0.0438 Acc: 98.7865\n",
      "test Loss: 0.1518 Acc: 96.9453\n",
      "\n",
      "Epoch 1168/1199\n",
      "------------------------\n",
      "train Loss: 0.0476 Acc: 98.6833\n",
      "test Loss: 0.1534 Acc: 96.8729\n",
      "\n",
      "Epoch 1169/1199\n",
      "------------------------\n",
      "train Loss: 0.0435 Acc: 98.7907\n",
      "test Loss: 0.1498 Acc: 97.0298\n",
      "\n",
      "Epoch 1170/1199\n",
      "------------------------\n",
      "train Loss: 0.0445 Acc: 98.7587\n",
      "test Loss: 0.1567 Acc: 96.9467\n",
      "\n",
      "Epoch 1171/1199\n",
      "------------------------\n",
      "train Loss: 0.0432 Acc: 98.7993\n",
      "test Loss: 0.1539 Acc: 96.8966\n",
      "\n",
      "Epoch 1172/1199\n",
      "------------------------\n",
      "train Loss: 0.0444 Acc: 98.7673\n",
      "test Loss: 0.1543 Acc: 96.8714\n",
      "\n",
      "Epoch 1173/1199\n",
      "------------------------\n",
      "train Loss: 0.0440 Acc: 98.7827\n",
      "test Loss: 0.1544 Acc: 96.8524\n",
      "\n",
      "Epoch 1174/1199\n",
      "------------------------\n",
      "train Loss: 0.0463 Acc: 98.7091\n",
      "test Loss: 0.1555 Acc: 96.9469\n",
      "\n",
      "Epoch 1175/1199\n",
      "------------------------\n",
      "train Loss: 0.0447 Acc: 98.7473\n",
      "test Loss: 0.1538 Acc: 96.9476\n",
      "\n",
      "Epoch 1176/1199\n",
      "------------------------\n",
      "train Loss: 0.0434 Acc: 98.8011\n",
      "test Loss: 0.1506 Acc: 96.9576\n",
      "\n",
      "Epoch 1177/1199\n",
      "------------------------\n",
      "train Loss: 0.0445 Acc: 98.7772\n",
      "test Loss: 0.1534 Acc: 96.9862\n",
      "\n",
      "Epoch 1178/1199\n",
      "------------------------\n",
      "train Loss: 0.0441 Acc: 98.7911\n",
      "test Loss: 0.1505 Acc: 97.0287\n",
      "\n",
      "Epoch 1179/1199\n",
      "------------------------\n",
      "train Loss: 0.0451 Acc: 98.7596\n",
      "test Loss: 0.1522 Acc: 96.9685\n",
      "\n",
      "Epoch 1180/1199\n",
      "------------------------\n",
      "train Loss: 0.0463 Acc: 98.7146\n",
      "test Loss: 0.1538 Acc: 96.9612\n",
      "\n",
      "Epoch 1181/1199\n",
      "------------------------\n",
      "train Loss: 0.0472 Acc: 98.6930\n",
      "test Loss: 0.1535 Acc: 96.9686\n",
      "\n",
      "Epoch 1182/1199\n",
      "------------------------\n",
      "train Loss: 0.0472 Acc: 98.6995\n",
      "test Loss: 0.1554 Acc: 96.9469\n",
      "\n",
      "Epoch 1183/1199\n",
      "------------------------\n",
      "train Loss: 0.0449 Acc: 98.7472\n",
      "test Loss: 0.1542 Acc: 96.9419\n",
      "\n",
      "Epoch 1184/1199\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0460 Acc: 98.7084\n",
      "test Loss: 0.1513 Acc: 97.0391\n",
      "\n",
      "Epoch 1185/1199\n",
      "------------------------\n",
      "train Loss: 0.0446 Acc: 98.7632\n",
      "test Loss: 0.1525 Acc: 96.9683\n",
      "\n",
      "Epoch 1186/1199\n",
      "------------------------\n",
      "train Loss: 0.0439 Acc: 98.7855\n",
      "test Loss: 0.1559 Acc: 96.8790\n",
      "\n",
      "Epoch 1187/1199\n",
      "------------------------\n",
      "train Loss: 0.0465 Acc: 98.7156\n",
      "test Loss: 0.1553 Acc: 96.9462\n",
      "\n",
      "Epoch 1188/1199\n",
      "------------------------\n",
      "train Loss: 0.0457 Acc: 98.7263\n",
      "test Loss: 0.1528 Acc: 96.9662\n",
      "\n",
      "Epoch 1189/1199\n",
      "------------------------\n",
      "train Loss: 0.0460 Acc: 98.7246\n",
      "test Loss: 0.1532 Acc: 97.0215\n",
      "\n",
      "Epoch 1190/1199\n",
      "------------------------\n",
      "train Loss: 0.0442 Acc: 98.7733\n",
      "test Loss: 0.1499 Acc: 96.9350\n",
      "\n",
      "Epoch 1191/1199\n",
      "------------------------\n",
      "train Loss: 0.0441 Acc: 98.7671\n",
      "test Loss: 0.1540 Acc: 96.9804\n",
      "\n",
      "Epoch 1192/1199\n",
      "------------------------\n",
      "train Loss: 0.0473 Acc: 98.6799\n",
      "test Loss: 0.1532 Acc: 97.0270\n",
      "\n",
      "Epoch 1193/1199\n",
      "------------------------\n",
      "train Loss: 0.0451 Acc: 98.7467\n",
      "test Loss: 0.1552 Acc: 96.8590\n",
      "\n",
      "Epoch 1194/1199\n",
      "------------------------\n",
      "train Loss: 0.0435 Acc: 98.7995\n",
      "test Loss: 0.1540 Acc: 96.9407\n",
      "\n",
      "Epoch 1195/1199\n",
      "------------------------\n",
      "train Loss: 0.0434 Acc: 98.7952\n",
      "test Loss: 0.1566 Acc: 96.9023\n",
      "\n",
      "Epoch 1196/1199\n",
      "------------------------\n",
      "train Loss: 0.0454 Acc: 98.7471\n",
      "test Loss: 0.1538 Acc: 96.9631\n",
      "\n",
      "Epoch 1197/1199\n",
      "------------------------\n",
      "train Loss: 0.0471 Acc: 98.6989\n",
      "test Loss: 0.1538 Acc: 97.0280\n",
      "\n",
      "Epoch 1198/1199\n",
      "------------------------\n",
      "train Loss: 0.0508 Acc: 98.6339\n",
      "test Loss: 0.1579 Acc: 96.8410\n",
      "\n",
      "Epoch 1199/1199\n",
      "------------------------\n",
      "train Loss: 0.0513 Acc: 98.6110\n",
      "test Loss: 0.1577 Acc: 96.8037\n",
      "\n",
      "Training complete in 192m 37s\n",
      "Best val Acc: 97.052738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type Model1. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "model_adam = mac_train_model(model=model_adam, criterion=criterion, optimizer=optimizer_adam, \n",
    "                          scheduler=exp_lr_scheduler, num_epochs=1200)\n",
    "torch.save(model_adam, './data/model_adam.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1199\n",
      "------------------------\n",
      "train Loss: 0.2521 Acc: 94.7714\n",
      "test Loss: 1.1996 Acc: 79.4876\n",
      "\n",
      "Epoch 1/1199\n",
      "------------------------\n",
      "train Loss: 2.0123 Acc: 67.5005\n",
      "test Loss: 2.7442 Acc: 59.1124\n",
      "\n",
      "Epoch 2/1199\n",
      "------------------------\n",
      "train Loss: 2.6890 Acc: 58.2699\n",
      "test Loss: 3.1334 Acc: 53.7248\n",
      "\n",
      "Epoch 3/1199\n",
      "------------------------\n",
      "train Loss: 5.9153 Acc: 44.1509\n",
      "test Loss: 7.3086 Acc: 38.4014\n",
      "\n",
      "Epoch 4/1199\n",
      "------------------------\n",
      "train Loss: 7.0583 Acc: 41.8272\n",
      "test Loss: 7.3894 Acc: 38.8101\n",
      "\n",
      "Epoch 5/1199\n",
      "------------------------\n",
      "train Loss: 8.5165 Acc: 41.2317\n",
      "test Loss: 9.8028 Acc: 41.1758\n",
      "\n",
      "Epoch 6/1199\n",
      "------------------------\n",
      "train Loss: 10.4037 Acc: 41.9070\n",
      "test Loss: 11.2110 Acc: 43.0456\n",
      "\n",
      "Epoch 7/1199\n",
      "------------------------\n",
      "train Loss: 12.4096 Acc: 42.9188\n",
      "test Loss: 13.9722 Acc: 42.1404\n",
      "\n",
      "Epoch 8/1199\n",
      "------------------------\n",
      "train Loss: 16.8186 Acc: 42.3044\n",
      "test Loss: 22.4195 Acc: 39.4660\n",
      "\n",
      "Epoch 9/1199\n",
      "------------------------\n",
      "train Loss: 24.8638 Acc: 37.8142\n",
      "test Loss: 26.3727 Acc: 37.3298\n",
      "\n",
      "Epoch 10/1199\n",
      "------------------------\n",
      "train Loss: 28.9703 Acc: 37.1852\n",
      "test Loss: 29.7167 Acc: 38.8590\n",
      "\n",
      "Epoch 11/1199\n",
      "------------------------\n",
      "train Loss: 33.5113 Acc: 38.5972\n",
      "test Loss: 36.1630 Acc: 36.6407\n",
      "\n",
      "Epoch 12/1199\n",
      "------------------------\n",
      "train Loss: 36.3095 Acc: 38.5710\n",
      "test Loss: 37.1573 Acc: 39.5772\n",
      "\n",
      "Epoch 13/1199\n",
      "------------------------\n",
      "train Loss: 40.1636 Acc: 38.3601\n",
      "test Loss: 41.7955 Acc: 39.3923\n",
      "\n",
      "Epoch 14/1199\n",
      "------------------------\n",
      "train Loss: 51.1049 Acc: 37.2492\n",
      "test Loss: 55.0509 Acc: 35.5164\n",
      "\n",
      "Epoch 15/1199\n",
      "------------------------\n",
      "train Loss: 61.2395 Acc: 37.6037\n",
      "test Loss: 65.5517 Acc: 36.5251\n",
      "\n",
      "Epoch 16/1199\n",
      "------------------------\n",
      "train Loss: 70.9603 Acc: 37.2766\n",
      "test Loss: 68.8960 Acc: 39.5655\n",
      "\n",
      "Epoch 17/1199\n",
      "------------------------\n",
      "train Loss: 79.8829 Acc: 34.9333\n",
      "test Loss: 81.5712 Acc: 31.9436\n",
      "\n",
      "Epoch 18/1199\n",
      "------------------------\n",
      "train Loss: 76.0603 Acc: 36.2693\n",
      "test Loss: 80.7942 Acc: 39.5101\n",
      "\n",
      "Epoch 19/1199\n",
      "------------------------\n",
      "train Loss: 82.0313 Acc: 39.3754\n",
      "test Loss: 86.6979 Acc: 37.7562\n",
      "\n",
      "Epoch 20/1199\n",
      "------------------------\n",
      "train Loss: 86.4414 Acc: 38.6910\n",
      "test Loss: 86.9474 Acc: 40.5370\n",
      "\n",
      "Epoch 21/1199\n",
      "------------------------\n",
      "train Loss: 88.2086 Acc: 40.1797\n",
      "test Loss: 89.0331 Acc: 41.4782\n",
      "\n",
      "Epoch 22/1199\n",
      "------------------------\n",
      "train Loss: 87.3057 Acc: 39.9749\n",
      "test Loss: 84.8514 Acc: 42.2015\n",
      "\n",
      "Epoch 23/1199\n",
      "------------------------\n",
      "train Loss: 94.1195 Acc: 41.1036\n",
      "test Loss: 92.5028 Acc: 40.9035\n",
      "\n",
      "Epoch 24/1199\n",
      "------------------------\n",
      "train Loss: 99.4610 Acc: 39.7172\n",
      "test Loss: 113.1442 Acc: 39.4679\n",
      "\n",
      "Epoch 25/1199\n",
      "------------------------\n",
      "train Loss: 110.4348 Acc: 41.3983\n",
      "test Loss: 114.6516 Acc: 43.0473\n",
      "\n",
      "Epoch 26/1199\n",
      "------------------------\n",
      "train Loss: 107.8821 Acc: 40.9256\n",
      "test Loss: 109.3671 Acc: 40.4643\n",
      "\n",
      "Epoch 27/1199\n",
      "------------------------\n",
      "train Loss: 107.8384 Acc: 41.3617\n",
      "test Loss: 106.8714 Acc: 42.5518\n",
      "\n",
      "Epoch 28/1199\n",
      "------------------------\n",
      "train Loss: 105.9729 Acc: 42.5372\n",
      "test Loss: 106.9435 Acc: 43.1672\n",
      "\n",
      "Epoch 29/1199\n",
      "------------------------\n",
      "train Loss: 107.8304 Acc: 43.1196\n",
      "test Loss: 114.5648 Acc: 43.7932\n",
      "\n",
      "Epoch 30/1199\n",
      "------------------------\n",
      "train Loss: 113.0295 Acc: 42.0229\n",
      "test Loss: 116.7261 Acc: 42.1696\n",
      "\n",
      "Epoch 31/1199\n",
      "------------------------\n",
      "train Loss: 117.4006 Acc: 43.4387\n",
      "test Loss: 121.4840 Acc: 43.2374\n",
      "\n",
      "Epoch 32/1199\n",
      "------------------------\n",
      "train Loss: 120.0619 Acc: 42.8791\n",
      "test Loss: 119.6255 Acc: 43.7851\n",
      "\n",
      "Epoch 33/1199\n",
      "------------------------\n",
      "train Loss: 125.6968 Acc: 41.4440\n",
      "test Loss: 129.5325 Acc: 40.8701\n",
      "\n",
      "Epoch 34/1199\n",
      "------------------------\n",
      "train Loss: 132.1687 Acc: 42.3944\n",
      "test Loss: 129.3178 Acc: 42.9746\n",
      "\n",
      "Epoch 35/1199\n",
      "------------------------\n",
      "train Loss: 131.2863 Acc: 42.5052\n",
      "test Loss: 130.0426 Acc: 43.1459\n",
      "\n",
      "Epoch 36/1199\n",
      "------------------------\n",
      "train Loss: 129.0326 Acc: 43.1543\n",
      "test Loss: 129.5139 Acc: 42.9352\n",
      "\n",
      "Epoch 37/1199\n",
      "------------------------\n",
      "train Loss: 130.2935 Acc: 43.2909\n",
      "test Loss: 127.5519 Acc: 43.1577\n",
      "\n",
      "Epoch 38/1199\n",
      "------------------------\n",
      "train Loss: 129.1752 Acc: 43.6526\n",
      "test Loss: 132.1741 Acc: 42.3097\n",
      "\n",
      "Epoch 39/1199\n",
      "------------------------\n",
      "train Loss: 131.0866 Acc: 43.0778\n",
      "test Loss: 132.5033 Acc: 42.8982\n",
      "\n",
      "Epoch 40/1199\n",
      "------------------------\n",
      "train Loss: 131.9980 Acc: 43.7392\n",
      "test Loss: 134.3432 Acc: 43.1431\n",
      "\n",
      "Epoch 41/1199\n",
      "------------------------\n",
      "train Loss: 133.7116 Acc: 44.1945\n",
      "test Loss: 135.3827 Acc: 43.7575\n",
      "\n",
      "Epoch 42/1199\n",
      "------------------------\n",
      "train Loss: 135.5466 Acc: 43.9487\n",
      "test Loss: 139.8202 Acc: 43.8792\n",
      "\n",
      "Epoch 43/1199\n",
      "------------------------\n",
      "train Loss: 137.2827 Acc: 44.5338\n",
      "test Loss: 138.8175 Acc: 43.5861\n",
      "\n",
      "Epoch 44/1199\n",
      "------------------------\n",
      "train Loss: 136.8656 Acc: 43.4989\n",
      "test Loss: 134.7029 Acc: 44.8086\n",
      "\n",
      "Epoch 45/1199\n",
      "------------------------\n",
      "train Loss: 137.7652 Acc: 44.8921\n",
      "test Loss: 136.0965 Acc: 45.1890\n",
      "\n",
      "Epoch 46/1199\n",
      "------------------------\n",
      "train Loss: 140.2363 Acc: 44.3987\n",
      "test Loss: 138.9931 Acc: 44.2713\n",
      "\n",
      "Epoch 47/1199\n",
      "------------------------\n",
      "train Loss: 141.0242 Acc: 44.3318\n",
      "test Loss: 141.8011 Acc: 45.7073\n",
      "\n",
      "Epoch 48/1199\n",
      "------------------------\n",
      "train Loss: 157.6133 Acc: 44.6244\n",
      "test Loss: 157.8234 Acc: 42.8387\n",
      "\n",
      "Epoch 49/1199\n",
      "------------------------\n",
      "train Loss: 158.1576 Acc: 43.0629\n",
      "test Loss: 157.5588 Acc: 44.0863\n",
      "\n",
      "Epoch 50/1199\n",
      "------------------------\n",
      "train Loss: 158.1101 Acc: 45.1672\n",
      "test Loss: 162.6565 Acc: 43.8943\n",
      "\n",
      "Epoch 51/1199\n",
      "------------------------\n",
      "train Loss: 163.4068 Acc: 42.9399\n",
      "test Loss: 163.4850 Acc: 43.3112\n",
      "\n",
      "Epoch 52/1199\n",
      "------------------------\n",
      "train Loss: 165.9292 Acc: 43.9246\n",
      "test Loss: 170.3278 Acc: 44.2067\n",
      "\n",
      "Epoch 53/1199\n",
      "------------------------\n",
      "train Loss: 168.2190 Acc: 43.9881\n",
      "test Loss: 171.1612 Acc: 43.4146\n",
      "\n",
      "Epoch 54/1199\n",
      "------------------------\n",
      "train Loss: 168.9805 Acc: 43.6472\n",
      "test Loss: 163.8376 Acc: 43.4140\n",
      "\n",
      "Epoch 55/1199\n",
      "------------------------\n",
      "train Loss: 169.9643 Acc: 43.7548\n",
      "test Loss: 166.1082 Acc: 44.2813\n",
      "\n",
      "Epoch 56/1199\n",
      "------------------------\n",
      "train Loss: 173.0123 Acc: 43.6884\n",
      "test Loss: 176.9912 Acc: 43.7861\n",
      "\n",
      "Epoch 57/1199\n",
      "------------------------\n",
      "train Loss: 180.7090 Acc: 42.9839\n",
      "test Loss: 186.7569 Acc: 42.9034\n",
      "\n",
      "Epoch 58/1199\n",
      "------------------------\n",
      "train Loss: 182.8971 Acc: 43.9345\n",
      "test Loss: 186.2506 Acc: 42.6098\n",
      "\n",
      "Epoch 59/1199\n",
      "------------------------\n",
      "train Loss: 185.3147 Acc: 44.0307\n",
      "test Loss: 182.1443 Acc: 45.8545\n",
      "\n",
      "Epoch 60/1199\n",
      "------------------------\n",
      "train Loss: 187.6411 Acc: 43.7029\n",
      "test Loss: 187.7782 Acc: 43.4404\n",
      "\n",
      "Epoch 61/1199\n",
      "------------------------\n",
      "train Loss: 187.8135 Acc: 44.2158\n",
      "test Loss: 194.4554 Acc: 44.5984\n",
      "\n",
      "Epoch 62/1199\n",
      "------------------------\n",
      "train Loss: 192.6265 Acc: 43.7801\n",
      "test Loss: 194.4536 Acc: 44.0136\n",
      "\n",
      "Epoch 63/1199\n",
      "------------------------\n",
      "train Loss: 197.8380 Acc: 44.5789\n",
      "test Loss: 199.0862 Acc: 44.0398\n",
      "\n",
      "Epoch 64/1199\n",
      "------------------------\n",
      "train Loss: 203.3462 Acc: 44.3922\n",
      "test Loss: 202.3481 Acc: 44.0468\n",
      "\n",
      "Epoch 65/1199\n",
      "------------------------\n",
      "train Loss: 203.2599 Acc: 43.7550\n",
      "test Loss: 208.0549 Acc: 44.8728\n",
      "\n",
      "Epoch 66/1199\n",
      "------------------------\n",
      "train Loss: 201.8538 Acc: 43.9569\n",
      "test Loss: 207.1113 Acc: 44.5778\n",
      "\n",
      "Epoch 67/1199\n",
      "------------------------\n",
      "train Loss: 209.0321 Acc: 44.1427\n",
      "test Loss: 207.0497 Acc: 43.8122\n",
      "\n",
      "Epoch 68/1199\n",
      "------------------------\n",
      "train Loss: 204.6548 Acc: 43.8629\n",
      "test Loss: 205.1049 Acc: 43.4659\n",
      "\n",
      "Epoch 69/1199\n",
      "------------------------\n",
      "train Loss: 208.6958 Acc: 43.6470\n",
      "test Loss: 211.7277 Acc: 44.0371\n",
      "\n",
      "Epoch 70/1199\n",
      "------------------------\n",
      "train Loss: 212.7336 Acc: 43.5630\n",
      "test Loss: 214.3773 Acc: 43.5248\n",
      "\n",
      "Epoch 71/1199\n",
      "------------------------\n",
      "train Loss: 212.8243 Acc: 43.8221\n",
      "test Loss: 213.8442 Acc: 43.1953\n",
      "\n",
      "Epoch 72/1199\n",
      "------------------------\n",
      "train Loss: 210.6739 Acc: 43.0960\n",
      "test Loss: 204.7413 Acc: 44.3932\n",
      "\n",
      "Epoch 73/1199\n",
      "------------------------\n",
      "train Loss: 209.9966 Acc: 44.0347\n",
      "test Loss: 207.8944 Acc: 43.3958\n",
      "\n",
      "Epoch 74/1199\n",
      "------------------------\n",
      "train Loss: 212.7443 Acc: 43.7646\n",
      "test Loss: 219.8721 Acc: 43.1321\n",
      "\n",
      "Epoch 75/1199\n",
      "------------------------\n",
      "train Loss: 221.1300 Acc: 43.8870\n",
      "test Loss: 228.0815 Acc: 43.2121\n",
      "\n",
      "Epoch 76/1199\n",
      "------------------------\n",
      "train Loss: 228.6030 Acc: 43.1998\n",
      "test Loss: 220.3941 Acc: 44.3053\n",
      "\n",
      "Epoch 77/1199\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 223.2785 Acc: 44.8679\n",
      "test Loss: 229.3112 Acc: 43.3112\n",
      "\n",
      "Epoch 78/1199\n",
      "------------------------\n",
      "train Loss: 228.1385 Acc: 42.9894\n",
      "test Loss: 223.7037 Acc: 43.9108\n",
      "\n",
      "Epoch 79/1199\n",
      "------------------------\n",
      "train Loss: 225.6572 Acc: 44.0784\n",
      "test Loss: 223.7259 Acc: 42.9941\n",
      "\n",
      "Epoch 80/1199\n",
      "------------------------\n",
      "train Loss: 231.6492 Acc: 43.7459\n",
      "test Loss: 230.1796 Acc: 44.8530\n",
      "\n",
      "Epoch 81/1199\n",
      "------------------------\n",
      "train Loss: 233.5015 Acc: 44.4937\n",
      "test Loss: 228.6742 Acc: 43.2855\n",
      "\n",
      "Epoch 82/1199\n",
      "------------------------\n",
      "train Loss: 235.9046 Acc: 42.4311\n",
      "test Loss: 233.3223 Acc: 42.6071\n",
      "\n",
      "Epoch 83/1199\n",
      "------------------------\n",
      "train Loss: 241.3952 Acc: 43.6112\n",
      "test Loss: 255.6742 Acc: 43.8400\n",
      "\n",
      "Epoch 84/1199\n",
      "------------------------\n",
      "train Loss: 249.5699 Acc: 44.1031\n",
      "test Loss: 251.2138 Acc: 43.0853\n",
      "\n",
      "Epoch 85/1199\n",
      "------------------------\n",
      "train Loss: 255.1396 Acc: 43.0662\n",
      "test Loss: 257.6467 Acc: 43.4212\n",
      "\n",
      "Epoch 86/1199\n",
      "------------------------\n",
      "train Loss: 254.5213 Acc: 44.0106\n",
      "test Loss: 252.6958 Acc: 42.8024\n",
      "\n",
      "Epoch 87/1199\n",
      "------------------------\n",
      "train Loss: 255.3127 Acc: 43.2090\n",
      "test Loss: 262.4539 Acc: 42.4464\n",
      "\n",
      "Epoch 88/1199\n",
      "------------------------\n",
      "train Loss: 261.4183 Acc: 43.6124\n",
      "test Loss: 268.2793 Acc: 44.3877\n",
      "\n",
      "Epoch 89/1199\n",
      "------------------------\n",
      "train Loss: 265.0817 Acc: 43.8483\n",
      "test Loss: 258.4206 Acc: 43.6248\n",
      "\n",
      "Epoch 90/1199\n",
      "------------------------\n",
      "train Loss: 262.1521 Acc: 43.1526\n",
      "test Loss: 261.7203 Acc: 44.0547\n",
      "\n",
      "Epoch 91/1199\n",
      "------------------------\n",
      "train Loss: 260.5334 Acc: 44.3789\n",
      "test Loss: 261.6921 Acc: 44.8136\n",
      "\n",
      "Epoch 92/1199\n",
      "------------------------\n",
      "train Loss: 265.6478 Acc: 43.8337\n",
      "test Loss: 267.5889 Acc: 43.7430\n",
      "\n",
      "Epoch 93/1199\n",
      "------------------------\n",
      "train Loss: 269.3364 Acc: 43.9179\n",
      "test Loss: 276.6454 Acc: 43.4855\n",
      "\n",
      "Epoch 94/1199\n",
      "------------------------\n",
      "train Loss: 275.0295 Acc: 43.2565\n",
      "test Loss: 274.6369 Acc: 43.6381\n",
      "\n",
      "Epoch 95/1199\n",
      "------------------------\n",
      "train Loss: 282.8528 Acc: 43.1301\n",
      "test Loss: 285.6546 Acc: 44.2212\n",
      "\n",
      "Epoch 96/1199\n",
      "------------------------\n",
      "train Loss: 283.5711 Acc: 44.3976\n",
      "test Loss: 289.0870 Acc: 43.8810\n",
      "\n",
      "Epoch 97/1199\n",
      "------------------------\n",
      "train Loss: 288.1474 Acc: 43.3756\n",
      "test Loss: 280.1631 Acc: 44.5682\n",
      "\n",
      "Epoch 98/1199\n",
      "------------------------\n",
      "train Loss: 283.5856 Acc: 44.6540\n",
      "test Loss: 294.3555 Acc: 42.8641\n",
      "\n",
      "Epoch 99/1199\n",
      "------------------------\n",
      "train Loss: 282.3947 Acc: 43.1362\n",
      "test Loss: 287.7232 Acc: 44.1255\n",
      "\n",
      "Epoch 100/1199\n",
      "------------------------\n",
      "train Loss: 293.5290 Acc: 44.5496\n",
      "test Loss: 299.2965 Acc: 44.8882\n",
      "\n",
      "Epoch 101/1199\n",
      "------------------------\n",
      "train Loss: 297.4143 Acc: 44.5677\n",
      "test Loss: 298.6481 Acc: 43.3102\n",
      "\n",
      "Epoch 102/1199\n",
      "------------------------\n",
      "train Loss: 304.5708 Acc: 44.3103\n",
      "test Loss: 307.4043 Acc: 44.8650\n",
      "\n",
      "Epoch 103/1199\n",
      "------------------------\n",
      "train Loss: 312.0653 Acc: 44.0681\n",
      "test Loss: 317.2729 Acc: 43.3587\n",
      "\n",
      "Epoch 104/1199\n",
      "------------------------\n",
      "train Loss: 314.7995 Acc: 44.2739\n",
      "test Loss: 313.7356 Acc: 45.2927\n",
      "\n",
      "Epoch 105/1199\n",
      "------------------------\n",
      "train Loss: 312.2103 Acc: 44.2994\n",
      "test Loss: 319.2395 Acc: 44.2031\n",
      "\n",
      "Epoch 106/1199\n",
      "------------------------\n",
      "train Loss: 319.1247 Acc: 44.8128\n",
      "test Loss: 320.1504 Acc: 45.1208\n",
      "\n",
      "Epoch 107/1199\n",
      "------------------------\n",
      "train Loss: 320.0274 Acc: 45.2493\n",
      "test Loss: 323.6406 Acc: 44.1948\n",
      "\n",
      "Epoch 108/1199\n",
      "------------------------\n",
      "train Loss: 328.0214 Acc: 44.1971\n",
      "test Loss: 330.9189 Acc: 44.8201\n",
      "\n",
      "Epoch 109/1199\n",
      "------------------------\n",
      "train Loss: 331.6438 Acc: 44.6982\n",
      "test Loss: 331.0414 Acc: 45.2956\n",
      "\n",
      "Epoch 110/1199\n",
      "------------------------\n",
      "train Loss: 338.1461 Acc: 44.7057\n",
      "test Loss: 339.9514 Acc: 44.6616\n",
      "\n",
      "Epoch 111/1199\n",
      "------------------------\n",
      "train Loss: 340.1164 Acc: 44.1192\n",
      "test Loss: 340.2337 Acc: 45.5604\n",
      "\n",
      "Epoch 112/1199\n",
      "------------------------\n",
      "train Loss: 342.8382 Acc: 44.9898\n",
      "test Loss: 353.2793 Acc: 43.9241\n",
      "\n",
      "Epoch 113/1199\n",
      "------------------------\n",
      "train Loss: 349.9616 Acc: 44.2011\n",
      "test Loss: 346.0991 Acc: 44.2012\n",
      "\n",
      "Epoch 114/1199\n",
      "------------------------\n",
      "train Loss: 352.1961 Acc: 45.0848\n",
      "test Loss: 346.0008 Acc: 45.2210\n",
      "\n",
      "Epoch 115/1199\n",
      "------------------------\n",
      "train Loss: 358.2293 Acc: 45.0183\n",
      "test Loss: 362.2275 Acc: 43.3374\n",
      "\n",
      "Epoch 116/1199\n",
      "------------------------\n",
      "train Loss: 362.9773 Acc: 43.9937\n",
      "test Loss: 366.4877 Acc: 44.0901\n",
      "\n",
      "Epoch 117/1199\n",
      "------------------------\n",
      "train Loss: 363.2990 Acc: 43.8870\n",
      "test Loss: 359.9365 Acc: 44.1158\n",
      "\n",
      "Epoch 118/1199\n",
      "------------------------\n",
      "train Loss: 370.0424 Acc: 44.3561\n",
      "test Loss: 366.9992 Acc: 44.0796\n",
      "\n",
      "Epoch 119/1199\n",
      "------------------------\n",
      "train Loss: 364.2649 Acc: 44.8198\n",
      "test Loss: 372.5219 Acc: 45.2027\n",
      "\n",
      "Epoch 120/1199\n",
      "------------------------\n",
      "train Loss: 366.4109 Acc: 44.8358\n",
      "test Loss: 370.5387 Acc: 45.1082\n",
      "\n",
      "Epoch 121/1199\n",
      "------------------------\n",
      "train Loss: 367.5347 Acc: 44.9517\n",
      "test Loss: 381.9085 Acc: 44.9861\n",
      "\n",
      "Epoch 122/1199\n",
      "------------------------\n",
      "train Loss: 372.7776 Acc: 44.2031\n",
      "test Loss: 375.7223 Acc: 43.3221\n",
      "\n",
      "Epoch 123/1199\n",
      "------------------------\n",
      "train Loss: 380.4517 Acc: 43.8451\n",
      "test Loss: 383.5590 Acc: 43.4841\n",
      "\n",
      "Epoch 124/1199\n",
      "------------------------\n",
      "train Loss: 388.2594 Acc: 44.5051\n",
      "test Loss: 382.5475 Acc: 43.6848\n",
      "\n",
      "Epoch 125/1199\n",
      "------------------------\n",
      "train Loss: 386.7261 Acc: 43.7461\n",
      "test Loss: 381.4830 Acc: 44.9243\n",
      "\n",
      "Epoch 126/1199\n",
      "------------------------\n",
      "train Loss: 382.5048 Acc: 45.4707\n",
      "test Loss: 387.0290 Acc: 44.7918\n",
      "\n",
      "Epoch 127/1199\n",
      "------------------------\n",
      "train Loss: 392.0292 Acc: 43.5796\n",
      "test Loss: 384.9931 Acc: 44.2504\n",
      "\n",
      "Epoch 128/1199\n",
      "------------------------\n",
      "train Loss: 390.2130 Acc: 43.9529\n",
      "test Loss: 388.5144 Acc: 45.3483\n",
      "\n",
      "Epoch 129/1199\n",
      "------------------------\n",
      "train Loss: 397.5042 Acc: 44.7365\n",
      "test Loss: 401.8023 Acc: 43.7176\n",
      "\n",
      "Epoch 130/1199\n",
      "------------------------\n",
      "train Loss: 403.2170 Acc: 43.8430\n",
      "test Loss: 400.3982 Acc: 43.7024\n",
      "\n",
      "Epoch 131/1199\n",
      "------------------------\n",
      "train Loss: 407.0537 Acc: 43.8745\n",
      "test Loss: 412.0954 Acc: 43.7518\n",
      "\n",
      "Epoch 132/1199\n",
      "------------------------\n",
      "train Loss: 419.8413 Acc: 43.7177\n",
      "test Loss: 422.3263 Acc: 43.5715\n",
      "\n",
      "Epoch 133/1199\n",
      "------------------------\n",
      "train Loss: 417.0218 Acc: 43.9823\n",
      "test Loss: 425.4918 Acc: 42.4794\n",
      "\n",
      "Epoch 134/1199\n",
      "------------------------\n",
      "train Loss: 427.6417 Acc: 42.8945\n",
      "test Loss: 418.5861 Acc: 44.3400\n",
      "\n",
      "Epoch 135/1199\n",
      "------------------------\n",
      "train Loss: 423.1632 Acc: 44.4923\n",
      "test Loss: 421.9177 Acc: 44.3436\n",
      "\n",
      "Epoch 136/1199\n",
      "------------------------\n",
      "train Loss: 419.0543 Acc: 44.9783\n",
      "test Loss: 428.1786 Acc: 44.1207\n",
      "\n",
      "Epoch 137/1199\n",
      "------------------------\n",
      "train Loss: 428.0375 Acc: 43.6244\n",
      "test Loss: 429.8887 Acc: 44.6651\n",
      "\n",
      "Epoch 138/1199\n",
      "------------------------\n",
      "train Loss: 433.2699 Acc: 43.8454\n",
      "test Loss: 436.9719 Acc: 43.6670\n",
      "\n",
      "Epoch 139/1199\n",
      "------------------------\n",
      "train Loss: 438.6439 Acc: 44.1427\n",
      "test Loss: 444.7690 Acc: 44.3271\n",
      "\n",
      "Epoch 140/1199\n",
      "------------------------\n",
      "train Loss: 434.5025 Acc: 44.6409\n",
      "test Loss: 431.8259 Acc: 44.1288\n",
      "\n",
      "Epoch 141/1199\n",
      "------------------------\n",
      "train Loss: 449.3653 Acc: 42.8789\n",
      "test Loss: 452.7200 Acc: 43.2644\n",
      "\n",
      "Epoch 142/1199\n",
      "------------------------\n",
      "train Loss: 450.2482 Acc: 44.3129\n",
      "test Loss: 456.0381 Acc: 43.9262\n",
      "\n",
      "Epoch 143/1199\n",
      "------------------------\n",
      "train Loss: 458.8996 Acc: 43.9244\n",
      "test Loss: 470.3534 Acc: 44.2633\n",
      "\n",
      "Epoch 144/1199\n",
      "------------------------\n",
      "train Loss: 460.3210 Acc: 44.1123\n",
      "test Loss: 459.9111 Acc: 44.8977\n",
      "\n",
      "Epoch 145/1199\n",
      "------------------------\n",
      "train Loss: 470.0921 Acc: 43.5912\n",
      "test Loss: 472.5513 Acc: 42.8311\n",
      "\n",
      "Epoch 146/1199\n",
      "------------------------\n",
      "train Loss: 461.9172 Acc: 44.5032\n",
      "test Loss: 460.2332 Acc: 45.2605\n",
      "\n",
      "Epoch 147/1199\n",
      "------------------------\n",
      "train Loss: 471.7668 Acc: 43.5714\n",
      "test Loss: 471.0953 Acc: 42.8406\n",
      "\n",
      "Epoch 148/1199\n",
      "------------------------\n",
      "train Loss: 481.0697 Acc: 43.8804\n",
      "test Loss: 476.9856 Acc: 44.5293\n",
      "\n",
      "Epoch 149/1199\n",
      "------------------------\n",
      "train Loss: 491.8055 Acc: 44.0030\n",
      "test Loss: 497.4052 Acc: 42.7366\n",
      "\n",
      "Epoch 150/1199\n",
      "------------------------\n",
      "train Loss: 495.5040 Acc: 41.9522\n",
      "test Loss: 492.8727 Acc: 42.8043\n",
      "\n",
      "Epoch 151/1199\n",
      "------------------------\n",
      "train Loss: 493.8714 Acc: 43.5119\n",
      "test Loss: 493.6816 Acc: 45.5845\n",
      "\n",
      "Epoch 152/1199\n",
      "------------------------\n",
      "train Loss: 498.7174 Acc: 44.1689\n",
      "test Loss: 501.7499 Acc: 43.3554\n",
      "\n",
      "Epoch 153/1199\n",
      "------------------------\n",
      "train Loss: 501.2228 Acc: 42.4987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 482.0705 Acc: 43.3596\n",
      "\n",
      "Epoch 154/1199\n",
      "------------------------\n",
      "train Loss: 492.8614 Acc: 43.9288\n",
      "test Loss: 492.9427 Acc: 44.8564\n",
      "\n",
      "Epoch 155/1199\n",
      "------------------------\n",
      "train Loss: 500.7900 Acc: 44.3910\n",
      "test Loss: 512.9525 Acc: 43.5298\n",
      "\n",
      "Epoch 156/1199\n",
      "------------------------\n",
      "train Loss: 518.9006 Acc: 44.1296\n",
      "test Loss: 501.9638 Acc: 43.7670\n",
      "\n",
      "Epoch 157/1199\n",
      "------------------------\n",
      "train Loss: 509.8074 Acc: 44.3547\n",
      "test Loss: 523.3869 Acc: 45.4452\n",
      "\n",
      "Epoch 158/1199\n",
      "------------------------\n",
      "train Loss: 512.6422 Acc: 45.1147\n",
      "test Loss: 508.2629 Acc: 44.5371\n",
      "\n",
      "Epoch 159/1199\n",
      "------------------------\n",
      "train Loss: 526.1001 Acc: 42.7812\n",
      "test Loss: 532.3772 Acc: 43.2810\n",
      "\n",
      "Epoch 160/1199\n",
      "------------------------\n",
      "train Loss: 545.8879 Acc: 43.9798\n",
      "test Loss: 541.4788 Acc: 43.3051\n",
      "\n",
      "Epoch 161/1199\n",
      "------------------------\n",
      "train Loss: 536.5242 Acc: 43.2587\n",
      "test Loss: 538.5226 Acc: 43.5233\n",
      "\n",
      "Epoch 162/1199\n",
      "------------------------\n",
      "train Loss: 548.3952 Acc: 44.2828\n",
      "test Loss: 542.1781 Acc: 45.3296\n",
      "\n",
      "Epoch 163/1199\n",
      "------------------------\n",
      "train Loss: 546.0728 Acc: 45.1909\n",
      "test Loss: 522.7842 Acc: 43.8518\n",
      "\n",
      "Epoch 164/1199\n",
      "------------------------\n",
      "train Loss: 530.0654 Acc: 42.7188\n",
      "test Loss: 537.7106 Acc: 40.9256\n",
      "\n",
      "Epoch 165/1199\n",
      "------------------------\n",
      "train Loss: 531.5772 Acc: 43.0627\n",
      "test Loss: 545.6899 Acc: 42.9554\n",
      "\n",
      "Epoch 166/1199\n",
      "------------------------\n",
      "train Loss: 538.2174 Acc: 44.0426\n",
      "test Loss: 545.7181 Acc: 43.8217\n",
      "\n",
      "Epoch 167/1199\n",
      "------------------------\n",
      "train Loss: 560.6090 Acc: 43.8291\n",
      "test Loss: 549.8401 Acc: 43.2057\n",
      "\n",
      "Epoch 168/1199\n",
      "------------------------\n",
      "train Loss: 563.3820 Acc: 44.8848\n",
      "test Loss: 562.0449 Acc: 45.1675\n",
      "\n",
      "Epoch 169/1199\n",
      "------------------------\n",
      "train Loss: 563.2313 Acc: 43.7170\n",
      "test Loss: 578.5607 Acc: 42.0155\n",
      "\n",
      "Epoch 170/1199\n",
      "------------------------\n",
      "train Loss: 564.3009 Acc: 42.1551\n",
      "test Loss: 560.2319 Acc: 43.0694\n",
      "\n",
      "Epoch 171/1199\n",
      "------------------------\n",
      "train Loss: 554.9007 Acc: 44.1181\n",
      "test Loss: 552.6891 Acc: 45.2495\n",
      "\n",
      "Epoch 172/1199\n",
      "------------------------\n",
      "train Loss: 565.4308 Acc: 44.4322\n",
      "test Loss: 558.4223 Acc: 44.3333\n",
      "\n",
      "Epoch 173/1199\n",
      "------------------------\n",
      "train Loss: 573.0060 Acc: 44.2558\n",
      "test Loss: 571.2927 Acc: 43.9203\n",
      "\n",
      "Epoch 174/1199\n",
      "------------------------\n",
      "train Loss: 571.6400 Acc: 43.9395\n",
      "test Loss: 576.7419 Acc: 44.2471\n",
      "\n",
      "Epoch 175/1199\n",
      "------------------------\n",
      "train Loss: 578.0431 Acc: 44.0571\n",
      "test Loss: 578.2526 Acc: 43.7739\n",
      "\n",
      "Epoch 176/1199\n",
      "------------------------\n",
      "train Loss: 572.0509 Acc: 43.1277\n",
      "test Loss: 552.8806 Acc: 43.9561\n",
      "\n",
      "Epoch 177/1199\n",
      "------------------------\n",
      "train Loss: 583.4237 Acc: 43.5915\n",
      "test Loss: 570.7367 Acc: 44.4329\n",
      "\n",
      "Epoch 178/1199\n",
      "------------------------\n",
      "train Loss: 582.3975 Acc: 44.3066\n",
      "test Loss: 594.8680 Acc: 44.9989\n",
      "\n",
      "Epoch 179/1199\n",
      "------------------------\n",
      "train Loss: 594.9415 Acc: 43.3325\n",
      "test Loss: 595.1230 Acc: 43.6908\n",
      "\n",
      "Epoch 180/1199\n",
      "------------------------\n",
      "train Loss: 586.5114 Acc: 43.8359\n",
      "test Loss: 590.8947 Acc: 43.6746\n",
      "\n",
      "Epoch 181/1199\n",
      "------------------------\n",
      "train Loss: 602.1063 Acc: 44.7142\n",
      "test Loss: 593.9257 Acc: 44.4293\n",
      "\n",
      "Epoch 182/1199\n",
      "------------------------\n",
      "train Loss: 605.2597 Acc: 45.5457\n",
      "test Loss: 609.8652 Acc: 45.7313\n",
      "\n",
      "Epoch 183/1199\n",
      "------------------------\n",
      "train Loss: 610.7721 Acc: 45.6188\n",
      "test Loss: 591.9842 Acc: 44.9718\n",
      "\n",
      "Epoch 184/1199\n",
      "------------------------\n",
      "train Loss: 604.1238 Acc: 44.4885\n",
      "test Loss: 617.8404 Acc: 43.5652\n",
      "\n",
      "Epoch 185/1199\n",
      "------------------------\n",
      "train Loss: 617.7867 Acc: 43.8152\n",
      "test Loss: 622.5859 Acc: 44.6303\n",
      "\n",
      "Epoch 186/1199\n",
      "------------------------\n",
      "train Loss: 630.2696 Acc: 44.3884\n",
      "test Loss: 633.5094 Acc: 45.2709\n",
      "\n",
      "Epoch 187/1199\n",
      "------------------------\n",
      "train Loss: 627.2621 Acc: 45.2607\n",
      "test Loss: 621.0188 Acc: 44.8395\n",
      "\n",
      "Epoch 188/1199\n",
      "------------------------\n",
      "train Loss: 623.6682 Acc: 43.4741\n",
      "test Loss: 626.1732 Acc: 43.7896\n",
      "\n",
      "Epoch 189/1199\n",
      "------------------------\n",
      "train Loss: 619.6555 Acc: 43.9569\n",
      "test Loss: 628.8742 Acc: 44.0943\n",
      "\n",
      "Epoch 190/1199\n",
      "------------------------\n",
      "train Loss: 626.6161 Acc: 44.1023\n",
      "test Loss: 633.7450 Acc: 44.4188\n",
      "\n",
      "Epoch 191/1199\n",
      "------------------------\n",
      "train Loss: 629.3606 Acc: 44.1399\n",
      "test Loss: 629.1110 Acc: 44.5410\n",
      "\n",
      "Epoch 192/1199\n",
      "------------------------\n",
      "train Loss: 629.7565 Acc: 45.1857\n",
      "test Loss: 639.9354 Acc: 45.4001\n",
      "\n",
      "Epoch 193/1199\n",
      "------------------------\n",
      "train Loss: 640.0962 Acc: 45.1166\n",
      "test Loss: 645.2615 Acc: 45.0486\n",
      "\n",
      "Epoch 194/1199\n",
      "------------------------\n",
      "train Loss: 645.5923 Acc: 44.5374\n",
      "test Loss: 667.1027 Acc: 43.5036\n",
      "\n",
      "Epoch 195/1199\n",
      "------------------------\n",
      "train Loss: 656.6617 Acc: 43.8371\n",
      "test Loss: 652.9594 Acc: 43.4663\n",
      "\n",
      "Epoch 196/1199\n",
      "------------------------\n",
      "train Loss: 657.7045 Acc: 44.6715\n",
      "test Loss: 674.8036 Acc: 44.4366\n",
      "\n",
      "Epoch 197/1199\n",
      "------------------------\n",
      "train Loss: 688.0919 Acc: 43.5990\n",
      "test Loss: 706.2956 Acc: 43.2146\n",
      "\n",
      "Epoch 198/1199\n",
      "------------------------\n",
      "train Loss: 694.6031 Acc: 43.8111\n",
      "test Loss: 704.5888 Acc: 43.3050\n",
      "\n",
      "Epoch 199/1199\n",
      "------------------------\n",
      "train Loss: 701.9356 Acc: 43.9984\n",
      "test Loss: 715.3359 Acc: 42.8544\n",
      "\n",
      "Epoch 200/1199\n",
      "------------------------\n",
      "train Loss: 693.4924 Acc: 43.2494\n",
      "test Loss: 711.2052 Acc: 43.7765\n",
      "\n",
      "Epoch 201/1199\n",
      "------------------------\n",
      "train Loss: 686.9535 Acc: 42.9901\n",
      "test Loss: 694.1864 Acc: 43.3309\n",
      "\n",
      "Epoch 202/1199\n",
      "------------------------\n",
      "train Loss: 705.8194 Acc: 44.3098\n",
      "test Loss: 719.9983 Acc: 44.1531\n",
      "\n",
      "Epoch 203/1199\n",
      "------------------------\n",
      "train Loss: 709.5998 Acc: 44.6777\n",
      "test Loss: 688.9974 Acc: 43.8374\n",
      "\n",
      "Epoch 204/1199\n",
      "------------------------\n",
      "train Loss: 707.0457 Acc: 42.9148\n",
      "test Loss: 699.4071 Acc: 43.2697\n",
      "\n",
      "Epoch 205/1199\n",
      "------------------------\n",
      "train Loss: 710.0778 Acc: 43.8426\n",
      "test Loss: 715.1870 Acc: 44.4279\n",
      "\n",
      "Epoch 206/1199\n",
      "------------------------\n",
      "train Loss: 707.3491 Acc: 44.4405\n",
      "test Loss: 698.7410 Acc: 44.0645\n",
      "\n",
      "Epoch 207/1199\n",
      "------------------------\n",
      "train Loss: 711.4123 Acc: 43.4410\n",
      "test Loss: 709.0504 Acc: 41.5694\n",
      "\n",
      "Epoch 208/1199\n",
      "------------------------\n",
      "train Loss: 728.1495 Acc: 41.5896\n",
      "test Loss: 731.1973 Acc: 43.0713\n",
      "\n",
      "Epoch 209/1199\n",
      "------------------------\n",
      "train Loss: 725.5658 Acc: 43.8315\n",
      "test Loss: 723.2811 Acc: 44.2414\n",
      "\n",
      "Epoch 210/1199\n",
      "------------------------\n",
      "train Loss: 734.8199 Acc: 44.1414\n",
      "test Loss: 748.4187 Acc: 44.0136\n",
      "\n",
      "Epoch 211/1199\n",
      "------------------------\n",
      "train Loss: 747.7349 Acc: 42.7953\n",
      "test Loss: 750.2905 Acc: 42.2967\n",
      "\n",
      "Epoch 212/1199\n",
      "------------------------\n",
      "train Loss: 727.9424 Acc: 42.9204\n",
      "test Loss: 751.1277 Acc: 42.3634\n",
      "\n",
      "Epoch 213/1199\n",
      "------------------------\n",
      "train Loss: 749.5118 Acc: 43.0595\n",
      "test Loss: 750.7231 Acc: 42.9906\n",
      "\n",
      "Epoch 214/1199\n",
      "------------------------\n",
      "train Loss: 771.6751 Acc: 42.3624\n",
      "test Loss: 757.9963 Acc: 41.8822\n",
      "\n",
      "Epoch 215/1199\n",
      "------------------------\n",
      "train Loss: 758.8332 Acc: 42.6012\n",
      "test Loss: 768.2807 Acc: 43.2678\n",
      "\n",
      "Epoch 216/1199\n",
      "------------------------\n",
      "train Loss: 772.2259 Acc: 43.7555\n",
      "test Loss: 800.1180 Acc: 43.2195\n",
      "\n",
      "Epoch 217/1199\n",
      "------------------------\n",
      "train Loss: 791.2560 Acc: 42.7573\n",
      "test Loss: 785.8713 Acc: 42.8464\n",
      "\n",
      "Epoch 218/1199\n",
      "------------------------\n",
      "train Loss: 789.3292 Acc: 42.2012\n",
      "test Loss: 790.0303 Acc: 42.9008\n",
      "\n",
      "Epoch 219/1199\n",
      "------------------------\n",
      "train Loss: 789.0929 Acc: 43.0254\n",
      "test Loss: 804.9451 Acc: 43.1824\n",
      "\n",
      "Epoch 220/1199\n",
      "------------------------\n",
      "train Loss: 791.6926 Acc: 44.0589\n",
      "test Loss: 820.7652 Acc: 43.7792\n",
      "\n",
      "Epoch 221/1199\n",
      "------------------------\n",
      "train Loss: 788.0621 Acc: 42.7995\n",
      "test Loss: 794.2371 Acc: 41.7109\n",
      "\n",
      "Epoch 222/1199\n",
      "------------------------\n",
      "train Loss: 799.3451 Acc: 42.6243\n",
      "test Loss: 805.4049 Acc: 42.7969\n",
      "\n",
      "Epoch 223/1199\n",
      "------------------------\n",
      "train Loss: 795.2817 Acc: 43.5482\n",
      "test Loss: 813.2891 Acc: 42.3468\n",
      "\n",
      "Epoch 224/1199\n",
      "------------------------\n",
      "train Loss: 809.8167 Acc: 41.6216\n",
      "test Loss: 783.1736 Acc: 40.8935\n",
      "\n",
      "Epoch 225/1199\n",
      "------------------------\n",
      "train Loss: 797.9986 Acc: 41.2535\n",
      "test Loss: 803.6454 Acc: 42.5900\n",
      "\n",
      "Epoch 226/1199\n",
      "------------------------\n",
      "train Loss: 822.8365 Acc: 42.7391\n",
      "test Loss: 836.3705 Acc: 44.0588\n",
      "\n",
      "Epoch 227/1199\n",
      "------------------------\n",
      "train Loss: 811.6538 Acc: 43.5429\n",
      "test Loss: 812.5594 Acc: 42.9808\n",
      "\n",
      "Epoch 228/1199\n",
      "------------------------\n",
      "train Loss: 819.4999 Acc: 41.9661\n",
      "test Loss: 812.8313 Acc: 41.7211\n",
      "\n",
      "Epoch 229/1199\n",
      "------------------------\n",
      "train Loss: 826.2095 Acc: 41.7516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Loss: 806.3951 Acc: 42.2376\n",
      "\n",
      "Epoch 230/1199\n",
      "------------------------\n",
      "train Loss: 830.5792 Acc: 42.0024\n",
      "test Loss: 843.8925 Acc: 43.0076\n",
      "\n",
      "Epoch 231/1199\n",
      "------------------------\n",
      "train Loss: 839.9271 Acc: 43.0450\n",
      "test Loss: 838.0506 Acc: 43.1865\n",
      "\n",
      "Epoch 232/1199\n",
      "------------------------\n",
      "train Loss: 834.4154 Acc: 42.9768\n",
      "test Loss: 837.0211 Acc: 41.8641\n",
      "\n",
      "Epoch 233/1199\n",
      "------------------------\n",
      "train Loss: 856.5462 Acc: 41.8642\n",
      "test Loss: 884.9680 Acc: 41.0420\n",
      "\n",
      "Epoch 234/1199\n",
      "------------------------\n",
      "train Loss: 876.7892 Acc: 41.3653\n",
      "test Loss: 860.9503 Acc: 42.5478\n",
      "\n",
      "Epoch 235/1199\n",
      "------------------------\n",
      "train Loss: 859.3150 Acc: 43.4042\n",
      "test Loss: 883.9154 Acc: 43.7601\n",
      "\n",
      "Epoch 236/1199\n",
      "------------------------\n",
      "train Loss: 878.9538 Acc: 42.6010\n",
      "test Loss: 874.0275 Acc: 42.8057\n",
      "\n",
      "Epoch 237/1199\n",
      "------------------------\n",
      "train Loss: 883.7461 Acc: 42.7308\n",
      "test Loss: 892.1512 Acc: 42.0931\n",
      "\n",
      "Epoch 238/1199\n",
      "------------------------\n",
      "train Loss: 871.0451 Acc: 42.0650\n",
      "test Loss: 834.6198 Acc: 42.3668\n",
      "\n",
      "Epoch 239/1199\n",
      "------------------------\n",
      "train Loss: 869.0341 Acc: 42.3596\n",
      "test Loss: 877.9843 Acc: 42.9278\n",
      "\n",
      "Epoch 240/1199\n",
      "------------------------\n",
      "train Loss: 869.1113 Acc: 43.8240\n",
      "test Loss: 918.6833 Acc: 43.5115\n",
      "\n",
      "Epoch 241/1199\n",
      "------------------------\n",
      "train Loss: 894.5806 Acc: 42.7231\n",
      "test Loss: 887.5867 Acc: 42.2599\n",
      "\n",
      "Epoch 242/1199\n",
      "------------------------\n",
      "train Loss: 884.7796 Acc: 41.0661\n",
      "test Loss: 879.4526 Acc: 42.6656\n",
      "\n",
      "Epoch 243/1199\n",
      "------------------------\n",
      "train Loss: 888.3007 Acc: 43.8572\n",
      "test Loss: 918.2071 Acc: 44.3941\n",
      "\n",
      "Epoch 244/1199\n",
      "------------------------\n",
      "train Loss: 898.4414 Acc: 43.9088\n",
      "test Loss: 917.3681 Acc: 43.1837\n",
      "\n",
      "Epoch 245/1199\n",
      "------------------------\n",
      "train Loss: 915.3878 Acc: 43.1302\n",
      "test Loss: 908.4619 Acc: 44.0746\n",
      "\n",
      "Epoch 246/1199\n",
      "------------------------\n",
      "train Loss: 911.0442 Acc: 43.3385\n",
      "test Loss: 953.1477 Acc: 43.4820\n",
      "\n",
      "Epoch 247/1199\n",
      "------------------------\n",
      "train Loss: 919.5373 Acc: 43.9673\n",
      "test Loss: 949.7999 Acc: 43.4043\n",
      "\n",
      "Epoch 248/1199\n",
      "------------------------\n",
      "train Loss: 920.1142 Acc: 43.1042\n",
      "test Loss: 917.2629 Acc: 42.2269\n",
      "\n",
      "Epoch 249/1199\n",
      "------------------------\n",
      "train Loss: 936.1870 Acc: 42.4540\n",
      "test Loss: 957.1149 Acc: 41.4323\n",
      "\n",
      "Epoch 250/1199\n",
      "------------------------\n",
      "train Loss: 939.7358 Acc: 43.5629\n",
      "test Loss: 952.1279 Acc: 44.8243\n",
      "\n",
      "Epoch 251/1199\n",
      "------------------------\n",
      "train Loss: 942.4951 Acc: 45.2393\n",
      "test Loss: 969.2989 Acc: 44.2355\n",
      "\n",
      "Epoch 252/1199\n",
      "------------------------\n",
      "train Loss: 938.9681 Acc: 43.0579\n",
      "test Loss: 934.8354 Acc: 43.5644\n",
      "\n",
      "Epoch 253/1199\n",
      "------------------------\n",
      "train Loss: 933.6431 Acc: 43.5504\n",
      "test Loss: 934.3661 Acc: 44.0851\n",
      "\n",
      "Epoch 254/1199\n",
      "------------------------\n",
      "train Loss: 939.8995 Acc: 44.2138\n",
      "test Loss: 914.9827 Acc: 44.3955\n",
      "\n",
      "Epoch 255/1199\n",
      "------------------------\n",
      "train Loss: 936.7482 Acc: 43.9277\n",
      "test Loss: 971.9699 Acc: 42.3304\n",
      "\n",
      "Epoch 256/1199\n",
      "------------------------\n",
      "train Loss: 956.3165 Acc: 41.7552\n",
      "test Loss: 945.4113 Acc: 43.3041\n",
      "\n",
      "Epoch 257/1199\n",
      "------------------------\n",
      "train Loss: 976.2748 Acc: 43.3636\n",
      "test Loss: 979.9115 Acc: 44.9008\n",
      "\n",
      "Epoch 258/1199\n",
      "------------------------\n",
      "train Loss: 977.4599 Acc: 44.4873\n",
      "test Loss: 976.3258 Acc: 44.3925\n",
      "\n",
      "Epoch 259/1199\n",
      "------------------------\n",
      "train Loss: 980.4529 Acc: 44.2184\n",
      "test Loss: 971.0931 Acc: 43.6899\n",
      "\n",
      "Epoch 260/1199\n",
      "------------------------\n",
      "train Loss: 994.2506 Acc: 42.9973\n",
      "test Loss: 993.9192 Acc: 43.7343\n",
      "\n",
      "Epoch 261/1199\n",
      "------------------------\n",
      "train Loss: 993.7476 Acc: 43.8696\n",
      "test Loss: 971.0840 Acc: 42.8362\n",
      "\n",
      "Epoch 262/1199\n",
      "------------------------\n",
      "train Loss: 978.2342 Acc: 43.9689\n",
      "test Loss: 962.6791 Acc: 43.9533\n",
      "\n",
      "Epoch 263/1199\n",
      "------------------------\n",
      "train Loss: 979.0436 Acc: 43.6226\n",
      "test Loss: 1008.7852 Acc: 44.6511\n",
      "\n",
      "Epoch 264/1199\n",
      "------------------------\n",
      "train Loss: 990.1164 Acc: 44.7151\n",
      "test Loss: 966.5630 Acc: 44.8790\n",
      "\n",
      "Epoch 265/1199\n",
      "------------------------\n",
      "train Loss: 975.0018 Acc: 44.9113\n",
      "test Loss: 962.2373 Acc: 44.7072\n",
      "\n",
      "Epoch 266/1199\n",
      "------------------------\n",
      "train Loss: 997.7879 Acc: 44.1631\n",
      "test Loss: 1016.7923 Acc: 43.0554\n",
      "\n",
      "Epoch 267/1199\n",
      "------------------------\n",
      "train Loss: 977.8127 Acc: 43.8847\n",
      "test Loss: 997.4751 Acc: 44.2198\n",
      "\n",
      "Epoch 268/1199\n",
      "------------------------\n",
      "train Loss: 995.5652 Acc: 43.5744\n",
      "test Loss: 997.4009 Acc: 43.1526\n",
      "\n",
      "Epoch 269/1199\n",
      "------------------------\n",
      "train Loss: 979.0011 Acc: 43.6892\n",
      "test Loss: 998.4084 Acc: 43.0450\n",
      "\n",
      "Epoch 270/1199\n",
      "------------------------\n",
      "train Loss: 998.8414 Acc: 43.5492\n",
      "test Loss: 1004.0332 Acc: 43.7523\n",
      "\n",
      "Epoch 271/1199\n",
      "------------------------\n",
      "train Loss: 1017.9149 Acc: 43.8842\n",
      "test Loss: 985.9123 Acc: 45.5326\n",
      "\n",
      "Epoch 272/1199\n",
      "------------------------\n",
      "train Loss: 997.8040 Acc: 44.6686\n",
      "test Loss: 1012.7478 Acc: 44.3081\n",
      "\n",
      "Epoch 273/1199\n",
      "------------------------\n",
      "train Loss: 1012.9248 Acc: 44.9019\n",
      "test Loss: 1000.7860 Acc: 45.0329\n",
      "\n",
      "Epoch 274/1199\n",
      "------------------------\n",
      "train Loss: 1003.0289 Acc: 44.3325\n",
      "test Loss: 1001.4335 Acc: 43.3271\n",
      "\n",
      "Epoch 275/1199\n",
      "------------------------\n",
      "train Loss: 1000.1353 Acc: 43.4478\n",
      "test Loss: 989.6603 Acc: 43.1894\n",
      "\n",
      "Epoch 276/1199\n",
      "------------------------\n",
      "train Loss: 1013.1288 Acc: 44.0823\n",
      "test Loss: 1030.2790 Acc: 43.8449\n",
      "\n",
      "Epoch 277/1199\n",
      "------------------------\n",
      "train Loss: 1024.2668 Acc: 44.1392\n",
      "test Loss: 1023.5858 Acc: 42.7302\n",
      "\n",
      "Epoch 278/1199\n",
      "------------------------\n",
      "train Loss: 1027.1357 Acc: 43.8706\n",
      "test Loss: 1040.2753 Acc: 44.0773\n",
      "\n",
      "Epoch 279/1199\n",
      "------------------------\n",
      "train Loss: 1028.3264 Acc: 44.3285\n",
      "test Loss: 1050.6405 Acc: 44.2348\n",
      "\n",
      "Epoch 280/1199\n",
      "------------------------\n",
      "train Loss: 1052.5761 Acc: 43.7355\n",
      "test Loss: 1039.8083 Acc: 43.5516\n",
      "\n",
      "Epoch 281/1199\n",
      "------------------------\n",
      "train Loss: 1040.6326 Acc: 43.8876\n",
      "test Loss: 1052.9222 Acc: 43.8215\n",
      "\n",
      "Epoch 282/1199\n",
      "------------------------\n",
      "train Loss: 1054.5654 Acc: 43.5609\n",
      "test Loss: 1040.5263 Acc: 43.6735\n",
      "\n",
      "Epoch 283/1199\n",
      "------------------------\n",
      "train Loss: 1038.2295 Acc: 44.0508\n",
      "test Loss: 1049.5410 Acc: 43.1264\n",
      "\n",
      "Epoch 284/1199\n",
      "------------------------\n",
      "train Loss: 1049.5086 Acc: 43.4371\n",
      "test Loss: 1031.7675 Acc: 44.0677\n",
      "\n",
      "Epoch 285/1199\n",
      "------------------------\n",
      "train Loss: 1056.7197 Acc: 43.2870\n",
      "test Loss: 1050.1207 Acc: 41.5183\n",
      "\n",
      "Epoch 286/1199\n",
      "------------------------\n",
      "train Loss: 1069.4450 Acc: 42.9647\n",
      "test Loss: 1063.4606 Acc: 43.1711\n",
      "\n",
      "Epoch 287/1199\n",
      "------------------------\n",
      "train Loss: 1071.2454 Acc: 43.5028\n",
      "test Loss: 1066.4444 Acc: 44.3499\n",
      "\n",
      "Epoch 288/1199\n",
      "------------------------\n",
      "train Loss: 1072.2947 Acc: 43.9705\n",
      "test Loss: 1081.9311 Acc: 44.3740\n",
      "\n",
      "Epoch 289/1199\n",
      "------------------------\n",
      "train Loss: 1073.2259 Acc: 44.2199\n",
      "test Loss: 1088.8127 Acc: 44.2913\n",
      "\n",
      "Epoch 290/1199\n",
      "------------------------\n",
      "train Loss: 1101.6275 Acc: 43.9508\n",
      "test Loss: 1102.2999 Acc: 43.6038\n",
      "\n",
      "Epoch 291/1199\n",
      "------------------------\n",
      "train Loss: 1096.3712 Acc: 43.2840\n",
      "test Loss: 1076.2841 Acc: 43.4986\n",
      "\n",
      "Epoch 292/1199\n",
      "------------------------\n",
      "train Loss: 1101.2220 Acc: 43.7854\n",
      "test Loss: 1118.1177 Acc: 44.1490\n",
      "\n",
      "Epoch 293/1199\n",
      "------------------------\n",
      "train Loss: 1089.2570 Acc: 44.1290\n",
      "test Loss: 1098.0578 Acc: 44.0640\n",
      "\n",
      "Epoch 294/1199\n",
      "------------------------\n",
      "train Loss: 1094.2189 Acc: 44.3770\n",
      "test Loss: 1126.1480 Acc: 43.8690\n",
      "\n",
      "Epoch 295/1199\n",
      "------------------------\n",
      "train Loss: 1117.3590 Acc: 43.2578\n",
      "test Loss: 1153.0460 Acc: 43.1285\n",
      "\n",
      "Epoch 296/1199\n",
      "------------------------\n",
      "train Loss: 1119.8372 Acc: 42.8229\n",
      "test Loss: 1102.5673 Acc: 43.4758\n",
      "\n",
      "Epoch 297/1199\n",
      "------------------------\n",
      "train Loss: 1124.6738 Acc: 44.0022\n",
      "test Loss: 1137.9058 Acc: 44.0816\n",
      "\n",
      "Epoch 298/1199\n",
      "------------------------\n",
      "train Loss: 1120.3990 Acc: 43.6422\n",
      "test Loss: 1156.6785 Acc: 43.7860\n",
      "\n",
      "Epoch 299/1199\n",
      "------------------------\n",
      "train Loss: 1135.6249 Acc: 43.5493\n",
      "test Loss: 1130.7671 Acc: 43.5499\n",
      "\n",
      "Epoch 300/1199\n",
      "------------------------\n",
      "train Loss: 1130.5263 Acc: 44.0893\n",
      "test Loss: 1120.1417 Acc: 44.5714\n",
      "\n",
      "Epoch 301/1199\n",
      "------------------------\n",
      "train Loss: 1118.2737 Acc: 43.9743\n",
      "test Loss: 1098.9214 Acc: 43.8364\n",
      "\n",
      "Epoch 302/1199\n",
      "------------------------\n",
      "train Loss: 1079.2038 Acc: 43.6851\n",
      "test Loss: 1065.3373 Acc: 43.9307\n",
      "\n",
      "Epoch 303/1199\n",
      "------------------------\n",
      "train Loss: 1073.3802 Acc: 43.5330\n",
      "test Loss: 1045.9854 Acc: 43.5658\n",
      "\n",
      "Epoch 304/1199\n",
      "------------------------\n",
      "train Loss: 1035.9488 Acc: 44.0220\n",
      "test Loss: 1025.1404 Acc: 44.3644\n",
      "\n",
      "Epoch 305/1199\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1036.4811 Acc: 43.7166\n",
      "test Loss: 1057.1123 Acc: 43.3457\n",
      "\n",
      "Epoch 306/1199\n",
      "------------------------\n",
      "train Loss: 1039.2197 Acc: 44.3115\n",
      "test Loss: 1024.7192 Acc: 44.4493\n",
      "\n",
      "Epoch 307/1199\n",
      "------------------------\n",
      "train Loss: 1020.5225 Acc: 44.6767\n",
      "test Loss: 1014.2817 Acc: 44.8460\n",
      "\n",
      "Epoch 308/1199\n",
      "------------------------\n",
      "train Loss: 1014.8359 Acc: 44.6163\n",
      "test Loss: 1020.3790 Acc: 45.0131\n",
      "\n",
      "Epoch 309/1199\n",
      "------------------------\n",
      "train Loss: 1005.0994 Acc: 43.7553\n",
      "test Loss: 993.2306 Acc: 44.2792\n",
      "\n",
      "Epoch 310/1199\n",
      "------------------------\n",
      "train Loss: 998.3981 Acc: 43.5276\n",
      "test Loss: 1006.4883 Acc: 44.2920\n",
      "\n",
      "Epoch 311/1199\n",
      "------------------------\n",
      "train Loss: 998.2600 Acc: 44.7743\n",
      "test Loss: 1003.5455 Acc: 45.5205\n",
      "\n",
      "Epoch 312/1199\n",
      "------------------------\n",
      "train Loss: 1006.7290 Acc: 45.2925\n",
      "test Loss: 978.1746 Acc: 45.7294\n",
      "\n",
      "Epoch 313/1199\n",
      "------------------------\n",
      "train Loss: 993.0281 Acc: 45.1289\n",
      "test Loss: 971.9456 Acc: 44.3613\n",
      "\n",
      "Epoch 314/1199\n",
      "------------------------\n",
      "train Loss: 966.9073 Acc: 44.7606\n",
      "test Loss: 960.4371 Acc: 44.0630\n",
      "\n",
      "Epoch 315/1199\n",
      "------------------------\n",
      "train Loss: 985.9166 Acc: 43.8018\n",
      "test Loss: 1002.8955 Acc: 43.8044\n",
      "\n",
      "Epoch 316/1199\n",
      "------------------------\n",
      "train Loss: 981.4820 Acc: 43.9715\n",
      "test Loss: 962.6961 Acc: 43.5400\n",
      "\n",
      "Epoch 317/1199\n",
      "------------------------\n",
      "train Loss: 964.2079 Acc: 45.0143\n",
      "test Loss: 954.5709 Acc: 44.7367\n",
      "\n",
      "Epoch 318/1199\n",
      "------------------------\n",
      "train Loss: 965.9989 Acc: 45.2678\n",
      "test Loss: 961.0480 Acc: 44.1930\n",
      "\n",
      "Epoch 319/1199\n",
      "------------------------\n",
      "train Loss: 963.8567 Acc: 44.4307\n",
      "test Loss: 938.5716 Acc: 44.5217\n",
      "\n",
      "Epoch 320/1199\n",
      "------------------------\n",
      "train Loss: 987.3913 Acc: 43.9495\n",
      "test Loss: 984.4253 Acc: 43.5274\n",
      "\n",
      "Epoch 321/1199\n",
      "------------------------\n",
      "train Loss: 979.7535 Acc: 43.3377\n",
      "test Loss: 974.9800 Acc: 44.6651\n",
      "\n",
      "Epoch 322/1199\n",
      "------------------------\n",
      "train Loss: 959.4650 Acc: 44.7071\n",
      "test Loss: 960.4533 Acc: 46.4270\n",
      "\n",
      "Epoch 323/1199\n",
      "------------------------\n",
      "train Loss: 977.0997 Acc: 45.9082\n",
      "test Loss: 984.9177 Acc: 46.3357\n",
      "\n",
      "Epoch 324/1199\n",
      "------------------------\n",
      "train Loss: 1004.6714 Acc: 45.0707\n",
      "test Loss: 1005.6815 Acc: 44.7417\n",
      "\n",
      "Epoch 325/1199\n",
      "------------------------\n",
      "train Loss: 1007.1384 Acc: 44.2012\n",
      "test Loss: 1000.2246 Acc: 43.8557\n",
      "\n",
      "Epoch 326/1199\n",
      "------------------------\n",
      "train Loss: 995.9904 Acc: 43.2450\n",
      "test Loss: 974.1484 Acc: 42.8238\n",
      "\n",
      "Epoch 327/1199\n",
      "------------------------\n",
      "train Loss: 976.0756 Acc: 44.1476\n",
      "test Loss: 966.3612 Acc: 44.2896\n",
      "\n",
      "Epoch 328/1199\n",
      "------------------------\n",
      "train Loss: 990.2931 Acc: 44.8213\n",
      "test Loss: 1014.1558 Acc: 45.1498\n",
      "\n",
      "Epoch 329/1199\n",
      "------------------------\n",
      "train Loss: 998.5942 Acc: 44.7657\n",
      "test Loss: 1029.0067 Acc: 43.7336\n",
      "\n",
      "Epoch 330/1199\n",
      "------------------------\n",
      "train Loss: 1004.7711 Acc: 44.0957\n",
      "test Loss: 1003.4601 Acc: 44.3996\n",
      "\n",
      "Epoch 331/1199\n",
      "------------------------\n",
      "train Loss: 986.7652 Acc: 43.9798\n",
      "test Loss: 997.9379 Acc: 44.4879\n",
      "\n",
      "Epoch 332/1199\n",
      "------------------------\n",
      "train Loss: 979.3452 Acc: 44.5350\n",
      "test Loss: 964.2159 Acc: 45.0386\n",
      "\n",
      "Epoch 333/1199\n",
      "------------------------\n",
      "train Loss: 1011.6520 Acc: 44.8172\n",
      "test Loss: 1001.5075 Acc: 46.1041\n",
      "\n",
      "Epoch 334/1199\n",
      "------------------------\n",
      "train Loss: 1003.5650 Acc: 45.8994\n",
      "test Loss: 999.0475 Acc: 45.5516\n",
      "\n",
      "Epoch 335/1199\n",
      "------------------------\n",
      "train Loss: 1014.0321 Acc: 45.2248\n",
      "test Loss: 1030.6601 Acc: 43.9759\n",
      "\n",
      "Epoch 336/1199\n",
      "------------------------\n",
      "train Loss: 1036.1005 Acc: 44.0178\n",
      "test Loss: 1018.0212 Acc: 43.2494\n",
      "\n",
      "Epoch 337/1199\n",
      "------------------------\n",
      "train Loss: 1023.4144 Acc: 44.3629\n",
      "test Loss: 1038.6943 Acc: 44.6554\n",
      "\n",
      "Epoch 338/1199\n",
      "------------------------\n",
      "train Loss: 1029.8626 Acc: 45.3686\n",
      "test Loss: 1045.9435 Acc: 45.5115\n",
      "\n",
      "Epoch 339/1199\n",
      "------------------------\n",
      "train Loss: 1024.9233 Acc: 45.2699\n",
      "test Loss: 1002.3382 Acc: 45.5678\n",
      "\n",
      "Epoch 340/1199\n",
      "------------------------\n",
      "train Loss: 1032.8532 Acc: 44.7976\n",
      "test Loss: 1008.2451 Acc: 45.1825\n",
      "\n",
      "Epoch 341/1199\n",
      "------------------------\n",
      "train Loss: 1020.7907 Acc: 44.1209\n",
      "test Loss: 1050.2662 Acc: 43.4136\n",
      "\n",
      "Epoch 342/1199\n",
      "------------------------\n",
      "train Loss: 1053.3128 Acc: 43.2137\n",
      "test Loss: 1067.6969 Acc: 43.5922\n",
      "\n",
      "Epoch 343/1199\n",
      "------------------------\n",
      "train Loss: 1043.4922 Acc: 43.3090\n",
      "test Loss: 1022.9227 Acc: 43.3216\n",
      "\n",
      "Epoch 344/1199\n",
      "------------------------\n",
      "train Loss: 1036.6981 Acc: 43.8486\n",
      "test Loss: 1070.3923 Acc: 43.9095\n",
      "\n",
      "Epoch 345/1199\n",
      "------------------------\n",
      "train Loss: 1032.6114 Acc: 44.4983\n",
      "test Loss: 1039.8857 Acc: 45.3524\n",
      "\n",
      "Epoch 346/1199\n",
      "------------------------\n",
      "train Loss: 1054.6158 Acc: 45.4522\n",
      "test Loss: 1037.0902 Acc: 46.2022\n",
      "\n",
      "Epoch 347/1199\n",
      "------------------------\n",
      "train Loss: 1044.7320 Acc: 45.2865\n",
      "test Loss: 1056.8562 Acc: 44.2158\n",
      "\n",
      "Epoch 348/1199\n",
      "------------------------\n",
      "train Loss: 1036.3914 Acc: 44.0375\n",
      "test Loss: 1032.8553 Acc: 44.2124\n",
      "\n",
      "Epoch 349/1199\n",
      "------------------------\n",
      "train Loss: 1054.7571 Acc: 43.9723\n",
      "test Loss: 1067.4641 Acc: 44.4043\n",
      "\n",
      "Epoch 350/1199\n",
      "------------------------\n",
      "train Loss: 1046.2964 Acc: 44.6782\n",
      "test Loss: 1081.5387 Acc: 45.0593\n",
      "\n",
      "Epoch 351/1199\n",
      "------------------------\n",
      "train Loss: 1083.2067 Acc: 44.0544\n",
      "test Loss: 1071.2323 Acc: 43.4309\n",
      "\n",
      "Epoch 352/1199\n",
      "------------------------\n",
      "train Loss: 1071.1432 Acc: 43.6931\n",
      "test Loss: 1098.2719 Acc: 42.2345\n",
      "\n",
      "Epoch 353/1199\n",
      "------------------------\n",
      "train Loss: 1058.5198 Acc: 42.5242\n",
      "test Loss: 1046.4774 Acc: 43.0606\n",
      "\n",
      "Epoch 354/1199\n",
      "------------------------\n",
      "train Loss: 1055.2516 Acc: 43.5617\n",
      "test Loss: 1072.8660 Acc: 43.6438\n",
      "\n",
      "Epoch 355/1199\n",
      "------------------------\n",
      "train Loss: 1075.1326 Acc: 44.2624\n",
      "test Loss: 1075.3230 Acc: 44.7828\n",
      "\n",
      "Epoch 356/1199\n",
      "------------------------\n",
      "train Loss: 1053.4799 Acc: 44.5256\n",
      "test Loss: 1054.6479 Acc: 44.8973\n",
      "\n",
      "Epoch 357/1199\n",
      "------------------------\n",
      "train Loss: 1075.7945 Acc: 43.7397\n",
      "test Loss: 1033.8490 Acc: 43.3273\n",
      "\n",
      "Epoch 358/1199\n",
      "------------------------\n",
      "train Loss: 1067.8027 Acc: 43.5560\n",
      "test Loss: 1076.3237 Acc: 42.7699\n",
      "\n",
      "Epoch 359/1199\n",
      "------------------------\n",
      "train Loss: 1068.2006 Acc: 42.9490\n",
      "test Loss: 1059.1285 Acc: 42.7371\n",
      "\n",
      "Epoch 360/1199\n",
      "------------------------\n",
      "train Loss: 1054.8042 Acc: 43.2394\n",
      "test Loss: 1080.0865 Acc: 44.2397\n",
      "\n",
      "Epoch 361/1199\n",
      "------------------------\n",
      "train Loss: 1058.7901 Acc: 44.0452\n",
      "test Loss: 1085.7688 Acc: 43.4141\n",
      "\n",
      "Epoch 362/1199\n",
      "------------------------\n",
      "train Loss: 1066.0284 Acc: 44.5550\n",
      "test Loss: 1065.2851 Acc: 44.3473\n",
      "\n",
      "Epoch 363/1199\n",
      "------------------------\n",
      "train Loss: 1057.4013 Acc: 43.9113\n",
      "test Loss: 1056.5877 Acc: 44.2599\n",
      "\n",
      "Epoch 364/1199\n",
      "------------------------\n",
      "train Loss: 1049.4456 Acc: 44.4487\n",
      "test Loss: 1042.6717 Acc: 45.0402\n",
      "\n",
      "Epoch 365/1199\n",
      "------------------------\n",
      "train Loss: 1068.5647 Acc: 44.6472\n",
      "test Loss: 1080.1380 Acc: 44.5058\n",
      "\n",
      "Epoch 366/1199\n",
      "------------------------\n",
      "train Loss: 1100.1719 Acc: 44.8774\n",
      "test Loss: 1114.3595 Acc: 44.9816\n",
      "\n",
      "Epoch 367/1199\n",
      "------------------------\n",
      "train Loss: 1100.2339 Acc: 44.1615\n",
      "test Loss: 1087.2293 Acc: 44.1367\n",
      "\n",
      "Epoch 368/1199\n",
      "------------------------\n",
      "train Loss: 1077.8879 Acc: 44.3815\n",
      "test Loss: 1052.1848 Acc: 44.9022\n",
      "\n",
      "Epoch 369/1199\n",
      "------------------------\n",
      "train Loss: 1074.0195 Acc: 44.5544\n",
      "test Loss: 1067.2334 Acc: 43.4074\n",
      "\n",
      "Epoch 370/1199\n",
      "------------------------\n",
      "train Loss: 1083.6810 Acc: 43.8347\n",
      "test Loss: 1081.2211 Acc: 44.1694\n",
      "\n",
      "Epoch 371/1199\n",
      "------------------------\n",
      "train Loss: 1081.6811 Acc: 44.1680\n",
      "test Loss: 1115.2322 Acc: 43.0965\n",
      "\n",
      "Epoch 372/1199\n",
      "------------------------\n",
      "train Loss: 1090.7571 Acc: 44.2239\n",
      "test Loss: 1104.7483 Acc: 43.8727\n",
      "\n",
      "Epoch 373/1199\n",
      "------------------------\n",
      "train Loss: 1099.6326 Acc: 44.3967\n",
      "test Loss: 1103.1101 Acc: 44.1051\n",
      "\n",
      "Epoch 374/1199\n",
      "------------------------\n",
      "train Loss: 1102.4559 Acc: 43.9940\n",
      "test Loss: 1081.6203 Acc: 43.5732\n",
      "\n",
      "Epoch 375/1199\n",
      "------------------------\n",
      "train Loss: 1103.6046 Acc: 43.5402\n",
      "test Loss: 1129.5642 Acc: 43.0608\n",
      "\n",
      "Epoch 376/1199\n",
      "------------------------\n",
      "train Loss: 1112.9004 Acc: 43.4938\n",
      "test Loss: 1095.0942 Acc: 44.2709\n",
      "\n",
      "Epoch 377/1199\n",
      "------------------------\n",
      "train Loss: 1085.1763 Acc: 45.0505\n",
      "test Loss: 1104.6012 Acc: 44.7709\n",
      "\n",
      "Epoch 378/1199\n",
      "------------------------\n",
      "train Loss: 1102.8473 Acc: 44.6122\n",
      "test Loss: 1144.3622 Acc: 44.1740\n",
      "\n",
      "Epoch 379/1199\n",
      "------------------------\n",
      "train Loss: 1092.9889 Acc: 45.1210\n",
      "test Loss: 1095.8316 Acc: 43.5190\n",
      "\n",
      "Epoch 380/1199\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1091.3732 Acc: 44.5732\n",
      "test Loss: 1089.4245 Acc: 43.7975\n",
      "\n",
      "Epoch 381/1199\n",
      "------------------------\n",
      "train Loss: 1106.0187 Acc: 43.8446\n",
      "test Loss: 1092.2378 Acc: 44.6200\n",
      "\n",
      "Epoch 382/1199\n",
      "------------------------\n",
      "train Loss: 1106.7179 Acc: 43.7159\n",
      "test Loss: 1126.6593 Acc: 44.8281\n",
      "\n",
      "Epoch 383/1199\n",
      "------------------------\n",
      "train Loss: 1105.0993 Acc: 44.7004\n",
      "test Loss: 1127.0195 Acc: 43.8841\n",
      "\n",
      "Epoch 384/1199\n",
      "------------------------\n",
      "train Loss: 1103.4154 Acc: 45.3680\n",
      "test Loss: 1086.0782 Acc: 45.7075\n",
      "\n",
      "Epoch 385/1199\n",
      "------------------------\n",
      "train Loss: 1082.8994 Acc: 44.9925\n",
      "test Loss: 1078.7584 Acc: 44.3701\n",
      "\n",
      "Epoch 386/1199\n",
      "------------------------\n",
      "train Loss: 1100.5462 Acc: 44.1775\n",
      "test Loss: 1120.2014 Acc: 42.6081\n",
      "\n",
      "Epoch 387/1199\n",
      "------------------------\n",
      "train Loss: 1106.9902 Acc: 43.8603\n",
      "test Loss: 1114.8228 Acc: 44.7144\n",
      "\n",
      "Epoch 388/1199\n",
      "------------------------\n",
      "train Loss: 1114.2677 Acc: 44.4211\n",
      "test Loss: 1089.1380 Acc: 44.5362\n",
      "\n",
      "Epoch 389/1199\n",
      "------------------------\n",
      "train Loss: 1119.4592 Acc: 44.1150\n",
      "test Loss: 1114.2178 Acc: 44.7742\n",
      "\n",
      "Epoch 390/1199\n",
      "------------------------\n",
      "train Loss: 1137.7393 Acc: 44.1015\n",
      "test Loss: 1145.5098 Acc: 44.4853\n",
      "\n",
      "Epoch 391/1199\n",
      "------------------------\n",
      "train Loss: 1144.3018 Acc: 44.6400\n",
      "test Loss: 1128.5207 Acc: 43.8749\n",
      "\n",
      "Epoch 392/1199\n",
      "------------------------\n",
      "train Loss: 1131.5086 Acc: 44.6816\n",
      "test Loss: 1155.0259 Acc: 44.4108\n",
      "\n",
      "Epoch 393/1199\n",
      "------------------------\n",
      "train Loss: 1141.8407 Acc: 44.5413\n",
      "test Loss: 1138.5435 Acc: 45.0445\n",
      "\n",
      "Epoch 394/1199\n",
      "------------------------\n",
      "train Loss: 1142.3353 Acc: 44.6486\n",
      "test Loss: 1164.7384 Acc: 44.4006\n",
      "\n",
      "Epoch 395/1199\n",
      "------------------------\n",
      "train Loss: 1162.9642 Acc: 44.8315\n",
      "test Loss: 1151.2283 Acc: 44.4571\n",
      "\n",
      "Epoch 396/1199\n",
      "------------------------\n",
      "train Loss: 1136.8893 Acc: 44.9103\n",
      "test Loss: 1150.4908 Acc: 45.7403\n",
      "\n",
      "Epoch 397/1199\n",
      "------------------------\n",
      "train Loss: 1143.4020 Acc: 44.5673\n",
      "test Loss: 1131.4663 Acc: 44.8536\n",
      "\n",
      "Epoch 398/1199\n",
      "------------------------\n",
      "train Loss: 1154.9441 Acc: 44.8218\n",
      "test Loss: 1165.2913 Acc: 42.5363\n",
      "\n",
      "Epoch 399/1199\n",
      "------------------------\n",
      "train Loss: 1156.1066 Acc: 44.4773\n",
      "test Loss: 1169.6308 Acc: 44.4550\n",
      "\n",
      "Epoch 400/1199\n",
      "------------------------\n",
      "train Loss: 1174.1699 Acc: 44.1737\n",
      "test Loss: 1190.7153 Acc: 43.8732\n",
      "\n",
      "Epoch 401/1199\n",
      "------------------------\n",
      "train Loss: 1139.5690 Acc: 44.6274\n",
      "test Loss: 1166.2545 Acc: 44.4587\n",
      "\n",
      "Epoch 402/1199\n",
      "------------------------\n",
      "train Loss: 1134.9041 Acc: 44.3396\n",
      "test Loss: 1151.3188 Acc: 45.3573\n",
      "\n",
      "Epoch 403/1199\n",
      "------------------------\n",
      "train Loss: 1121.5175 Acc: 45.0992\n",
      "test Loss: 1135.9682 Acc: 44.3433\n",
      "\n",
      "Epoch 404/1199\n",
      "------------------------\n",
      "train Loss: 1143.6563 Acc: 44.1944\n",
      "test Loss: 1136.1431 Acc: 44.4898\n",
      "\n",
      "Epoch 405/1199\n",
      "------------------------\n",
      "train Loss: 1137.2400 Acc: 44.2150\n",
      "test Loss: 1151.6107 Acc: 43.9962\n",
      "\n",
      "Epoch 406/1199\n",
      "------------------------\n",
      "train Loss: 1146.4935 Acc: 43.8538\n",
      "test Loss: 1131.1595 Acc: 44.3860\n",
      "\n",
      "Epoch 407/1199\n",
      "------------------------\n",
      "train Loss: 1148.8065 Acc: 43.8756\n",
      "test Loss: 1161.8317 Acc: 43.2045\n",
      "\n",
      "Epoch 408/1199\n",
      "------------------------\n",
      "train Loss: 1147.3415 Acc: 44.4080\n",
      "test Loss: 1144.3340 Acc: 44.7877\n",
      "\n",
      "Epoch 409/1199\n",
      "------------------------\n",
      "train Loss: 1165.8798 Acc: 45.4016\n",
      "test Loss: 1176.3516 Acc: 44.9664\n",
      "\n",
      "Epoch 410/1199\n",
      "------------------------\n",
      "train Loss: 1189.3489 Acc: 44.5275\n",
      "test Loss: 1163.9979 Acc: 44.9928\n",
      "\n",
      "Epoch 411/1199\n",
      "------------------------\n",
      "train Loss: 1178.4383 Acc: 44.4201\n",
      "test Loss: 1172.5305 Acc: 44.3098\n",
      "\n",
      "Epoch 412/1199\n",
      "------------------------\n",
      "train Loss: 1159.8362 Acc: 44.7448\n",
      "test Loss: 1170.0145 Acc: 44.5201\n",
      "\n",
      "Epoch 413/1199\n",
      "------------------------\n",
      "train Loss: 1170.1904 Acc: 44.9489\n",
      "test Loss: 1168.2233 Acc: 45.0179\n",
      "\n",
      "Epoch 414/1199\n",
      "------------------------\n",
      "train Loss: 1186.8381 Acc: 44.3165\n",
      "test Loss: 1219.1385 Acc: 43.9397\n",
      "\n",
      "Epoch 415/1199\n",
      "------------------------\n",
      "train Loss: 1164.4785 Acc: 44.2371\n",
      "test Loss: 1188.1601 Acc: 43.6015\n",
      "\n",
      "Epoch 416/1199\n",
      "------------------------\n",
      "train Loss: 1166.3242 Acc: 42.9600\n",
      "test Loss: 1186.1901 Acc: 42.1692\n",
      "\n",
      "Epoch 417/1199\n",
      "------------------------\n",
      "train Loss: 1145.4310 Acc: 42.8077\n",
      "test Loss: 1161.5863 Acc: 43.4324\n",
      "\n",
      "Epoch 418/1199\n",
      "------------------------\n",
      "train Loss: 1143.6848 Acc: 44.2454\n",
      "test Loss: 1188.1235 Acc: 44.4215\n",
      "\n",
      "Epoch 419/1199\n",
      "------------------------\n",
      "train Loss: 1154.6995 Acc: 44.4447\n",
      "test Loss: 1162.1099 Acc: 44.7029\n",
      "\n",
      "Epoch 420/1199\n",
      "------------------------\n",
      "train Loss: 1139.2847 Acc: 44.7723\n",
      "test Loss: 1198.6198 Acc: 44.8333\n",
      "\n",
      "Epoch 421/1199\n",
      "------------------------\n",
      "train Loss: 1177.2602 Acc: 45.4859\n",
      "test Loss: 1171.3393 Acc: 45.0188\n",
      "\n",
      "Epoch 422/1199\n",
      "------------------------\n",
      "train Loss: 1172.6691 Acc: 44.6500\n",
      "test Loss: 1182.0529 Acc: 43.1974\n",
      "\n",
      "Epoch 423/1199\n",
      "------------------------\n",
      "train Loss: 1189.8118 Acc: 44.5423\n",
      "test Loss: 1212.3074 Acc: 44.2423\n",
      "\n",
      "Epoch 424/1199\n",
      "------------------------\n",
      "train Loss: 1187.8167 Acc: 44.3203\n",
      "test Loss: 1201.3168 Acc: 44.4369\n",
      "\n",
      "Epoch 425/1199\n",
      "------------------------\n",
      "train Loss: 1211.9128 Acc: 44.0035\n",
      "test Loss: 1202.8150 Acc: 43.4378\n",
      "\n",
      "Epoch 426/1199\n",
      "------------------------\n",
      "train Loss: 1193.7391 Acc: 43.0156\n",
      "test Loss: 1191.8452 Acc: 43.8250\n",
      "\n",
      "Epoch 427/1199\n",
      "------------------------\n",
      "train Loss: 1201.9783 Acc: 43.9133\n",
      "test Loss: 1202.8979 Acc: 45.1833\n",
      "\n",
      "Epoch 428/1199\n",
      "------------------------\n",
      "train Loss: 1212.5056 Acc: 44.4575\n",
      "test Loss: 1207.5659 Acc: 44.5739\n",
      "\n",
      "Epoch 429/1199\n",
      "------------------------\n",
      "train Loss: 1212.6675 Acc: 45.4470\n",
      "test Loss: 1236.0517 Acc: 44.3452\n",
      "\n",
      "Epoch 430/1199\n",
      "------------------------\n",
      "train Loss: 1236.0846 Acc: 44.6068\n",
      "test Loss: 1240.9591 Acc: 44.4048\n",
      "\n",
      "Epoch 431/1199\n",
      "------------------------\n",
      "train Loss: 1218.8401 Acc: 43.9529\n",
      "test Loss: 1208.6586 Acc: 44.3828\n",
      "\n",
      "Epoch 432/1199\n",
      "------------------------\n",
      "train Loss: 1221.2904 Acc: 44.1518\n",
      "test Loss: 1261.6858 Acc: 44.2744\n",
      "\n",
      "Epoch 433/1199\n",
      "------------------------\n",
      "train Loss: 1241.1289 Acc: 44.3220\n",
      "test Loss: 1252.9462 Acc: 44.9131\n",
      "\n",
      "Epoch 434/1199\n",
      "------------------------\n",
      "train Loss: 1260.6989 Acc: 44.0231\n",
      "test Loss: 1244.9986 Acc: 45.1072\n",
      "\n",
      "Epoch 435/1199\n",
      "------------------------\n",
      "train Loss: 1234.7289 Acc: 44.1699\n",
      "test Loss: 1231.0347 Acc: 45.5776\n",
      "\n",
      "Epoch 436/1199\n",
      "------------------------\n",
      "train Loss: 1243.7834 Acc: 44.9103\n",
      "test Loss: 1248.4593 Acc: 44.1808\n",
      "\n",
      "Epoch 437/1199\n",
      "------------------------\n",
      "train Loss: 1248.0474 Acc: 43.7848\n",
      "test Loss: 1252.0474 Acc: 43.9993\n",
      "\n",
      "Epoch 438/1199\n",
      "------------------------\n",
      "train Loss: 1256.1273 Acc: 43.0837\n",
      "test Loss: 1246.9410 Acc: 42.8516\n",
      "\n",
      "Epoch 439/1199\n",
      "------------------------\n",
      "train Loss: 1258.6059 Acc: 43.2369\n",
      "test Loss: 1255.2115 Acc: 44.9910\n",
      "\n",
      "Epoch 440/1199\n",
      "------------------------\n",
      "train Loss: 1262.5959 Acc: 44.7824\n",
      "test Loss: 1252.2878 Acc: 46.3142\n",
      "\n",
      "Epoch 441/1199\n",
      "------------------------\n",
      "train Loss: 1240.8131 Acc: 45.4763\n",
      "test Loss: 1242.7071 Acc: 44.7718\n",
      "\n",
      "Epoch 442/1199\n",
      "------------------------\n",
      "train Loss: 1248.5876 Acc: 45.4500\n",
      "test Loss: 1246.8547 Acc: 44.2224\n",
      "\n",
      "Epoch 443/1199\n",
      "------------------------\n",
      "train Loss: 1249.6063 Acc: 44.7023\n",
      "test Loss: 1233.1861 Acc: 44.1352\n",
      "\n",
      "Epoch 444/1199\n",
      "------------------------\n",
      "train Loss: 1246.5840 Acc: 44.4853\n",
      "test Loss: 1253.3648 Acc: 44.6851\n",
      "\n",
      "Epoch 445/1199\n",
      "------------------------\n",
      "train Loss: 1235.5235 Acc: 45.1884\n",
      "test Loss: 1245.1555 Acc: 45.4505\n",
      "\n",
      "Epoch 446/1199\n",
      "------------------------\n",
      "train Loss: 1245.5589 Acc: 45.3203\n",
      "test Loss: 1241.6173 Acc: 45.3652\n",
      "\n",
      "Epoch 447/1199\n",
      "------------------------\n",
      "train Loss: 1255.4332 Acc: 45.7626\n",
      "test Loss: 1222.9082 Acc: 46.1443\n",
      "\n",
      "Epoch 448/1199\n",
      "------------------------\n",
      "train Loss: 1253.1596 Acc: 45.6914\n",
      "test Loss: 1235.2686 Acc: 45.4970\n",
      "\n",
      "Epoch 449/1199\n",
      "------------------------\n",
      "train Loss: 1272.8440 Acc: 44.3154\n",
      "test Loss: 1237.3371 Acc: 44.8284\n",
      "\n",
      "Epoch 450/1199\n",
      "------------------------\n",
      "train Loss: 1243.7645 Acc: 44.5520\n",
      "test Loss: 1253.0623 Acc: 44.0740\n",
      "\n",
      "Epoch 451/1199\n",
      "------------------------\n",
      "train Loss: 1235.2926 Acc: 44.3406\n",
      "test Loss: 1238.3513 Acc: 44.2307\n",
      "\n",
      "Epoch 452/1199\n",
      "------------------------\n",
      "train Loss: 1246.6093 Acc: 44.6594\n",
      "test Loss: 1232.5250 Acc: 44.0400\n",
      "\n",
      "Epoch 453/1199\n",
      "------------------------\n",
      "train Loss: 1238.0477 Acc: 45.0726\n",
      "test Loss: 1218.8028 Acc: 44.7158\n",
      "\n",
      "Epoch 454/1199\n",
      "------------------------\n",
      "train Loss: 1220.6316 Acc: 44.6863\n",
      "test Loss: 1210.1965 Acc: 44.3264\n",
      "\n",
      "Epoch 455/1199\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1208.5849 Acc: 44.1139\n",
      "test Loss: 1207.4293 Acc: 43.6069\n",
      "\n",
      "Epoch 456/1199\n",
      "------------------------\n",
      "train Loss: 1250.5971 Acc: 43.2765\n",
      "test Loss: 1224.8772 Acc: 43.5595\n",
      "\n",
      "Epoch 457/1199\n",
      "------------------------\n",
      "train Loss: 1261.7214 Acc: 43.6273\n",
      "test Loss: 1265.2171 Acc: 44.7238\n",
      "\n",
      "Epoch 458/1199\n",
      "------------------------\n",
      "train Loss: 1257.6590 Acc: 44.7292\n",
      "test Loss: 1250.1610 Acc: 45.2728\n",
      "\n",
      "Epoch 459/1199\n",
      "------------------------\n",
      "train Loss: 1261.7216 Acc: 45.3430\n",
      "test Loss: 1289.5694 Acc: 45.0255\n",
      "\n",
      "Epoch 460/1199\n",
      "------------------------\n",
      "train Loss: 1278.8424 Acc: 45.2810\n",
      "test Loss: 1274.0664 Acc: 44.9151\n",
      "\n",
      "Epoch 461/1199\n",
      "------------------------\n",
      "train Loss: 1281.5095 Acc: 44.1614\n",
      "test Loss: 1266.0716 Acc: 44.2018\n",
      "\n",
      "Epoch 462/1199\n",
      "------------------------\n",
      "train Loss: 1276.4590 Acc: 44.1844\n",
      "test Loss: 1282.7143 Acc: 43.6217\n",
      "\n",
      "Epoch 463/1199\n",
      "------------------------\n",
      "train Loss: 1286.3227 Acc: 44.0076\n",
      "test Loss: 1314.0259 Acc: 43.6462\n",
      "\n",
      "Epoch 464/1199\n",
      "------------------------\n",
      "train Loss: 1301.3019 Acc: 44.0293\n",
      "test Loss: 1317.8508 Acc: 43.5065\n",
      "\n",
      "Epoch 465/1199\n",
      "------------------------\n",
      "train Loss: 1308.4992 Acc: 44.3073\n",
      "test Loss: 1273.5532 Acc: 44.0487\n",
      "\n",
      "Epoch 466/1199\n",
      "------------------------\n",
      "train Loss: 1292.2334 Acc: 44.1334\n",
      "test Loss: 1316.0948 Acc: 43.7801\n",
      "\n",
      "Epoch 467/1199\n",
      "------------------------\n",
      "train Loss: 1301.1987 Acc: 43.8953\n",
      "test Loss: 1330.3103 Acc: 44.0659\n",
      "\n",
      "Epoch 468/1199\n",
      "------------------------\n",
      "train Loss: 1310.9456 Acc: 44.0574\n",
      "test Loss: 1338.3420 Acc: 44.3739\n",
      "\n",
      "Epoch 469/1199\n",
      "------------------------\n",
      "train Loss: 1308.8213 Acc: 44.3037\n",
      "test Loss: 1285.9880 Acc: 45.2139\n",
      "\n",
      "Epoch 470/1199\n",
      "------------------------\n",
      "train Loss: 1294.3109 Acc: 45.1032\n",
      "test Loss: 1263.6549 Acc: 45.7098\n",
      "\n",
      "Epoch 471/1199\n",
      "------------------------\n",
      "train Loss: 1279.3424 Acc: 45.4001\n",
      "test Loss: 1324.2038 Acc: 45.0469\n",
      "\n",
      "Epoch 472/1199\n",
      "------------------------\n",
      "train Loss: 1308.4012 Acc: 44.2680\n",
      "test Loss: 1292.8782 Acc: 43.7632\n",
      "\n",
      "Epoch 473/1199\n",
      "------------------------\n",
      "train Loss: 1292.7813 Acc: 43.5301\n",
      "test Loss: 1310.5683 Acc: 44.1713\n",
      "\n",
      "Epoch 474/1199\n",
      "------------------------\n",
      "train Loss: 1300.2187 Acc: 43.9978\n",
      "test Loss: 1321.7296 Acc: 44.2184\n",
      "\n",
      "Epoch 475/1199\n",
      "------------------------\n",
      "train Loss: 1292.1297 Acc: 44.5685\n",
      "test Loss: 1265.9013 Acc: 44.7404\n",
      "\n",
      "Epoch 476/1199\n",
      "------------------------\n",
      "train Loss: 1270.4855 Acc: 44.9701\n",
      "test Loss: 1306.2707 Acc: 44.0765\n",
      "\n",
      "Epoch 477/1199\n",
      "------------------------\n",
      "train Loss: 1288.1100 Acc: 44.2039\n",
      "test Loss: 1297.6311 Acc: 43.7060\n",
      "\n",
      "Epoch 478/1199\n",
      "------------------------\n",
      "train Loss: 1291.1104 Acc: 43.8826\n",
      "test Loss: 1271.5080 Acc: 44.8799\n",
      "\n",
      "Epoch 479/1199\n",
      "------------------------\n",
      "train Loss: 1277.0786 Acc: 44.7159\n",
      "test Loss: 1296.6781 Acc: 45.0300\n",
      "\n",
      "Epoch 480/1199\n",
      "------------------------\n",
      "train Loss: 1300.3841 Acc: 44.5474\n",
      "test Loss: 1323.0470 Acc: 44.2056\n",
      "\n",
      "Epoch 481/1199\n",
      "------------------------\n",
      "train Loss: 1291.8786 Acc: 44.8546\n",
      "test Loss: 1316.0074 Acc: 43.4677\n",
      "\n",
      "Epoch 482/1199\n",
      "------------------------\n",
      "train Loss: 1319.4003 Acc: 44.0670\n",
      "test Loss: 1299.0441 Acc: 44.8424\n",
      "\n",
      "Epoch 483/1199\n",
      "------------------------\n",
      "train Loss: 1316.7580 Acc: 44.0464\n",
      "test Loss: 1303.0826 Acc: 44.6106\n",
      "\n",
      "Epoch 484/1199\n",
      "------------------------\n",
      "train Loss: 1319.3095 Acc: 44.5921\n",
      "test Loss: 1327.8990 Acc: 44.7022\n",
      "\n",
      "Epoch 485/1199\n",
      "------------------------\n",
      "train Loss: 1332.4395 Acc: 44.4953\n",
      "test Loss: 1326.7506 Acc: 44.9025\n",
      "\n",
      "Epoch 486/1199\n",
      "------------------------\n",
      "train Loss: 1324.1795 Acc: 45.0352\n",
      "test Loss: 1345.2371 Acc: 45.0331\n",
      "\n",
      "Epoch 487/1199\n",
      "------------------------\n",
      "train Loss: 1331.7576 Acc: 44.2531\n",
      "test Loss: 1302.9993 Acc: 44.0297\n",
      "\n",
      "Epoch 488/1199\n",
      "------------------------\n",
      "train Loss: 1313.9439 Acc: 43.9775\n",
      "test Loss: 1290.6861 Acc: 44.7241\n",
      "\n",
      "Epoch 489/1199\n",
      "------------------------\n",
      "train Loss: 1318.9729 Acc: 44.0974\n",
      "test Loss: 1329.6117 Acc: 44.1321\n",
      "\n",
      "Epoch 490/1199\n",
      "------------------------\n",
      "train Loss: 1320.0004 Acc: 44.0306\n",
      "test Loss: 1325.5115 Acc: 44.3911\n",
      "\n",
      "Epoch 491/1199\n",
      "------------------------\n",
      "train Loss: 1328.9499 Acc: 44.1477\n",
      "test Loss: 1361.8673 Acc: 43.8264\n",
      "\n",
      "Epoch 492/1199\n",
      "------------------------\n",
      "train Loss: 1328.6534 Acc: 44.2429\n",
      "test Loss: 1357.0437 Acc: 44.6182\n",
      "\n",
      "Epoch 493/1199\n",
      "------------------------\n",
      "train Loss: 1336.2010 Acc: 44.7837\n",
      "test Loss: 1317.3345 Acc: 45.1378\n",
      "\n",
      "Epoch 494/1199\n",
      "------------------------\n",
      "train Loss: 1346.4349 Acc: 44.2520\n",
      "test Loss: 1327.0139 Acc: 43.6647\n",
      "\n",
      "Epoch 495/1199\n",
      "------------------------\n",
      "train Loss: 1343.4256 Acc: 43.9079\n",
      "test Loss: 1374.5893 Acc: 42.7197\n",
      "\n",
      "Epoch 496/1199\n",
      "------------------------\n",
      "train Loss: 1351.5713 Acc: 43.1956\n",
      "test Loss: 1371.3062 Acc: 42.4839\n",
      "\n",
      "Epoch 497/1199\n",
      "------------------------\n",
      "train Loss: 1363.9290 Acc: 43.5190\n",
      "test Loss: 1379.5226 Acc: 43.8787\n",
      "\n",
      "Epoch 498/1199\n",
      "------------------------\n",
      "train Loss: 1345.8070 Acc: 44.3502\n",
      "test Loss: 1361.1803 Acc: 45.5915\n",
      "\n",
      "Epoch 499/1199\n",
      "------------------------\n",
      "train Loss: 1356.4623 Acc: 45.4584\n",
      "test Loss: 1362.9938 Acc: 45.6229\n",
      "\n",
      "Epoch 500/1199\n",
      "------------------------\n",
      "train Loss: 1368.9220 Acc: 45.5246\n",
      "test Loss: 1355.9335 Acc: 45.8440\n",
      "\n",
      "Epoch 501/1199\n",
      "------------------------\n",
      "train Loss: 1370.1193 Acc: 44.6032\n",
      "test Loss: 1377.9488 Acc: 44.4877\n",
      "\n",
      "Epoch 502/1199\n",
      "------------------------\n",
      "train Loss: 1353.6647 Acc: 43.9041\n",
      "test Loss: 1333.8613 Acc: 44.8017\n",
      "\n",
      "Epoch 503/1199\n",
      "------------------------\n",
      "train Loss: 1348.4517 Acc: 43.8189\n",
      "test Loss: 1352.3328 Acc: 44.3858\n",
      "\n",
      "Epoch 504/1199\n",
      "------------------------\n",
      "train Loss: 1351.3679 Acc: 44.3355\n",
      "test Loss: 1373.5618 Acc: 44.0784\n",
      "\n",
      "Epoch 505/1199\n",
      "------------------------\n",
      "train Loss: 1362.2062 Acc: 44.0667\n",
      "test Loss: 1391.7047 Acc: 43.4089\n",
      "\n",
      "Epoch 506/1199\n",
      "------------------------\n",
      "train Loss: 1378.4426 Acc: 43.7467\n",
      "test Loss: 1361.3199 Acc: 43.0290\n",
      "\n",
      "Epoch 507/1199\n",
      "------------------------\n",
      "train Loss: 1383.9806 Acc: 42.9735\n",
      "test Loss: 1384.9378 Acc: 43.1794\n",
      "\n",
      "Epoch 508/1199\n",
      "------------------------\n",
      "train Loss: 1379.1668 Acc: 44.0819\n",
      "test Loss: 1379.0324 Acc: 44.2419\n",
      "\n",
      "Epoch 509/1199\n",
      "------------------------\n",
      "train Loss: 1365.0677 Acc: 44.5058\n",
      "test Loss: 1366.6233 Acc: 44.6466\n",
      "\n",
      "Epoch 510/1199\n",
      "------------------------\n",
      "train Loss: 1376.0018 Acc: 44.1351\n",
      "test Loss: 1376.8119 Acc: 44.9200\n",
      "\n",
      "Epoch 511/1199\n",
      "------------------------\n",
      "train Loss: 1400.3416 Acc: 44.1148\n",
      "test Loss: 1394.0539 Acc: 44.2538\n",
      "\n",
      "Epoch 512/1199\n",
      "------------------------\n",
      "train Loss: 1434.6462 Acc: 44.3319\n",
      "test Loss: 1417.3049 Acc: 44.6892\n",
      "\n",
      "Epoch 513/1199\n",
      "------------------------\n",
      "train Loss: 1432.1856 Acc: 44.2285\n",
      "test Loss: 1417.9981 Acc: 44.9547\n",
      "\n",
      "Epoch 514/1199\n",
      "------------------------\n",
      "train Loss: 1414.5835 Acc: 44.9485\n",
      "test Loss: 1356.6917 Acc: 44.7037\n",
      "\n",
      "Epoch 515/1199\n",
      "------------------------\n",
      "train Loss: 1389.2847 Acc: 45.0433\n",
      "test Loss: 1411.6692 Acc: 44.9193\n",
      "\n",
      "Epoch 516/1199\n",
      "------------------------\n",
      "train Loss: 1397.5433 Acc: 44.5497\n",
      "test Loss: 1382.1995 Acc: 43.7101\n",
      "\n",
      "Epoch 517/1199\n",
      "------------------------\n",
      "train Loss: 1381.4929 Acc: 44.3192\n",
      "test Loss: 1390.2330 Acc: 44.0642\n",
      "\n",
      "Epoch 518/1199\n",
      "------------------------\n",
      "train Loss: 1361.6122 Acc: 44.3403\n",
      "test Loss: 1386.4536 Acc: 44.2010\n",
      "\n",
      "Epoch 519/1199\n",
      "------------------------\n",
      "train Loss: 1390.7417 Acc: 44.0164\n",
      "test Loss: 1394.8163 Acc: 44.1229\n",
      "\n",
      "Epoch 520/1199\n",
      "------------------------\n",
      "train Loss: 1410.2329 Acc: 43.4822\n",
      "test Loss: 1405.9006 Acc: 43.0185\n",
      "\n",
      "Epoch 521/1199\n",
      "------------------------\n",
      "train Loss: 1422.9384 Acc: 43.8516\n",
      "test Loss: 1389.9215 Acc: 41.9952\n",
      "\n",
      "Epoch 522/1199\n",
      "------------------------\n",
      "train Loss: 1417.5871 Acc: 42.9359\n",
      "test Loss: 1397.0398 Acc: 42.9226\n",
      "\n",
      "Epoch 523/1199\n",
      "------------------------\n",
      "train Loss: 1361.2418 Acc: 44.0175\n",
      "test Loss: 1351.5834 Acc: 44.2397\n",
      "\n",
      "Epoch 524/1199\n",
      "------------------------\n",
      "train Loss: 1377.0068 Acc: 44.4308\n",
      "test Loss: 1367.3341 Acc: 45.0764\n",
      "\n",
      "Epoch 525/1199\n",
      "------------------------\n",
      "train Loss: 1392.7425 Acc: 44.4764\n",
      "test Loss: 1425.0532 Acc: 44.7500\n",
      "\n",
      "Epoch 526/1199\n",
      "------------------------\n",
      "train Loss: 1422.5928 Acc: 44.5884\n",
      "test Loss: 1428.3849 Acc: 44.7469\n",
      "\n",
      "Epoch 527/1199\n",
      "------------------------\n",
      "train Loss: 1420.6826 Acc: 44.5855\n",
      "test Loss: 1415.2871 Acc: 44.4129\n",
      "\n",
      "Epoch 528/1199\n",
      "------------------------\n",
      "train Loss: 1408.4546 Acc: 44.5796\n",
      "test Loss: 1391.0622 Acc: 44.5431\n",
      "\n",
      "Epoch 529/1199\n",
      "------------------------\n",
      "train Loss: 1424.8220 Acc: 44.7674\n",
      "test Loss: 1399.2766 Acc: 45.1612\n",
      "\n",
      "Epoch 530/1199\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1437.6074 Acc: 44.2163\n",
      "test Loss: 1406.0157 Acc: 44.6305\n",
      "\n",
      "Epoch 531/1199\n",
      "------------------------\n",
      "train Loss: 1412.5190 Acc: 44.9529\n",
      "test Loss: 1423.8243 Acc: 44.6127\n",
      "\n",
      "Epoch 532/1199\n",
      "------------------------\n",
      "train Loss: 1405.3536 Acc: 44.8794\n",
      "test Loss: 1414.3859 Acc: 44.6220\n",
      "\n",
      "Epoch 533/1199\n",
      "------------------------\n",
      "train Loss: 1430.3747 Acc: 43.7635\n",
      "test Loss: 1436.0387 Acc: 43.9666\n",
      "\n",
      "Epoch 534/1199\n",
      "------------------------\n",
      "train Loss: 1453.9456 Acc: 44.0667\n",
      "test Loss: 1443.5215 Acc: 43.9578\n",
      "\n",
      "Epoch 535/1199\n",
      "------------------------\n",
      "train Loss: 1442.0626 Acc: 43.8959\n",
      "test Loss: 1476.7793 Acc: 42.7653\n",
      "\n",
      "Epoch 536/1199\n",
      "------------------------\n",
      "train Loss: 1446.7710 Acc: 44.2726\n",
      "test Loss: 1472.2987 Acc: 44.6148\n",
      "\n",
      "Epoch 537/1199\n",
      "------------------------\n",
      "train Loss: 1435.0078 Acc: 44.4990\n",
      "test Loss: 1426.4219 Acc: 45.1188\n",
      "\n",
      "Epoch 538/1199\n",
      "------------------------\n",
      "train Loss: 1457.2352 Acc: 44.8709\n",
      "test Loss: 1487.4379 Acc: 45.4787\n",
      "\n",
      "Epoch 539/1199\n",
      "------------------------\n",
      "train Loss: 1456.0583 Acc: 45.0697\n",
      "test Loss: 1487.5216 Acc: 45.3409\n",
      "\n",
      "Epoch 540/1199\n",
      "------------------------\n",
      "train Loss: 1436.8098 Acc: 45.1783\n",
      "test Loss: 1444.3149 Acc: 45.1369\n",
      "\n",
      "Epoch 541/1199\n",
      "------------------------\n",
      "train Loss: 1431.2843 Acc: 44.3886\n",
      "test Loss: 1464.3604 Acc: 44.1354\n",
      "\n",
      "Epoch 542/1199\n",
      "------------------------\n",
      "train Loss: 1463.8566 Acc: 43.6723\n",
      "test Loss: 1461.4983 Acc: 44.4176\n",
      "\n",
      "Epoch 543/1199\n",
      "------------------------\n",
      "train Loss: 1475.2173 Acc: 44.2447\n",
      "test Loss: 1456.2925 Acc: 44.0949\n",
      "\n",
      "Epoch 544/1199\n",
      "------------------------\n",
      "train Loss: 1419.0560 Acc: 44.5463\n",
      "test Loss: 1454.3015 Acc: 45.1222\n",
      "\n",
      "Epoch 545/1199\n",
      "------------------------\n",
      "train Loss: 1437.0813 Acc: 45.4431\n",
      "test Loss: 1421.8057 Acc: 45.1440\n",
      "\n",
      "Epoch 546/1199\n",
      "------------------------\n",
      "train Loss: 1426.4249 Acc: 44.8894\n",
      "test Loss: 1424.5360 Acc: 45.2702\n",
      "\n",
      "Epoch 547/1199\n",
      "------------------------\n",
      "train Loss: 1440.4045 Acc: 44.3865\n",
      "test Loss: 1400.3991 Acc: 44.8897\n",
      "\n",
      "Epoch 548/1199\n",
      "------------------------\n",
      "train Loss: 1431.7141 Acc: 43.7227\n",
      "test Loss: 1413.0353 Acc: 43.9455\n",
      "\n",
      "Epoch 549/1199\n",
      "------------------------\n",
      "train Loss: 1445.4529 Acc: 43.9989\n",
      "test Loss: 1478.2381 Acc: 44.0633\n",
      "\n",
      "Epoch 550/1199\n",
      "------------------------\n",
      "train Loss: 1453.1528 Acc: 44.0221\n",
      "test Loss: 1456.0978 Acc: 43.7414\n",
      "\n",
      "Epoch 551/1199\n",
      "------------------------\n",
      "train Loss: 1491.7672 Acc: 44.7245\n",
      "test Loss: 1491.2746 Acc: 44.1540\n",
      "\n",
      "Epoch 552/1199\n",
      "------------------------\n",
      "train Loss: 1468.5981 Acc: 44.5588\n",
      "test Loss: 1504.9612 Acc: 44.0803\n",
      "\n",
      "Epoch 553/1199\n",
      "------------------------\n",
      "train Loss: 1504.3969 Acc: 43.8964\n",
      "test Loss: 1507.7169 Acc: 44.5531\n",
      "\n",
      "Epoch 554/1199\n",
      "------------------------\n",
      "train Loss: 1525.8393 Acc: 44.0269\n",
      "test Loss: 1461.0628 Acc: 44.7186\n",
      "\n",
      "Epoch 555/1199\n",
      "------------------------\n",
      "train Loss: 1518.9015 Acc: 44.7486\n",
      "test Loss: 1534.9859 Acc: 44.2521\n",
      "\n",
      "Epoch 556/1199\n",
      "------------------------\n",
      "train Loss: 1523.4450 Acc: 45.1440\n",
      "test Loss: 1515.9328 Acc: 43.9868\n",
      "\n",
      "Epoch 557/1199\n",
      "------------------------\n",
      "train Loss: 1525.9958 Acc: 45.5505\n",
      "test Loss: 1512.9407 Acc: 45.5326\n",
      "\n",
      "Epoch 558/1199\n",
      "------------------------\n",
      "train Loss: 1504.2078 Acc: 45.3451\n",
      "test Loss: 1476.7682 Acc: 44.5315\n",
      "\n",
      "Epoch 559/1199\n",
      "------------------------\n",
      "train Loss: 1523.0590 Acc: 44.0054\n",
      "test Loss: 1514.6628 Acc: 43.8820\n",
      "\n",
      "Epoch 560/1199\n",
      "------------------------\n",
      "train Loss: 1526.7011 Acc: 43.8715\n",
      "test Loss: 1541.6470 Acc: 44.4718\n",
      "\n",
      "Epoch 561/1199\n",
      "------------------------\n",
      "train Loss: 1517.6072 Acc: 43.8903\n",
      "test Loss: 1535.1459 Acc: 44.9479\n",
      "\n",
      "Epoch 562/1199\n",
      "------------------------\n",
      "train Loss: 1499.7563 Acc: 44.7250\n",
      "test Loss: 1505.4604 Acc: 45.9598\n",
      "\n",
      "Epoch 563/1199\n",
      "------------------------\n",
      "train Loss: 1484.3494 Acc: 44.7962\n",
      "test Loss: 1475.9979 Acc: 44.8466\n",
      "\n",
      "Epoch 564/1199\n",
      "------------------------\n",
      "train Loss: 1479.5307 Acc: 44.2850\n",
      "test Loss: 1470.1590 Acc: 44.2951\n",
      "\n",
      "Epoch 565/1199\n",
      "------------------------\n",
      "train Loss: 1490.9752 Acc: 43.8955\n",
      "test Loss: 1486.6604 Acc: 44.2737\n",
      "\n",
      "Epoch 566/1199\n",
      "------------------------\n",
      "train Loss: 1494.7588 Acc: 43.8636\n",
      "test Loss: 1499.5630 Acc: 44.4756\n",
      "\n",
      "Epoch 567/1199\n",
      "------------------------\n",
      "train Loss: 1489.2846 Acc: 44.4777\n",
      "test Loss: 1476.7471 Acc: 44.7854\n",
      "\n",
      "Epoch 568/1199\n",
      "------------------------\n",
      "train Loss: 1496.9278 Acc: 44.6676\n",
      "test Loss: 1535.7420 Acc: 44.4863\n",
      "\n",
      "Epoch 569/1199\n",
      "------------------------\n",
      "train Loss: 1530.7320 Acc: 44.9889\n",
      "test Loss: 1527.2506 Acc: 45.5704\n",
      "\n",
      "Epoch 570/1199\n",
      "------------------------\n",
      "train Loss: 1547.4297 Acc: 44.7121\n",
      "test Loss: 1581.0856 Acc: 45.3300\n",
      "\n",
      "Epoch 571/1199\n",
      "------------------------\n",
      "train Loss: 1559.1234 Acc: 44.9797\n",
      "test Loss: 1511.3222 Acc: 44.7442\n",
      "\n",
      "Epoch 572/1199\n",
      "------------------------\n",
      "train Loss: 1556.6225 Acc: 44.2141\n",
      "test Loss: 1587.6623 Acc: 43.2647\n",
      "\n",
      "Epoch 573/1199\n",
      "------------------------\n",
      "train Loss: 1586.0855 Acc: 43.6018\n",
      "test Loss: 1592.4257 Acc: 43.0143\n",
      "\n",
      "Epoch 574/1199\n",
      "------------------------\n",
      "train Loss: 1579.1797 Acc: 42.8718\n",
      "test Loss: 1605.7942 Acc: 42.7278\n",
      "\n",
      "Epoch 575/1199\n",
      "------------------------\n",
      "train Loss: 1569.7786 Acc: 43.5586\n",
      "test Loss: 1562.3331 Acc: 43.7098\n",
      "\n",
      "Epoch 576/1199\n",
      "------------------------\n",
      "train Loss: 1584.8685 Acc: 43.5991\n",
      "test Loss: 1613.3682 Acc: 44.4889\n",
      "\n",
      "Epoch 577/1199\n",
      "------------------------\n",
      "train Loss: 1581.8495 Acc: 44.1152\n",
      "test Loss: 1604.0363 Acc: 45.2821\n",
      "\n",
      "Epoch 578/1199\n",
      "------------------------\n",
      "train Loss: 1555.9958 Acc: 45.1656\n",
      "test Loss: 1596.0673 Acc: 45.1136\n",
      "\n",
      "Epoch 579/1199\n",
      "------------------------\n",
      "train Loss: 1584.6275 Acc: 44.9782\n",
      "test Loss: 1605.1317 Acc: 44.2932\n",
      "\n",
      "Epoch 580/1199\n",
      "------------------------\n",
      "train Loss: 1609.2672 Acc: 43.9669\n",
      "test Loss: 1620.1380 Acc: 43.8480\n",
      "\n",
      "Epoch 581/1199\n",
      "------------------------\n",
      "train Loss: 1585.0464 Acc: 44.2715\n",
      "test Loss: 1544.8987 Acc: 43.3922\n",
      "\n",
      "Epoch 582/1199\n",
      "------------------------\n",
      "train Loss: 1546.0182 Acc: 44.7606\n",
      "test Loss: 1546.5594 Acc: 45.1630\n",
      "\n",
      "Epoch 583/1199\n",
      "------------------------\n",
      "train Loss: 1562.6218 Acc: 45.2283\n",
      "test Loss: 1638.0414 Acc: 44.6618\n",
      "\n",
      "Epoch 584/1199\n",
      "------------------------\n",
      "train Loss: 1594.9374 Acc: 44.6837\n",
      "test Loss: 1597.7585 Acc: 43.9996\n",
      "\n",
      "Epoch 585/1199\n",
      "------------------------\n",
      "train Loss: 1588.8774 Acc: 44.3857\n",
      "test Loss: 1601.4944 Acc: 44.0461\n",
      "\n",
      "Epoch 586/1199\n",
      "------------------------\n",
      "train Loss: 1589.5819 Acc: 43.7362\n",
      "test Loss: 1591.6455 Acc: 44.2091\n",
      "\n",
      "Epoch 587/1199\n",
      "------------------------\n",
      "train Loss: 1585.1978 Acc: 43.9199\n",
      "test Loss: 1621.3119 Acc: 43.6880\n",
      "\n",
      "Epoch 588/1199\n",
      "------------------------\n",
      "train Loss: 1585.2007 Acc: 44.0056\n",
      "test Loss: 1589.9861 Acc: 44.3003\n",
      "\n",
      "Epoch 589/1199\n",
      "------------------------\n",
      "train Loss: 1572.4367 Acc: 44.3631\n",
      "test Loss: 1579.4636 Acc: 44.6721\n",
      "\n",
      "Epoch 590/1199\n",
      "------------------------\n",
      "train Loss: 1571.2526 Acc: 44.4768\n",
      "test Loss: 1573.7984 Acc: 45.4341\n",
      "\n",
      "Epoch 591/1199\n",
      "------------------------\n",
      "train Loss: 1583.1644 Acc: 45.0612\n",
      "test Loss: 1596.0524 Acc: 45.3505\n",
      "\n",
      "Epoch 592/1199\n",
      "------------------------\n",
      "train Loss: 1562.6753 Acc: 44.9163\n",
      "test Loss: 1609.2006 Acc: 43.9285\n",
      "\n",
      "Epoch 593/1199\n",
      "------------------------\n",
      "train Loss: 1596.8073 Acc: 44.4787\n",
      "test Loss: 1606.5798 Acc: 44.2139\n",
      "\n",
      "Epoch 594/1199\n",
      "------------------------\n",
      "train Loss: 1594.9745 Acc: 44.1759\n",
      "test Loss: 1635.7350 Acc: 43.2072\n",
      "\n",
      "Epoch 595/1199\n",
      "------------------------\n",
      "train Loss: 1637.7446 Acc: 44.0573\n",
      "test Loss: 1643.8359 Acc: 44.2324\n",
      "\n",
      "Epoch 596/1199\n",
      "------------------------\n",
      "train Loss: 1632.1208 Acc: 43.9614\n",
      "test Loss: 1604.2365 Acc: 43.9300\n",
      "\n",
      "Epoch 597/1199\n",
      "------------------------\n",
      "train Loss: 1633.2473 Acc: 44.0544\n",
      "test Loss: 1632.5365 Acc: 44.2272\n",
      "\n",
      "Epoch 598/1199\n",
      "------------------------\n",
      "train Loss: 1642.6986 Acc: 43.2453\n",
      "test Loss: 1603.4822 Acc: 42.8853\n",
      "\n",
      "Epoch 599/1199\n",
      "------------------------\n",
      "train Loss: 1627.6539 Acc: 43.8028\n",
      "test Loss: 1629.6497 Acc: 43.5272\n",
      "\n",
      "Epoch 600/1199\n",
      "------------------------\n",
      "train Loss: 1603.3355 Acc: 44.0802\n",
      "test Loss: 1601.0908 Acc: 43.2924\n",
      "\n",
      "Epoch 601/1199\n",
      "------------------------\n",
      "train Loss: 1589.8388 Acc: 44.1226\n",
      "test Loss: 1567.4313 Acc: 44.4452\n",
      "\n",
      "Epoch 602/1199\n",
      "------------------------\n",
      "train Loss: 1539.3144 Acc: 44.0114\n",
      "test Loss: 1555.8975 Acc: 44.2212\n",
      "\n",
      "Epoch 603/1199\n",
      "------------------------\n",
      "train Loss: 1547.2499 Acc: 44.3293\n",
      "test Loss: 1513.6876 Acc: 43.7532\n",
      "\n",
      "Epoch 604/1199\n",
      "------------------------\n",
      "train Loss: 1491.8053 Acc: 44.1506\n",
      "test Loss: 1508.4195 Acc: 43.8046\n",
      "\n",
      "Epoch 605/1199\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1484.5264 Acc: 43.8196\n",
      "test Loss: 1430.1728 Acc: 44.6500\n",
      "\n",
      "Epoch 606/1199\n",
      "------------------------\n",
      "train Loss: 1459.8155 Acc: 43.9238\n",
      "test Loss: 1461.8796 Acc: 44.4010\n",
      "\n",
      "Epoch 607/1199\n",
      "------------------------\n",
      "train Loss: 1444.7833 Acc: 44.0074\n",
      "test Loss: 1430.3756 Acc: 44.1234\n",
      "\n",
      "Epoch 608/1199\n",
      "------------------------\n",
      "train Loss: 1454.8023 Acc: 43.5906\n",
      "test Loss: 1440.9060 Acc: 43.8095\n",
      "\n",
      "Epoch 609/1199\n",
      "------------------------\n",
      "train Loss: 1415.1960 Acc: 43.8025\n",
      "test Loss: 1420.3900 Acc: 44.8395\n",
      "\n",
      "Epoch 610/1199\n",
      "------------------------\n",
      "train Loss: 1386.1615 Acc: 45.1904\n",
      "test Loss: 1393.0150 Acc: 45.4794\n",
      "\n",
      "Epoch 611/1199\n",
      "------------------------\n",
      "train Loss: 1387.6556 Acc: 45.3667\n",
      "test Loss: 1386.0307 Acc: 45.0804\n",
      "\n",
      "Epoch 612/1199\n",
      "------------------------\n",
      "train Loss: 1375.9515 Acc: 45.4071\n",
      "test Loss: 1402.7498 Acc: 45.5664\n",
      "\n",
      "Epoch 613/1199\n",
      "------------------------\n",
      "train Loss: 1393.6041 Acc: 45.0448\n",
      "test Loss: 1386.8753 Acc: 45.1585\n",
      "\n",
      "Epoch 614/1199\n",
      "------------------------\n",
      "train Loss: 1388.0191 Acc: 45.5326\n",
      "test Loss: 1424.7265 Acc: 46.1072\n",
      "\n",
      "Epoch 615/1199\n",
      "------------------------\n",
      "train Loss: 1369.7008 Acc: 45.7080\n",
      "test Loss: 1335.5721 Acc: 45.9547\n",
      "\n",
      "Epoch 616/1199\n",
      "------------------------\n",
      "train Loss: 1365.4919 Acc: 45.0303\n",
      "test Loss: 1366.7484 Acc: 44.8030\n",
      "\n",
      "Epoch 617/1199\n",
      "------------------------\n",
      "train Loss: 1339.1924 Acc: 45.1768\n",
      "test Loss: 1372.8647 Acc: 44.1376\n",
      "\n",
      "Epoch 618/1199\n",
      "------------------------\n",
      "train Loss: 1334.3012 Acc: 44.5135\n",
      "test Loss: 1318.0277 Acc: 44.7324\n",
      "\n",
      "Epoch 619/1199\n",
      "------------------------\n",
      "train Loss: 1311.0804 Acc: 44.6726\n",
      "test Loss: 1312.9029 Acc: 44.9231\n",
      "\n",
      "Epoch 620/1199\n",
      "------------------------\n",
      "train Loss: 1308.6513 Acc: 44.8210\n",
      "test Loss: 1329.3706 Acc: 44.9652\n",
      "\n",
      "Epoch 621/1199\n",
      "------------------------\n",
      "train Loss: 1314.9543 Acc: 44.5660\n",
      "test Loss: 1298.7477 Acc: 44.9213\n",
      "\n",
      "Epoch 622/1199\n",
      "------------------------\n",
      "train Loss: 1302.0094 Acc: 44.7964\n",
      "test Loss: 1320.6544 Acc: 44.4524\n",
      "\n",
      "Epoch 623/1199\n",
      "------------------------\n",
      "train Loss: 1316.6668 Acc: 44.4191\n",
      "test Loss: 1354.2590 Acc: 43.7829\n",
      "\n",
      "Epoch 624/1199\n",
      "------------------------\n",
      "train Loss: 1336.5427 Acc: 44.0672\n",
      "test Loss: 1328.3457 Acc: 45.2700\n",
      "\n",
      "Epoch 625/1199\n",
      "------------------------\n",
      "train Loss: 1315.0232 Acc: 43.7597\n",
      "test Loss: 1331.7024 Acc: 44.3288\n",
      "\n",
      "Epoch 626/1199\n",
      "------------------------\n",
      "train Loss: 1314.6277 Acc: 44.2179\n",
      "test Loss: 1293.8413 Acc: 44.4283\n",
      "\n",
      "Epoch 627/1199\n",
      "------------------------\n",
      "train Loss: 1312.1529 Acc: 44.4346\n",
      "test Loss: 1323.4743 Acc: 45.5010\n",
      "\n",
      "Epoch 628/1199\n",
      "------------------------\n",
      "train Loss: 1312.4408 Acc: 45.4871\n",
      "test Loss: 1320.7849 Acc: 45.2053\n",
      "\n",
      "Epoch 629/1199\n",
      "------------------------\n",
      "train Loss: 1333.6578 Acc: 45.0163\n",
      "test Loss: 1316.0150 Acc: 45.6253\n",
      "\n",
      "Epoch 630/1199\n",
      "------------------------\n",
      "train Loss: 1306.1272 Acc: 45.1806\n",
      "test Loss: 1296.1230 Acc: 45.3602\n",
      "\n",
      "Epoch 631/1199\n",
      "------------------------\n",
      "train Loss: 1331.1545 Acc: 45.1377\n",
      "test Loss: 1308.6011 Acc: 45.7745\n",
      "\n",
      "Epoch 632/1199\n",
      "------------------------\n",
      "train Loss: 1311.3223 Acc: 44.8689\n",
      "test Loss: 1342.1655 Acc: 45.7816\n",
      "\n",
      "Epoch 633/1199\n",
      "------------------------\n",
      "train Loss: 1353.3154 Acc: 44.6999\n",
      "test Loss: 1355.1774 Acc: 44.3823\n",
      "\n",
      "Epoch 634/1199\n",
      "------------------------\n",
      "train Loss: 1368.3956 Acc: 43.8548\n",
      "test Loss: 1360.3237 Acc: 44.4977\n",
      "\n",
      "Epoch 635/1199\n",
      "------------------------\n",
      "train Loss: 1336.9116 Acc: 44.6568\n",
      "test Loss: 1323.0017 Acc: 44.5022\n",
      "\n",
      "Epoch 636/1199\n",
      "------------------------\n",
      "train Loss: 1313.0331 Acc: 44.6872\n",
      "test Loss: 1311.1333 Acc: 45.2868\n",
      "\n",
      "Epoch 637/1199\n",
      "------------------------\n",
      "train Loss: 1301.6804 Acc: 44.9582\n",
      "test Loss: 1270.3045 Acc: 45.5892\n",
      "\n",
      "Epoch 638/1199\n",
      "------------------------\n",
      "train Loss: 1282.1315 Acc: 45.2808\n",
      "test Loss: 1294.7968 Acc: 45.1878\n",
      "\n",
      "Epoch 639/1199\n",
      "------------------------\n",
      "train Loss: 1287.8791 Acc: 44.9001\n",
      "test Loss: 1310.9166 Acc: 45.3301\n",
      "\n",
      "Epoch 640/1199\n",
      "------------------------\n",
      "train Loss: 1301.6800 Acc: 44.4897\n",
      "test Loss: 1266.9844 Acc: 44.9182\n",
      "\n",
      "Epoch 641/1199\n",
      "------------------------\n",
      "train Loss: 1282.0190 Acc: 44.4589\n",
      "test Loss: 1295.4506 Acc: 42.9570\n",
      "\n",
      "Epoch 642/1199\n",
      "------------------------\n",
      "train Loss: 1295.0996 Acc: 44.4170\n",
      "test Loss: 1306.1862 Acc: 44.1753\n",
      "\n",
      "Epoch 643/1199\n",
      "------------------------\n",
      "train Loss: 1304.2650 Acc: 43.6472\n",
      "test Loss: 1311.4520 Acc: 44.4429\n",
      "\n",
      "Epoch 644/1199\n",
      "------------------------\n",
      "train Loss: 1318.6024 Acc: 44.3387\n",
      "test Loss: 1317.2457 Acc: 44.3877\n",
      "\n",
      "Epoch 645/1199\n",
      "------------------------\n",
      "train Loss: 1323.3982 Acc: 44.1736\n",
      "test Loss: 1314.5162 Acc: 44.0768\n",
      "\n",
      "Epoch 646/1199\n",
      "------------------------\n",
      "train Loss: 1329.9218 Acc: 44.1565\n",
      "test Loss: 1317.4752 Acc: 43.9215\n",
      "\n",
      "Epoch 647/1199\n",
      "------------------------\n",
      "train Loss: 1340.3649 Acc: 44.1866\n",
      "test Loss: 1341.9010 Acc: 44.8438\n",
      "\n",
      "Epoch 648/1199\n",
      "------------------------\n",
      "train Loss: 1329.5201 Acc: 44.9750\n",
      "test Loss: 1311.0163 Acc: 44.9666\n",
      "\n",
      "Epoch 649/1199\n",
      "------------------------\n",
      "train Loss: 1308.4187 Acc: 45.2672\n",
      "test Loss: 1310.9359 Acc: 45.2455\n",
      "\n",
      "Epoch 650/1199\n",
      "------------------------\n",
      "train Loss: 1317.1409 Acc: 45.1501\n",
      "test Loss: 1319.2803 Acc: 45.6906\n",
      "\n",
      "Epoch 651/1199\n",
      "------------------------\n",
      "train Loss: 1309.0082 Acc: 45.9091\n",
      "test Loss: 1319.4637 Acc: 46.0301\n",
      "\n",
      "Epoch 652/1199\n",
      "------------------------\n",
      "train Loss: 1338.1773 Acc: 45.5783\n",
      "test Loss: 1320.3918 Acc: 44.9557\n",
      "\n",
      "Epoch 653/1199\n",
      "------------------------\n",
      "train Loss: 1323.8820 Acc: 45.8699\n",
      "test Loss: 1340.5671 Acc: 45.5826\n",
      "\n",
      "Epoch 654/1199\n",
      "------------------------\n",
      "train Loss: 1312.8946 Acc: 45.0688\n",
      "test Loss: 1333.3109 Acc: 45.7493\n",
      "\n",
      "Epoch 655/1199\n",
      "------------------------\n",
      "train Loss: 1320.8836 Acc: 45.2645\n",
      "test Loss: 1312.4338 Acc: 45.5586\n",
      "\n",
      "Epoch 656/1199\n",
      "------------------------\n",
      "train Loss: 1333.5361 Acc: 44.8128\n",
      "test Loss: 1346.7465 Acc: 44.0260\n",
      "\n",
      "Epoch 657/1199\n",
      "------------------------\n",
      "train Loss: 1351.1550 Acc: 44.0729\n",
      "test Loss: 1352.6475 Acc: 44.3250\n",
      "\n",
      "Epoch 658/1199\n",
      "------------------------\n",
      "train Loss: 1337.7334 Acc: 44.1145\n",
      "test Loss: 1340.9880 Acc: 44.1844\n",
      "\n",
      "Epoch 659/1199\n",
      "------------------------\n",
      "train Loss: 1344.1218 Acc: 43.8071\n",
      "test Loss: 1304.1184 Acc: 43.6570\n",
      "\n",
      "Epoch 660/1199\n",
      "------------------------\n",
      "train Loss: 1335.4879 Acc: 43.9215\n",
      "test Loss: 1362.6624 Acc: 44.0711\n",
      "\n",
      "Epoch 661/1199\n",
      "------------------------\n",
      "train Loss: 1341.4463 Acc: 44.3637\n",
      "test Loss: 1349.4095 Acc: 44.7830\n",
      "\n",
      "Epoch 662/1199\n",
      "------------------------\n",
      "train Loss: 1376.0536 Acc: 44.2149\n",
      "test Loss: 1385.2507 Acc: 45.2844\n",
      "\n",
      "Epoch 663/1199\n",
      "------------------------\n",
      "train Loss: 1383.4738 Acc: 44.8711\n",
      "test Loss: 1384.4080 Acc: 44.3770\n",
      "\n",
      "Epoch 664/1199\n",
      "------------------------\n",
      "train Loss: 1384.3262 Acc: 44.8824\n",
      "test Loss: 1369.8093 Acc: 44.7310\n",
      "\n",
      "Epoch 665/1199\n",
      "------------------------\n",
      "train Loss: 1357.1780 Acc: 45.3425\n",
      "test Loss: 1341.0734 Acc: 45.6412\n",
      "\n",
      "Epoch 666/1199\n",
      "------------------------\n",
      "train Loss: 1343.8222 Acc: 45.4174\n",
      "test Loss: 1348.2635 Acc: 45.0403\n",
      "\n",
      "Epoch 667/1199\n",
      "------------------------\n",
      "train Loss: 1331.8198 Acc: 44.9517\n",
      "test Loss: 1342.4196 Acc: 44.1666\n",
      "\n",
      "Epoch 668/1199\n",
      "------------------------\n",
      "train Loss: 1330.5213 Acc: 44.6760\n",
      "test Loss: 1347.9467 Acc: 44.5739\n",
      "\n",
      "Epoch 669/1199\n",
      "------------------------\n",
      "train Loss: 1365.6796 Acc: 43.6652\n",
      "test Loss: 1377.9751 Acc: 43.2471\n",
      "\n",
      "Epoch 670/1199\n",
      "------------------------\n",
      "train Loss: 1352.4148 Acc: 43.9190\n",
      "test Loss: 1405.8257 Acc: 43.2193\n",
      "\n",
      "Epoch 671/1199\n",
      "------------------------\n",
      "train Loss: 1343.3541 Acc: 43.3967\n",
      "test Loss: 1344.2854 Acc: 44.2421\n",
      "\n",
      "Epoch 672/1199\n",
      "------------------------\n",
      "train Loss: 1357.7429 Acc: 43.9902\n",
      "test Loss: 1353.1782 Acc: 43.8089\n",
      "\n",
      "Epoch 673/1199\n",
      "------------------------\n",
      "train Loss: 1348.3552 Acc: 45.0808\n",
      "test Loss: 1326.6918 Acc: 45.3927\n",
      "\n",
      "Epoch 674/1199\n",
      "------------------------\n",
      "train Loss: 1355.8327 Acc: 44.6080\n",
      "test Loss: 1377.1905 Acc: 44.6428\n",
      "\n",
      "Epoch 675/1199\n",
      "------------------------\n",
      "train Loss: 1356.6532 Acc: 45.1270\n",
      "test Loss: 1392.0139 Acc: 45.7300\n",
      "\n",
      "Epoch 676/1199\n",
      "------------------------\n",
      "train Loss: 1334.3822 Acc: 45.7453\n",
      "test Loss: 1362.4815 Acc: 45.5599\n",
      "\n",
      "Epoch 677/1199\n",
      "------------------------\n",
      "train Loss: 1365.2949 Acc: 45.3681\n",
      "test Loss: 1331.9424 Acc: 46.6018\n",
      "\n",
      "Epoch 678/1199\n",
      "------------------------\n",
      "train Loss: 1377.7163 Acc: 44.8572\n",
      "test Loss: 1365.5267 Acc: 44.0286\n",
      "\n",
      "Epoch 679/1199\n",
      "------------------------\n",
      "train Loss: 1337.1443 Acc: 44.8131\n",
      "test Loss: 1340.5301 Acc: 44.3941\n",
      "\n",
      "Epoch 680/1199\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1325.9791 Acc: 44.9841\n",
      "test Loss: 1340.4750 Acc: 44.7165\n",
      "\n",
      "Epoch 681/1199\n",
      "------------------------\n",
      "train Loss: 1344.1408 Acc: 44.4386\n",
      "test Loss: 1336.9329 Acc: 44.2060\n",
      "\n",
      "Epoch 682/1199\n",
      "------------------------\n",
      "train Loss: 1350.3654 Acc: 44.0875\n",
      "test Loss: 1341.5039 Acc: 45.1289\n",
      "\n",
      "Epoch 683/1199\n",
      "------------------------\n",
      "train Loss: 1353.6168 Acc: 45.1663\n",
      "test Loss: 1377.2065 Acc: 44.6920\n",
      "\n",
      "Epoch 684/1199\n",
      "------------------------\n",
      "train Loss: 1363.0173 Acc: 44.6682\n",
      "test Loss: 1364.6916 Acc: 44.0504\n",
      "\n",
      "Epoch 685/1199\n",
      "------------------------\n",
      "train Loss: 1363.8151 Acc: 45.7687\n",
      "test Loss: 1349.3418 Acc: 44.4117\n",
      "\n",
      "Epoch 686/1199\n",
      "------------------------\n",
      "train Loss: 1351.9513 Acc: 44.9276\n",
      "test Loss: 1345.5342 Acc: 45.5908\n",
      "\n",
      "Epoch 687/1199\n",
      "------------------------\n",
      "train Loss: 1357.3366 Acc: 45.1403\n",
      "test Loss: 1348.2105 Acc: 44.6640\n",
      "\n",
      "Epoch 688/1199\n",
      "------------------------\n",
      "train Loss: 1351.9679 Acc: 44.9564\n",
      "test Loss: 1338.8197 Acc: 44.4117\n",
      "\n",
      "Epoch 689/1199\n",
      "------------------------\n",
      "train Loss: 1364.1909 Acc: 43.7680\n",
      "test Loss: 1341.3105 Acc: 43.8015\n",
      "\n",
      "Epoch 690/1199\n",
      "------------------------\n",
      "train Loss: 1367.2649 Acc: 43.2927\n",
      "test Loss: 1352.9700 Acc: 42.9796\n",
      "\n",
      "Epoch 691/1199\n",
      "------------------------\n",
      "train Loss: 1359.2418 Acc: 43.9421\n",
      "test Loss: 1371.9720 Acc: 43.0490\n",
      "\n",
      "Epoch 692/1199\n",
      "------------------------\n",
      "train Loss: 1357.8670 Acc: 44.4638\n",
      "test Loss: 1336.7663 Acc: 44.3678\n",
      "\n",
      "Epoch 693/1199\n",
      "------------------------\n",
      "train Loss: 1354.1454 Acc: 44.5455\n",
      "test Loss: 1349.6001 Acc: 44.7842\n",
      "\n",
      "Epoch 694/1199\n",
      "------------------------\n",
      "train Loss: 1364.7953 Acc: 44.3912\n",
      "test Loss: 1356.8261 Acc: 44.3246\n",
      "\n",
      "Epoch 695/1199\n",
      "------------------------\n",
      "train Loss: 1362.6660 Acc: 44.7627\n",
      "test Loss: 1367.4611 Acc: 44.7492\n",
      "\n",
      "Epoch 696/1199\n",
      "------------------------\n",
      "train Loss: 1358.6316 Acc: 44.5893\n",
      "test Loss: 1365.3789 Acc: 44.7283\n",
      "\n",
      "Epoch 697/1199\n",
      "------------------------\n",
      "train Loss: 1367.0738 Acc: 45.1519\n",
      "test Loss: 1358.1779 Acc: 45.3231\n",
      "\n",
      "Epoch 698/1199\n",
      "------------------------\n",
      "train Loss: 1345.8712 Acc: 45.4378\n",
      "test Loss: 1366.9950 Acc: 44.9450\n",
      "\n",
      "Epoch 699/1199\n",
      "------------------------\n",
      "train Loss: 1346.6809 Acc: 45.4488\n",
      "test Loss: 1340.3843 Acc: 45.6037\n",
      "\n",
      "Epoch 700/1199\n",
      "------------------------\n",
      "train Loss: 1330.2101 Acc: 45.6694\n",
      "test Loss: 1348.3086 Acc: 45.0649\n",
      "\n",
      "Epoch 701/1199\n",
      "------------------------\n",
      "train Loss: 1337.6340 Acc: 44.9166\n",
      "test Loss: 1355.6303 Acc: 45.0633\n",
      "\n",
      "Epoch 702/1199\n",
      "------------------------\n",
      "train Loss: 1342.8817 Acc: 44.7057\n",
      "test Loss: 1352.2628 Acc: 45.4657\n",
      "\n",
      "Epoch 703/1199\n",
      "------------------------\n",
      "train Loss: 1344.5258 Acc: 44.3448\n",
      "test Loss: 1335.7390 Acc: 44.7954\n",
      "\n",
      "Epoch 704/1199\n",
      "------------------------\n",
      "train Loss: 1354.5685 Acc: 44.4611\n",
      "test Loss: 1341.0660 Acc: 44.0402\n",
      "\n",
      "Epoch 705/1199\n",
      "------------------------\n",
      "train Loss: 1344.5147 Acc: 44.5209\n",
      "test Loss: 1341.5681 Acc: 45.1193\n",
      "\n",
      "Epoch 706/1199\n",
      "------------------------\n",
      "train Loss: 1344.2023 Acc: 44.8768\n",
      "test Loss: 1333.3327 Acc: 45.4054\n",
      "\n",
      "Epoch 707/1199\n",
      "------------------------\n",
      "train Loss: 1356.5136 Acc: 44.8024\n",
      "test Loss: 1370.6323 Acc: 44.2376\n",
      "\n",
      "Epoch 708/1199\n",
      "------------------------\n",
      "train Loss: 1347.9816 Acc: 44.0180\n",
      "test Loss: 1365.4466 Acc: 43.1081\n",
      "\n",
      "Epoch 709/1199\n",
      "------------------------\n",
      "train Loss: 1353.1173 Acc: 43.5210\n",
      "test Loss: 1315.2140 Acc: 43.8816\n",
      "\n",
      "Epoch 710/1199\n",
      "------------------------\n",
      "train Loss: 1343.1355 Acc: 43.4293\n",
      "test Loss: 1359.3607 Acc: 43.0644\n",
      "\n",
      "Epoch 711/1199\n",
      "------------------------\n",
      "train Loss: 1343.4919 Acc: 43.0579\n",
      "test Loss: 1349.8530 Acc: 43.2658\n",
      "\n",
      "Epoch 712/1199\n",
      "------------------------\n",
      "train Loss: 1362.8494 Acc: 42.7902\n",
      "test Loss: 1324.7347 Acc: 43.7269\n",
      "\n",
      "Epoch 713/1199\n",
      "------------------------\n",
      "train Loss: 1333.9431 Acc: 44.1341\n",
      "test Loss: 1367.0984 Acc: 44.3980\n",
      "\n",
      "Epoch 714/1199\n",
      "------------------------\n",
      "train Loss: 1324.4573 Acc: 45.2083\n",
      "test Loss: 1340.1527 Acc: 45.6909\n",
      "\n",
      "Epoch 715/1199\n",
      "------------------------\n",
      "train Loss: 1381.6970 Acc: 45.4206\n",
      "test Loss: 1391.6171 Acc: 45.8569\n",
      "\n",
      "Epoch 716/1199\n",
      "------------------------\n",
      "train Loss: 1355.7702 Acc: 45.2919\n",
      "test Loss: 1372.3353 Acc: 45.6255\n",
      "\n",
      "Epoch 717/1199\n",
      "------------------------\n",
      "train Loss: 1366.4172 Acc: 45.6814\n",
      "test Loss: 1374.8643 Acc: 45.6267\n",
      "\n",
      "Epoch 718/1199\n",
      "------------------------\n",
      "train Loss: 1360.8782 Acc: 45.3183\n",
      "test Loss: 1349.4707 Acc: 45.2471\n",
      "\n",
      "Epoch 719/1199\n",
      "------------------------\n",
      "train Loss: 1354.7413 Acc: 44.2869\n",
      "test Loss: 1354.7463 Acc: 44.4281\n",
      "\n",
      "Epoch 720/1199\n",
      "------------------------\n",
      "train Loss: 1332.1216 Acc: 44.0553\n",
      "test Loss: 1359.0796 Acc: 42.9906\n",
      "\n",
      "Epoch 721/1199\n",
      "------------------------\n",
      "train Loss: 1365.8807 Acc: 42.4903\n",
      "test Loss: 1360.9445 Acc: 42.2335\n",
      "\n",
      "Epoch 722/1199\n",
      "------------------------\n",
      "train Loss: 1380.5719 Acc: 42.0797\n",
      "test Loss: 1334.8574 Acc: 42.5065\n",
      "\n",
      "Epoch 723/1199\n",
      "------------------------\n",
      "train Loss: 1360.7911 Acc: 43.3823\n",
      "test Loss: 1353.4924 Acc: 43.8941\n",
      "\n",
      "Epoch 724/1199\n",
      "------------------------\n",
      "train Loss: 1375.1171 Acc: 43.8082\n",
      "test Loss: 1362.8033 Acc: 45.1709\n",
      "\n",
      "Epoch 725/1199\n",
      "------------------------\n",
      "train Loss: 1383.2629 Acc: 44.9685\n",
      "test Loss: 1394.1077 Acc: 45.5403\n",
      "\n",
      "Epoch 726/1199\n",
      "------------------------\n",
      "train Loss: 1385.8555 Acc: 45.7710\n",
      "test Loss: 1358.3874 Acc: 45.4638\n",
      "\n",
      "Epoch 727/1199\n",
      "------------------------\n",
      "train Loss: 1355.4557 Acc: 45.4430\n",
      "test Loss: 1389.2983 Acc: 44.8853\n",
      "\n",
      "Epoch 728/1199\n",
      "------------------------\n",
      "train Loss: 1372.9798 Acc: 44.7976\n",
      "test Loss: 1398.8476 Acc: 44.6492\n",
      "\n",
      "Epoch 729/1199\n",
      "------------------------\n",
      "train Loss: 1383.7228 Acc: 44.3233\n",
      "test Loss: 1394.3563 Acc: 43.7079\n",
      "\n",
      "Epoch 730/1199\n",
      "------------------------\n",
      "train Loss: 1404.2785 Acc: 44.0139\n",
      "test Loss: 1397.8661 Acc: 43.5540\n",
      "\n",
      "Epoch 731/1199\n",
      "------------------------\n",
      "train Loss: 1397.0916 Acc: 43.9395\n",
      "test Loss: 1392.4559 Acc: 44.1471\n",
      "\n",
      "Epoch 732/1199\n",
      "------------------------\n",
      "train Loss: 1373.3811 Acc: 44.9413\n",
      "test Loss: 1398.9125 Acc: 45.2094\n",
      "\n",
      "Epoch 733/1199\n",
      "------------------------\n",
      "train Loss: 1397.5546 Acc: 44.6324\n",
      "test Loss: 1414.4307 Acc: 44.8369\n",
      "\n",
      "Epoch 734/1199\n",
      "------------------------\n",
      "train Loss: 1380.6004 Acc: 45.2846\n",
      "test Loss: 1391.9030 Acc: 45.3410\n",
      "\n",
      "Epoch 735/1199\n",
      "------------------------\n",
      "train Loss: 1392.5619 Acc: 44.9144\n",
      "test Loss: 1428.2118 Acc: 44.3953\n",
      "\n",
      "Epoch 736/1199\n",
      "------------------------\n",
      "train Loss: 1392.9835 Acc: 44.6750\n",
      "test Loss: 1430.9530 Acc: 43.6823\n",
      "\n",
      "Epoch 737/1199\n",
      "------------------------\n",
      "train Loss: 1410.8635 Acc: 44.4596\n",
      "test Loss: 1404.4658 Acc: 44.5675\n",
      "\n",
      "Epoch 738/1199\n",
      "------------------------\n",
      "train Loss: 1412.8539 Acc: 44.5097\n",
      "test Loss: 1422.2870 Acc: 43.5369\n",
      "\n",
      "Epoch 739/1199\n",
      "------------------------\n",
      "train Loss: 1392.5817 Acc: 44.1934\n",
      "test Loss: 1396.2676 Acc: 43.6405\n",
      "\n",
      "Epoch 740/1199\n",
      "------------------------\n",
      "train Loss: 1379.8683 Acc: 44.2220\n",
      "test Loss: 1389.7181 Acc: 44.3457\n",
      "\n",
      "Epoch 741/1199\n",
      "------------------------\n",
      "train Loss: 1384.0273 Acc: 44.0606\n",
      "test Loss: 1401.5762 Acc: 44.1122\n",
      "\n",
      "Epoch 742/1199\n",
      "------------------------\n",
      "train Loss: 1394.2693 Acc: 44.2448\n",
      "test Loss: 1423.2749 Acc: 43.3613\n",
      "\n",
      "Epoch 743/1199\n",
      "------------------------\n",
      "train Loss: 1393.8624 Acc: 44.0951\n",
      "test Loss: 1398.8383 Acc: 43.5844\n",
      "\n",
      "Epoch 744/1199\n",
      "------------------------\n",
      "train Loss: 1398.3699 Acc: 44.4915\n",
      "test Loss: 1453.7543 Acc: 43.4200\n",
      "\n",
      "Epoch 745/1199\n",
      "------------------------\n",
      "train Loss: 1412.5700 Acc: 44.1895\n",
      "test Loss: 1434.0454 Acc: 45.0676\n",
      "\n",
      "Epoch 746/1199\n",
      "------------------------\n",
      "train Loss: 1416.5607 Acc: 44.8351\n",
      "test Loss: 1413.5809 Acc: 44.4891\n",
      "\n",
      "Epoch 747/1199\n",
      "------------------------\n",
      "train Loss: 1418.9138 Acc: 44.4407\n",
      "test Loss: 1424.3154 Acc: 43.2566\n",
      "\n",
      "Epoch 748/1199\n",
      "------------------------\n",
      "train Loss: 1411.3217 Acc: 44.4274\n",
      "test Loss: 1424.3843 Acc: 44.2958\n",
      "\n",
      "Epoch 749/1199\n",
      "------------------------\n",
      "train Loss: 1407.6797 Acc: 44.3318\n",
      "test Loss: 1440.7513 Acc: 44.1644\n",
      "\n",
      "Epoch 750/1199\n",
      "------------------------\n",
      "train Loss: 1416.7120 Acc: 44.2928\n",
      "test Loss: 1390.1584 Acc: 45.1776\n",
      "\n",
      "Epoch 751/1199\n",
      "------------------------\n",
      "train Loss: 1407.8125 Acc: 44.6201\n",
      "test Loss: 1392.6907 Acc: 44.3571\n",
      "\n",
      "Epoch 752/1199\n",
      "------------------------\n",
      "train Loss: 1401.8347 Acc: 44.5170\n",
      "test Loss: 1393.5925 Acc: 45.4989\n",
      "\n",
      "Epoch 753/1199\n",
      "------------------------\n",
      "train Loss: 1411.9679 Acc: 45.1416\n",
      "test Loss: 1426.3086 Acc: 44.0768\n",
      "\n",
      "Epoch 754/1199\n",
      "------------------------\n",
      "train Loss: 1429.9440 Acc: 45.9400\n",
      "test Loss: 1420.7468 Acc: 45.0732\n",
      "\n",
      "Epoch 755/1199\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1427.4946 Acc: 45.5900\n",
      "test Loss: 1436.1672 Acc: 44.8929\n",
      "\n",
      "Epoch 756/1199\n",
      "------------------------\n",
      "train Loss: 1413.5468 Acc: 45.0235\n",
      "test Loss: 1472.0229 Acc: 44.7925\n",
      "\n",
      "Epoch 757/1199\n",
      "------------------------\n",
      "train Loss: 1410.5907 Acc: 45.0300\n",
      "test Loss: 1440.4411 Acc: 44.6366\n",
      "\n",
      "Epoch 758/1199\n",
      "------------------------\n",
      "train Loss: 1411.5992 Acc: 44.3910\n",
      "test Loss: 1456.0519 Acc: 44.2139\n",
      "\n",
      "Epoch 759/1199\n",
      "------------------------\n",
      "train Loss: 1436.3013 Acc: 44.3946\n",
      "test Loss: 1427.7037 Acc: 43.9889\n",
      "\n",
      "Epoch 760/1199\n",
      "------------------------\n",
      "train Loss: 1425.4328 Acc: 44.0702\n",
      "test Loss: 1436.7164 Acc: 44.2504\n",
      "\n",
      "Epoch 761/1199\n",
      "------------------------\n",
      "train Loss: 1419.6808 Acc: 44.2145\n",
      "test Loss: 1447.0962 Acc: 44.3155\n",
      "\n",
      "Epoch 762/1199\n",
      "------------------------\n",
      "train Loss: 1416.9235 Acc: 44.3557\n",
      "test Loss: 1435.4688 Acc: 44.4614\n",
      "\n",
      "Epoch 763/1199\n",
      "------------------------\n",
      "train Loss: 1412.6164 Acc: 45.1787\n",
      "test Loss: 1426.1899 Acc: 45.1321\n",
      "\n",
      "Epoch 764/1199\n",
      "------------------------\n",
      "train Loss: 1409.4285 Acc: 45.2249\n",
      "test Loss: 1420.0890 Acc: 44.7984\n",
      "\n",
      "Epoch 765/1199\n",
      "------------------------\n",
      "train Loss: 1405.5945 Acc: 44.7136\n",
      "test Loss: 1424.3915 Acc: 44.4771\n",
      "\n",
      "Epoch 766/1199\n",
      "------------------------\n",
      "train Loss: 1425.6716 Acc: 43.9795\n",
      "test Loss: 1449.5557 Acc: 44.5758\n",
      "\n",
      "Epoch 767/1199\n",
      "------------------------\n",
      "train Loss: 1421.6130 Acc: 43.5131\n",
      "test Loss: 1405.9710 Acc: 42.7366\n",
      "\n",
      "Epoch 768/1199\n",
      "------------------------\n",
      "train Loss: 1416.7379 Acc: 43.6440\n",
      "test Loss: 1425.6223 Acc: 43.7827\n",
      "\n",
      "Epoch 769/1199\n",
      "------------------------\n",
      "train Loss: 1399.1189 Acc: 43.7696\n",
      "test Loss: 1384.6386 Acc: 44.2771\n",
      "\n",
      "Epoch 770/1199\n",
      "------------------------\n",
      "train Loss: 1399.8733 Acc: 43.9181\n",
      "test Loss: 1403.7865 Acc: 43.8262\n",
      "\n",
      "Epoch 771/1199\n",
      "------------------------\n",
      "train Loss: 1422.9815 Acc: 43.8548\n",
      "test Loss: 1433.4026 Acc: 43.5571\n",
      "\n",
      "Epoch 772/1199\n",
      "------------------------\n",
      "train Loss: 1404.4491 Acc: 44.2959\n",
      "test Loss: 1435.8252 Acc: 44.7127\n",
      "\n",
      "Epoch 773/1199\n",
      "------------------------\n",
      "train Loss: 1445.5209 Acc: 44.4158\n",
      "test Loss: 1434.7842 Acc: 45.6039\n",
      "\n",
      "Epoch 774/1199\n",
      "------------------------\n",
      "train Loss: 1430.5234 Acc: 44.8685\n",
      "test Loss: 1497.9109 Acc: 44.3098\n",
      "\n",
      "Epoch 775/1199\n",
      "------------------------\n",
      "train Loss: 1453.3840 Acc: 44.7188\n",
      "test Loss: 1449.4355 Acc: 45.4517\n",
      "\n",
      "Epoch 776/1199\n",
      "------------------------\n",
      "train Loss: 1450.4215 Acc: 44.9832\n",
      "test Loss: 1466.3566 Acc: 44.9647\n",
      "\n",
      "Epoch 777/1199\n",
      "------------------------\n",
      "train Loss: 1433.0042 Acc: 44.5988\n",
      "test Loss: 1453.4989 Acc: 45.1217\n",
      "\n",
      "Epoch 778/1199\n",
      "------------------------\n",
      "train Loss: 1435.9289 Acc: 44.6543\n",
      "test Loss: 1488.1862 Acc: 44.2531\n",
      "\n",
      "Epoch 779/1199\n",
      "------------------------\n",
      "train Loss: 1444.4263 Acc: 44.5101\n",
      "test Loss: 1458.1883 Acc: 43.5794\n",
      "\n",
      "Epoch 780/1199\n",
      "------------------------\n",
      "train Loss: 1432.0254 Acc: 44.1792\n",
      "test Loss: 1442.9001 Acc: 43.8302\n",
      "\n",
      "Epoch 781/1199\n",
      "------------------------\n",
      "train Loss: 1468.7350 Acc: 43.4137\n",
      "test Loss: 1430.5418 Acc: 43.3119\n",
      "\n",
      "Epoch 782/1199\n",
      "------------------------\n",
      "train Loss: 1449.3370 Acc: 43.7881\n",
      "test Loss: 1454.6814 Acc: 43.3366\n",
      "\n",
      "Epoch 783/1199\n",
      "------------------------\n",
      "train Loss: 1425.3693 Acc: 44.3704\n",
      "test Loss: 1414.9131 Acc: 45.1511\n",
      "\n",
      "Epoch 784/1199\n",
      "------------------------\n",
      "train Loss: 1422.3570 Acc: 44.9941\n",
      "test Loss: 1425.1259 Acc: 44.8198\n",
      "\n",
      "Epoch 785/1199\n",
      "------------------------\n",
      "train Loss: 1414.6862 Acc: 44.7597\n",
      "test Loss: 1409.7939 Acc: 45.4728\n",
      "\n",
      "Epoch 786/1199\n",
      "------------------------\n",
      "train Loss: 1414.6803 Acc: 44.8927\n",
      "test Loss: 1431.8374 Acc: 44.1920\n",
      "\n",
      "Epoch 787/1199\n",
      "------------------------\n",
      "train Loss: 1425.8413 Acc: 44.4624\n",
      "test Loss: 1415.2066 Acc: 44.4687\n",
      "\n",
      "Epoch 788/1199\n",
      "------------------------\n",
      "train Loss: 1422.9129 Acc: 44.1136\n",
      "test Loss: 1453.7235 Acc: 43.8032\n",
      "\n",
      "Epoch 789/1199\n",
      "------------------------\n",
      "train Loss: 1437.4495 Acc: 43.5969\n",
      "test Loss: 1408.1554 Acc: 43.8240\n",
      "\n",
      "Epoch 790/1199\n",
      "------------------------\n",
      "train Loss: 1412.4740 Acc: 43.9655\n",
      "test Loss: 1413.7193 Acc: 44.1170\n",
      "\n",
      "Epoch 791/1199\n",
      "------------------------\n",
      "train Loss: 1431.3771 Acc: 43.8035\n",
      "test Loss: 1461.2798 Acc: 43.4286\n",
      "\n",
      "Epoch 792/1199\n",
      "------------------------\n",
      "train Loss: 1430.7461 Acc: 44.4310\n",
      "test Loss: 1453.1808 Acc: 45.0625\n",
      "\n",
      "Epoch 793/1199\n",
      "------------------------\n",
      "train Loss: 1444.0215 Acc: 44.2017\n",
      "test Loss: 1430.0756 Acc: 45.1704\n",
      "\n",
      "Epoch 794/1199\n",
      "------------------------\n",
      "train Loss: 1444.2665 Acc: 44.6021\n",
      "test Loss: 1480.6030 Acc: 44.6307\n",
      "\n",
      "Epoch 795/1199\n",
      "------------------------\n",
      "train Loss: 1444.8160 Acc: 44.7320\n",
      "test Loss: 1476.7208 Acc: 45.7379\n",
      "\n",
      "Epoch 796/1199\n",
      "------------------------\n",
      "train Loss: 1468.6027 Acc: 45.5178\n",
      "test Loss: 1452.2686 Acc: 44.4984\n",
      "\n",
      "Epoch 797/1199\n",
      "------------------------\n",
      "train Loss: 1443.4260 Acc: 45.1223\n",
      "test Loss: 1488.8749 Acc: 44.7129\n",
      "\n",
      "Epoch 798/1199\n",
      "------------------------\n",
      "train Loss: 1450.2064 Acc: 45.3285\n",
      "test Loss: 1445.9423 Acc: 45.1664\n",
      "\n",
      "Epoch 799/1199\n",
      "------------------------\n",
      "train Loss: 1458.5987 Acc: 44.8116\n",
      "test Loss: 1470.8741 Acc: 44.6416\n",
      "\n",
      "Epoch 800/1199\n",
      "------------------------\n",
      "train Loss: 1459.3881 Acc: 44.9196\n",
      "test Loss: 1460.8230 Acc: 44.9759\n",
      "\n",
      "Epoch 801/1199\n",
      "------------------------\n",
      "train Loss: 1462.4433 Acc: 44.8846\n",
      "test Loss: 1450.0452 Acc: 44.6474\n",
      "\n",
      "Epoch 802/1199\n",
      "------------------------\n",
      "train Loss: 1459.0458 Acc: 44.5900\n",
      "test Loss: 1454.9760 Acc: 44.7214\n",
      "\n",
      "Epoch 803/1199\n",
      "------------------------\n",
      "train Loss: 1458.4096 Acc: 44.5821\n",
      "test Loss: 1456.2811 Acc: 44.3597\n",
      "\n",
      "Epoch 804/1199\n",
      "------------------------\n",
      "train Loss: 1438.3512 Acc: 44.3562\n",
      "test Loss: 1420.5831 Acc: 44.7549\n",
      "\n",
      "Epoch 805/1199\n",
      "------------------------\n",
      "train Loss: 1433.3074 Acc: 44.3630\n",
      "test Loss: 1401.4274 Acc: 45.0274\n",
      "\n",
      "Epoch 806/1199\n",
      "------------------------\n",
      "train Loss: 1443.4551 Acc: 44.6011\n",
      "test Loss: 1428.6593 Acc: 44.5518\n",
      "\n",
      "Epoch 807/1199\n",
      "------------------------\n",
      "train Loss: 1467.2566 Acc: 44.1180\n",
      "test Loss: 1484.6714 Acc: 44.1385\n",
      "\n",
      "Epoch 808/1199\n",
      "------------------------\n",
      "train Loss: 1456.0236 Acc: 44.7584\n",
      "test Loss: 1482.7183 Acc: 45.3324\n",
      "\n",
      "Epoch 809/1199\n",
      "------------------------\n",
      "train Loss: 1455.3517 Acc: 44.4639\n",
      "test Loss: 1470.1478 Acc: 44.1424\n",
      "\n",
      "Epoch 810/1199\n",
      "------------------------\n",
      "train Loss: 1450.2213 Acc: 44.3595\n",
      "test Loss: 1465.7397 Acc: 44.4032\n",
      "\n",
      "Epoch 811/1199\n",
      "------------------------\n",
      "train Loss: 1461.2901 Acc: 44.3521\n",
      "test Loss: 1430.1664 Acc: 43.5689\n",
      "\n",
      "Epoch 812/1199\n",
      "------------------------\n",
      "train Loss: 1467.2779 Acc: 44.5857\n",
      "test Loss: 1453.1695 Acc: 43.2475\n",
      "\n",
      "Epoch 813/1199\n",
      "------------------------\n",
      "train Loss: 1466.5610 Acc: 43.8567\n",
      "test Loss: 1453.3012 Acc: 43.9822\n",
      "\n",
      "Epoch 814/1199\n",
      "------------------------\n",
      "train Loss: 1466.4714 Acc: 44.4754\n",
      "test Loss: 1429.4236 Acc: 44.1569\n",
      "\n",
      "Epoch 815/1199\n",
      "------------------------\n",
      "train Loss: 1449.7244 Acc: 44.7624\n",
      "test Loss: 1423.7431 Acc: 44.6170\n",
      "\n",
      "Epoch 816/1199\n",
      "------------------------\n",
      "train Loss: 1436.8783 Acc: 44.5005\n",
      "test Loss: 1447.8316 Acc: 45.4597\n",
      "\n",
      "Epoch 817/1199\n",
      "------------------------\n",
      "train Loss: 1448.1275 Acc: 44.0099\n",
      "test Loss: 1414.8260 Acc: 44.8666\n",
      "\n",
      "Epoch 818/1199\n",
      "------------------------\n",
      "train Loss: 1435.8508 Acc: 44.1833\n",
      "test Loss: 1468.1265 Acc: 43.9561\n",
      "\n",
      "Epoch 819/1199\n",
      "------------------------\n",
      "train Loss: 1447.3387 Acc: 43.9548\n",
      "test Loss: 1458.0666 Acc: 43.1932\n",
      "\n",
      "Epoch 820/1199\n",
      "------------------------\n",
      "train Loss: 1447.3777 Acc: 43.8106\n",
      "test Loss: 1470.6891 Acc: 43.6165\n",
      "\n",
      "Epoch 821/1199\n",
      "------------------------\n",
      "train Loss: 1435.9857 Acc: 43.9398\n",
      "test Loss: 1453.0810 Acc: 43.8915\n",
      "\n",
      "Epoch 822/1199\n",
      "------------------------\n",
      "train Loss: 1440.6419 Acc: 44.4118\n",
      "test Loss: 1462.2527 Acc: 44.3524\n",
      "\n",
      "Epoch 823/1199\n",
      "------------------------\n",
      "train Loss: 1438.2386 Acc: 44.0226\n",
      "test Loss: 1470.9637 Acc: 43.8231\n",
      "\n",
      "Epoch 824/1199\n",
      "------------------------\n",
      "train Loss: 1450.8726 Acc: 44.2692\n",
      "test Loss: 1465.6301 Acc: 44.0348\n",
      "\n",
      "Epoch 825/1199\n",
      "------------------------\n",
      "train Loss: 1451.6967 Acc: 44.3252\n",
      "test Loss: 1474.2717 Acc: 43.7171\n",
      "\n",
      "Epoch 826/1199\n",
      "------------------------\n",
      "train Loss: 1435.0073 Acc: 43.9393\n",
      "test Loss: 1457.2492 Acc: 44.1720\n",
      "\n",
      "Epoch 827/1199\n",
      "------------------------\n",
      "train Loss: 1426.5900 Acc: 44.8473\n",
      "test Loss: 1454.8847 Acc: 44.7749\n",
      "\n",
      "Epoch 828/1199\n",
      "------------------------\n",
      "train Loss: 1440.1235 Acc: 44.5538\n",
      "test Loss: 1422.0074 Acc: 45.2536\n",
      "\n",
      "Epoch 829/1199\n",
      "------------------------\n",
      "train Loss: 1448.1646 Acc: 44.8238\n",
      "test Loss: 1445.8826 Acc: 44.2141\n",
      "\n",
      "Epoch 830/1199\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1443.8866 Acc: 44.7255\n",
      "test Loss: 1467.8335 Acc: 44.5379\n",
      "\n",
      "Epoch 831/1199\n",
      "------------------------\n",
      "train Loss: 1443.6160 Acc: 44.7858\n",
      "test Loss: 1411.8236 Acc: 44.5682\n",
      "\n",
      "Epoch 832/1199\n",
      "------------------------\n",
      "train Loss: 1473.2938 Acc: 44.9103\n",
      "test Loss: 1455.8400 Acc: 45.5011\n",
      "\n",
      "Epoch 833/1199\n",
      "------------------------\n",
      "train Loss: 1430.4121 Acc: 44.6872\n",
      "test Loss: 1451.2992 Acc: 44.7426\n",
      "\n",
      "Epoch 834/1199\n",
      "------------------------\n",
      "train Loss: 1428.9420 Acc: 44.8420\n",
      "test Loss: 1459.7121 Acc: 44.6535\n",
      "\n",
      "Epoch 835/1199\n",
      "------------------------\n",
      "train Loss: 1449.3119 Acc: 44.1795\n",
      "test Loss: 1480.6734 Acc: 44.1535\n",
      "\n",
      "Epoch 836/1199\n",
      "------------------------\n",
      "train Loss: 1460.5162 Acc: 44.2056\n",
      "test Loss: 1445.1939 Acc: 44.1065\n",
      "\n",
      "Epoch 837/1199\n",
      "------------------------\n",
      "train Loss: 1453.0897 Acc: 43.7268\n",
      "test Loss: 1458.5426 Acc: 43.4459\n",
      "\n",
      "Epoch 838/1199\n",
      "------------------------\n",
      "train Loss: 1435.3782 Acc: 43.4455\n",
      "test Loss: 1454.6393 Acc: 43.7200\n",
      "\n",
      "Epoch 839/1199\n",
      "------------------------\n",
      "train Loss: 1441.9450 Acc: 43.6835\n",
      "test Loss: 1435.5318 Acc: 44.1832\n",
      "\n",
      "Epoch 840/1199\n",
      "------------------------\n",
      "train Loss: 1461.3077 Acc: 44.3564\n",
      "test Loss: 1439.3468 Acc: 44.7832\n",
      "\n",
      "Epoch 841/1199\n",
      "------------------------\n",
      "train Loss: 1438.8806 Acc: 45.1026\n",
      "test Loss: 1448.7179 Acc: 44.2903\n",
      "\n",
      "Epoch 842/1199\n",
      "------------------------\n",
      "train Loss: 1480.8383 Acc: 44.4082\n",
      "test Loss: 1485.6072 Acc: 44.3543\n",
      "\n",
      "Epoch 843/1199\n",
      "------------------------\n",
      "train Loss: 1470.8635 Acc: 44.4957\n",
      "test Loss: 1492.3461 Acc: 44.7882\n",
      "\n",
      "Epoch 844/1199\n",
      "------------------------\n",
      "train Loss: 1492.0828 Acc: 44.5079\n",
      "test Loss: 1478.9843 Acc: 44.4226\n",
      "\n",
      "Epoch 845/1199\n",
      "------------------------\n",
      "train Loss: 1489.9812 Acc: 44.2336\n",
      "test Loss: 1490.6671 Acc: 43.7687\n",
      "\n",
      "Epoch 846/1199\n",
      "------------------------\n",
      "train Loss: 1486.2069 Acc: 44.1382\n",
      "test Loss: 1482.8446 Acc: 43.3096\n",
      "\n",
      "Epoch 847/1199\n",
      "------------------------\n",
      "train Loss: 1494.5051 Acc: 44.5320\n",
      "test Loss: 1506.8733 Acc: 44.2449\n",
      "\n",
      "Epoch 848/1199\n",
      "------------------------\n",
      "train Loss: 1517.0064 Acc: 44.2349\n",
      "test Loss: 1473.8477 Acc: 45.0683\n",
      "\n",
      "Epoch 849/1199\n",
      "------------------------\n",
      "train Loss: 1499.1242 Acc: 44.4407\n",
      "test Loss: 1472.2803 Acc: 44.7493\n",
      "\n",
      "Epoch 850/1199\n",
      "------------------------\n",
      "train Loss: 1488.7529 Acc: 45.1788\n",
      "test Loss: 1494.0985 Acc: 45.9003\n",
      "\n",
      "Epoch 851/1199\n",
      "------------------------\n",
      "train Loss: 1493.6248 Acc: 44.9939\n",
      "test Loss: 1471.5783 Acc: 45.2438\n",
      "\n",
      "Epoch 852/1199\n",
      "------------------------\n",
      "train Loss: 1484.5740 Acc: 45.3171\n",
      "test Loss: 1523.9840 Acc: 45.5105\n",
      "\n",
      "Epoch 853/1199\n",
      "------------------------\n",
      "train Loss: 1495.9867 Acc: 45.9283\n",
      "test Loss: 1489.6995 Acc: 45.8020\n",
      "\n",
      "Epoch 854/1199\n",
      "------------------------\n",
      "train Loss: 1500.4228 Acc: 44.7859\n",
      "test Loss: 1474.1230 Acc: 45.1182\n",
      "\n",
      "Epoch 855/1199\n",
      "------------------------\n",
      "train Loss: 1480.8695 Acc: 44.9163\n",
      "test Loss: 1484.3034 Acc: 45.0714\n",
      "\n",
      "Epoch 856/1199\n",
      "------------------------\n",
      "train Loss: 1499.6673 Acc: 43.9032\n",
      "test Loss: 1526.3831 Acc: 42.7996\n",
      "\n",
      "Epoch 857/1199\n",
      "------------------------\n",
      "train Loss: 1523.2286 Acc: 43.1551\n",
      "test Loss: 1517.4501 Acc: 44.2431\n",
      "\n",
      "Epoch 858/1199\n",
      "------------------------\n",
      "train Loss: 1515.6375 Acc: 43.1877\n",
      "test Loss: 1503.4392 Acc: 43.0791\n",
      "\n",
      "Epoch 859/1199\n",
      "------------------------\n",
      "train Loss: 1510.0414 Acc: 43.4038\n",
      "test Loss: 1500.4691 Acc: 43.9247\n",
      "\n",
      "Epoch 860/1199\n",
      "------------------------\n",
      "train Loss: 1500.5430 Acc: 43.8864\n",
      "test Loss: 1526.1601 Acc: 45.1224\n",
      "\n",
      "Epoch 861/1199\n",
      "------------------------\n",
      "train Loss: 1497.0122 Acc: 44.5506\n",
      "test Loss: 1507.4807 Acc: 44.9469\n",
      "\n",
      "Epoch 862/1199\n",
      "------------------------\n",
      "train Loss: 1491.6377 Acc: 44.9541\n",
      "test Loss: 1524.9766 Acc: 45.3118\n",
      "\n",
      "Epoch 863/1199\n",
      "------------------------\n",
      "train Loss: 1489.8017 Acc: 44.5257\n",
      "test Loss: 1489.6881 Acc: 45.0604\n",
      "\n",
      "Epoch 864/1199\n",
      "------------------------\n",
      "train Loss: 1471.8172 Acc: 44.9886\n",
      "test Loss: 1489.1675 Acc: 44.4628\n",
      "\n",
      "Epoch 865/1199\n",
      "------------------------\n",
      "train Loss: 1475.5385 Acc: 44.6097\n",
      "test Loss: 1464.6553 Acc: 44.9507\n",
      "\n",
      "Epoch 866/1199\n",
      "------------------------\n",
      "train Loss: 1463.0705 Acc: 44.5065\n",
      "test Loss: 1458.2549 Acc: 44.4963\n",
      "\n",
      "Epoch 867/1199\n",
      "------------------------\n",
      "train Loss: 1461.4512 Acc: 44.3016\n",
      "test Loss: 1501.2337 Acc: 44.9201\n",
      "\n",
      "Epoch 868/1199\n",
      "------------------------\n",
      "train Loss: 1483.6820 Acc: 44.6377\n",
      "test Loss: 1483.8148 Acc: 45.9965\n",
      "\n",
      "Epoch 869/1199\n",
      "------------------------\n",
      "train Loss: 1500.0104 Acc: 44.6254\n",
      "test Loss: 1468.6746 Acc: 45.5073\n",
      "\n",
      "Epoch 870/1199\n",
      "------------------------\n",
      "train Loss: 1474.7432 Acc: 45.4510\n",
      "test Loss: 1486.6535 Acc: 44.8172\n",
      "\n",
      "Epoch 871/1199\n",
      "------------------------\n",
      "train Loss: 1501.5064 Acc: 45.2437\n",
      "test Loss: 1469.4117 Acc: 45.7279\n",
      "\n",
      "Epoch 872/1199\n",
      "------------------------\n",
      "train Loss: 1494.5400 Acc: 45.3833\n",
      "test Loss: 1546.3037 Acc: 43.7198\n",
      "\n",
      "Epoch 873/1199\n",
      "------------------------\n",
      "train Loss: 1515.4957 Acc: 45.1276\n",
      "test Loss: 1499.1147 Acc: 44.8157\n",
      "\n",
      "Epoch 874/1199\n",
      "------------------------\n",
      "train Loss: 1513.0151 Acc: 44.2525\n",
      "test Loss: 1507.6013 Acc: 44.1784\n",
      "\n",
      "Epoch 875/1199\n",
      "------------------------\n",
      "train Loss: 1500.0297 Acc: 44.2793\n",
      "test Loss: 1526.8702 Acc: 43.6447\n",
      "\n",
      "Epoch 876/1199\n",
      "------------------------\n",
      "train Loss: 1512.0150 Acc: 44.0239\n",
      "test Loss: 1481.1727 Acc: 44.1288\n",
      "\n",
      "Epoch 877/1199\n",
      "------------------------\n",
      "train Loss: 1510.4347 Acc: 43.5439\n",
      "test Loss: 1530.1436 Acc: 43.6169\n",
      "\n",
      "Epoch 878/1199\n",
      "------------------------\n",
      "train Loss: 1511.9329 Acc: 44.0058\n",
      "test Loss: 1505.3871 Acc: 44.4276\n",
      "\n",
      "Epoch 879/1199\n",
      "------------------------\n",
      "train Loss: 1518.1956 Acc: 44.4089\n",
      "test Loss: 1488.5180 Acc: 44.1942\n",
      "\n",
      "Epoch 880/1199\n",
      "------------------------\n",
      "train Loss: 1513.3031 Acc: 44.6658\n",
      "test Loss: 1545.0536 Acc: 43.6164\n",
      "\n",
      "Epoch 881/1199\n",
      "------------------------\n",
      "train Loss: 1535.1751 Acc: 43.9529\n",
      "test Loss: 1474.0451 Acc: 44.3177\n",
      "\n",
      "Epoch 882/1199\n",
      "------------------------\n",
      "train Loss: 1508.5317 Acc: 44.3262\n",
      "test Loss: 1525.2233 Acc: 44.1091\n",
      "\n",
      "Epoch 883/1199\n",
      "------------------------\n",
      "train Loss: 1515.7208 Acc: 44.1471\n",
      "test Loss: 1518.9896 Acc: 44.6492\n",
      "\n",
      "Epoch 884/1199\n",
      "------------------------\n",
      "train Loss: 1518.3350 Acc: 44.0463\n",
      "test Loss: 1497.8545 Acc: 44.4523\n",
      "\n",
      "Epoch 885/1199\n",
      "------------------------\n",
      "train Loss: 1505.2588 Acc: 44.5048\n",
      "test Loss: 1525.9437 Acc: 43.6998\n",
      "\n",
      "Epoch 886/1199\n",
      "------------------------\n",
      "train Loss: 1505.7873 Acc: 45.0434\n",
      "test Loss: 1457.5946 Acc: 45.5609\n",
      "\n",
      "Epoch 887/1199\n",
      "------------------------\n",
      "train Loss: 1510.6797 Acc: 45.2574\n",
      "test Loss: 1521.9212 Acc: 44.9044\n",
      "\n",
      "Epoch 888/1199\n",
      "------------------------\n",
      "train Loss: 1515.2445 Acc: 45.3114\n",
      "test Loss: 1528.1931 Acc: 43.8283\n",
      "\n",
      "Epoch 889/1199\n",
      "------------------------\n",
      "train Loss: 1529.1293 Acc: 44.8707\n",
      "test Loss: 1538.0202 Acc: 44.7944\n",
      "\n",
      "Epoch 890/1199\n",
      "------------------------\n",
      "train Loss: 1528.2087 Acc: 44.3668\n",
      "test Loss: 1554.2396 Acc: 43.8734\n",
      "\n",
      "Epoch 891/1199\n",
      "------------------------\n",
      "train Loss: 1515.1645 Acc: 44.3207\n",
      "test Loss: 1546.8079 Acc: 43.4178\n",
      "\n",
      "Epoch 892/1199\n",
      "------------------------\n",
      "train Loss: 1535.1651 Acc: 43.9130\n",
      "test Loss: 1533.0609 Acc: 44.1730\n",
      "\n",
      "Epoch 893/1199\n",
      "------------------------\n",
      "train Loss: 1532.0071 Acc: 44.1436\n",
      "test Loss: 1545.1314 Acc: 44.0055\n",
      "\n",
      "Epoch 894/1199\n",
      "------------------------\n",
      "train Loss: 1550.7585 Acc: 43.7878\n",
      "test Loss: 1527.6780 Acc: 43.2616\n",
      "\n",
      "Epoch 895/1199\n",
      "------------------------\n",
      "train Loss: 1550.2894 Acc: 43.3194\n",
      "test Loss: 1540.8986 Acc: 44.0262\n",
      "\n",
      "Epoch 896/1199\n",
      "------------------------\n",
      "train Loss: 1534.6046 Acc: 44.4029\n",
      "test Loss: 1522.1545 Acc: 43.6849\n",
      "\n",
      "Epoch 897/1199\n",
      "------------------------\n",
      "train Loss: 1518.6156 Acc: 44.6945\n",
      "test Loss: 1502.2754 Acc: 45.6749\n",
      "\n",
      "Epoch 898/1199\n",
      "------------------------\n",
      "train Loss: 1500.5802 Acc: 44.2869\n",
      "test Loss: 1495.1801 Acc: 44.3718\n",
      "\n",
      "Epoch 899/1199\n",
      "------------------------\n",
      "train Loss: 1506.7233 Acc: 44.4244\n",
      "test Loss: 1513.1039 Acc: 43.9658\n",
      "\n",
      "Epoch 900/1199\n",
      "------------------------\n",
      "train Loss: 1493.9687 Acc: 44.2618\n",
      "test Loss: 1546.9893 Acc: 43.1352\n",
      "\n",
      "Epoch 901/1199\n",
      "------------------------\n",
      "train Loss: 1480.6084 Acc: 44.2103\n",
      "test Loss: 1455.3353 Acc: 44.4017\n",
      "\n",
      "Epoch 902/1199\n",
      "------------------------\n",
      "train Loss: 1472.2178 Acc: 44.4056\n",
      "test Loss: 1480.3180 Acc: 44.7882\n",
      "\n",
      "Epoch 903/1199\n",
      "------------------------\n",
      "train Loss: 1442.8152 Acc: 44.4118\n",
      "test Loss: 1455.9010 Acc: 44.2680\n",
      "\n",
      "Epoch 904/1199\n",
      "------------------------\n",
      "train Loss: 1428.7311 Acc: 44.5284\n",
      "test Loss: 1457.9186 Acc: 45.0884\n",
      "\n",
      "Epoch 905/1199\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1428.3497 Acc: 44.0978\n",
      "test Loss: 1436.9776 Acc: 44.5588\n",
      "\n",
      "Epoch 906/1199\n",
      "------------------------\n",
      "train Loss: 1422.6992 Acc: 44.9058\n",
      "test Loss: 1458.6141 Acc: 44.2531\n",
      "\n",
      "Epoch 907/1199\n",
      "------------------------\n",
      "train Loss: 1438.8798 Acc: 45.0047\n",
      "test Loss: 1444.7612 Acc: 44.0797\n",
      "\n",
      "Epoch 908/1199\n",
      "------------------------\n",
      "train Loss: 1430.6640 Acc: 45.0030\n",
      "test Loss: 1408.9164 Acc: 45.0422\n",
      "\n",
      "Epoch 909/1199\n",
      "------------------------\n",
      "train Loss: 1420.9600 Acc: 44.8724\n",
      "test Loss: 1437.4128 Acc: 44.5328\n",
      "\n",
      "Epoch 910/1199\n",
      "------------------------\n",
      "train Loss: 1395.8676 Acc: 44.9895\n",
      "test Loss: 1396.3755 Acc: 45.6044\n",
      "\n",
      "Epoch 911/1199\n",
      "------------------------\n",
      "train Loss: 1414.7786 Acc: 44.6699\n",
      "test Loss: 1416.0378 Acc: 44.0373\n",
      "\n",
      "Epoch 912/1199\n",
      "------------------------\n",
      "train Loss: 1416.9877 Acc: 44.4829\n",
      "test Loss: 1415.3284 Acc: 44.1537\n",
      "\n",
      "Epoch 913/1199\n",
      "------------------------\n",
      "train Loss: 1413.8696 Acc: 44.0565\n",
      "test Loss: 1427.1728 Acc: 45.1859\n",
      "\n",
      "Epoch 914/1199\n",
      "------------------------\n",
      "train Loss: 1400.4198 Acc: 44.3119\n",
      "test Loss: 1389.8186 Acc: 43.2520\n",
      "\n",
      "Epoch 915/1199\n",
      "------------------------\n",
      "train Loss: 1389.4490 Acc: 44.1044\n",
      "test Loss: 1390.8624 Acc: 44.5543\n",
      "\n",
      "Epoch 916/1199\n",
      "------------------------\n",
      "train Loss: 1369.5596 Acc: 44.1395\n",
      "test Loss: 1407.1181 Acc: 43.6549\n",
      "\n",
      "Epoch 917/1199\n",
      "------------------------\n",
      "train Loss: 1385.2928 Acc: 44.3326\n",
      "test Loss: 1338.7527 Acc: 44.1504\n",
      "\n",
      "Epoch 918/1199\n",
      "------------------------\n",
      "train Loss: 1363.3576 Acc: 44.2921\n",
      "test Loss: 1352.8758 Acc: 43.7202\n",
      "\n",
      "Epoch 919/1199\n",
      "------------------------\n",
      "train Loss: 1357.8535 Acc: 44.9449\n",
      "test Loss: 1359.4667 Acc: 44.6367\n",
      "\n",
      "Epoch 920/1199\n",
      "------------------------\n",
      "train Loss: 1330.5112 Acc: 45.2678\n",
      "test Loss: 1338.2294 Acc: 44.5531\n",
      "\n",
      "Epoch 921/1199\n",
      "------------------------\n",
      "train Loss: 1326.2830 Acc: 45.1877\n",
      "test Loss: 1327.5706 Acc: 45.0201\n",
      "\n",
      "Epoch 922/1199\n",
      "------------------------\n",
      "train Loss: 1329.0053 Acc: 45.3061\n",
      "test Loss: 1317.6736 Acc: 44.6533\n",
      "\n",
      "Epoch 923/1199\n",
      "------------------------\n",
      "train Loss: 1309.9493 Acc: 45.7056\n",
      "test Loss: 1302.3561 Acc: 45.1229\n",
      "\n",
      "Epoch 924/1199\n",
      "------------------------\n",
      "train Loss: 1312.7426 Acc: 45.3120\n",
      "test Loss: 1290.0065 Acc: 45.4861\n",
      "\n",
      "Epoch 925/1199\n",
      "------------------------\n",
      "train Loss: 1323.5798 Acc: 45.3270\n",
      "test Loss: 1313.1262 Acc: 44.7745\n",
      "\n",
      "Epoch 926/1199\n",
      "------------------------\n",
      "train Loss: 1315.0342 Acc: 45.5077\n",
      "test Loss: 1337.5830 Acc: 44.7371\n",
      "\n",
      "Epoch 927/1199\n",
      "------------------------\n",
      "train Loss: 1333.3517 Acc: 45.1335\n",
      "test Loss: 1321.3275 Acc: 44.6526\n",
      "\n",
      "Epoch 928/1199\n",
      "------------------------\n",
      "train Loss: 1316.7335 Acc: 45.3547\n",
      "test Loss: 1325.9898 Acc: 45.8744\n",
      "\n",
      "Epoch 929/1199\n",
      "------------------------\n",
      "train Loss: 1297.4226 Acc: 46.0282\n",
      "test Loss: 1297.3570 Acc: 46.3296\n",
      "\n",
      "Epoch 930/1199\n",
      "------------------------\n",
      "train Loss: 1302.9613 Acc: 45.4257\n",
      "test Loss: 1334.6954 Acc: 44.3148\n",
      "\n",
      "Epoch 931/1199\n",
      "------------------------\n",
      "train Loss: 1311.5714 Acc: 45.7741\n",
      "test Loss: 1318.0105 Acc: 44.8994\n",
      "\n",
      "Epoch 932/1199\n",
      "------------------------\n",
      "train Loss: 1288.1485 Acc: 45.6018\n",
      "test Loss: 1332.6871 Acc: 44.7274\n",
      "\n",
      "Epoch 933/1199\n",
      "------------------------\n",
      "train Loss: 1304.0069 Acc: 44.8707\n",
      "test Loss: 1329.2135 Acc: 44.4785\n",
      "\n",
      "Epoch 934/1199\n",
      "------------------------\n",
      "train Loss: 1297.7750 Acc: 45.0239\n",
      "test Loss: 1291.5069 Acc: 45.8429\n",
      "\n",
      "Epoch 935/1199\n",
      "------------------------\n",
      "train Loss: 1304.3525 Acc: 45.1695\n",
      "test Loss: 1276.5732 Acc: 45.3486\n",
      "\n",
      "Epoch 936/1199\n",
      "------------------------\n",
      "train Loss: 1310.0512 Acc: 45.1332\n",
      "test Loss: 1310.3665 Acc: 45.1932\n",
      "\n",
      "Epoch 937/1199\n",
      "------------------------\n",
      "train Loss: 1303.4398 Acc: 44.7350\n",
      "test Loss: 1286.6083 Acc: 45.2134\n",
      "\n",
      "Epoch 938/1199\n",
      "------------------------\n",
      "train Loss: 1287.3709 Acc: 45.0970\n",
      "test Loss: 1258.4330 Acc: 46.5305\n",
      "\n",
      "Epoch 939/1199\n",
      "------------------------\n",
      "train Loss: 1283.5266 Acc: 45.3921\n",
      "test Loss: 1276.1262 Acc: 46.2794\n",
      "\n",
      "Epoch 940/1199\n",
      "------------------------\n",
      "train Loss: 1273.8312 Acc: 45.0361\n",
      "test Loss: 1279.5530 Acc: 45.3783\n",
      "\n",
      "Epoch 941/1199\n",
      "------------------------\n",
      "train Loss: 1261.1906 Acc: 45.7452\n",
      "test Loss: 1257.4611 Acc: 46.2220\n",
      "\n",
      "Epoch 942/1199\n",
      "------------------------\n",
      "train Loss: 1259.3229 Acc: 45.6214\n",
      "test Loss: 1264.8979 Acc: 45.6923\n",
      "\n",
      "Epoch 943/1199\n",
      "------------------------\n",
      "train Loss: 1268.5474 Acc: 45.1260\n",
      "test Loss: 1275.5318 Acc: 45.3505\n",
      "\n",
      "Epoch 944/1199\n",
      "------------------------\n",
      "train Loss: 1283.6793 Acc: 44.8202\n",
      "test Loss: 1291.3888 Acc: 45.0424\n",
      "\n",
      "Epoch 945/1199\n",
      "------------------------\n",
      "train Loss: 1278.6599 Acc: 44.4479\n",
      "test Loss: 1289.7619 Acc: 44.3903\n",
      "\n",
      "Epoch 946/1199\n",
      "------------------------\n",
      "train Loss: 1282.0423 Acc: 45.1753\n",
      "test Loss: 1288.1039 Acc: 44.5657\n",
      "\n",
      "Epoch 947/1199\n",
      "------------------------\n",
      "train Loss: 1263.9565 Acc: 44.7309\n",
      "test Loss: 1278.5279 Acc: 45.2101\n",
      "\n",
      "Epoch 948/1199\n",
      "------------------------\n",
      "train Loss: 1273.2860 Acc: 44.5622\n",
      "test Loss: 1242.9417 Acc: 45.2685\n",
      "\n",
      "Epoch 949/1199\n",
      "------------------------\n",
      "train Loss: 1252.7427 Acc: 45.2005\n",
      "test Loss: 1245.0803 Acc: 45.0419\n",
      "\n",
      "Epoch 950/1199\n",
      "------------------------\n",
      "train Loss: 1271.4489 Acc: 45.4918\n",
      "test Loss: 1281.5062 Acc: 45.9089\n",
      "\n",
      "Epoch 951/1199\n",
      "------------------------\n",
      "train Loss: 1267.8073 Acc: 45.3722\n",
      "test Loss: 1259.4292 Acc: 44.5037\n",
      "\n",
      "Epoch 952/1199\n",
      "------------------------\n",
      "train Loss: 1255.6100 Acc: 45.5586\n",
      "test Loss: 1309.3228 Acc: 45.0754\n",
      "\n",
      "Epoch 953/1199\n",
      "------------------------\n",
      "train Loss: 1264.8634 Acc: 45.9365\n",
      "test Loss: 1271.0690 Acc: 45.2901\n",
      "\n",
      "Epoch 954/1199\n",
      "------------------------\n",
      "train Loss: 1270.4847 Acc: 45.3556\n",
      "test Loss: 1281.8367 Acc: 45.6381\n",
      "\n",
      "Epoch 955/1199\n",
      "------------------------\n",
      "train Loss: 1272.7537 Acc: 45.5336\n",
      "test Loss: 1265.0771 Acc: 45.6205\n",
      "\n",
      "Epoch 956/1199\n",
      "------------------------\n",
      "train Loss: 1266.1111 Acc: 45.3240\n",
      "test Loss: 1296.1039 Acc: 45.0649\n",
      "\n",
      "Epoch 957/1199\n",
      "------------------------\n",
      "train Loss: 1270.4001 Acc: 45.5109\n",
      "test Loss: 1276.4561 Acc: 45.6749\n",
      "\n",
      "Epoch 958/1199\n",
      "------------------------\n",
      "train Loss: 1267.4388 Acc: 45.3479\n",
      "test Loss: 1268.0457 Acc: 45.9756\n",
      "\n",
      "Epoch 959/1199\n",
      "------------------------\n",
      "train Loss: 1270.6277 Acc: 44.8531\n",
      "test Loss: 1282.3176 Acc: 44.0063\n",
      "\n",
      "Epoch 960/1199\n",
      "------------------------\n",
      "train Loss: 1258.7980 Acc: 45.1985\n",
      "test Loss: 1247.9926 Acc: 45.7048\n",
      "\n",
      "Epoch 961/1199\n",
      "------------------------\n",
      "train Loss: 1261.8915 Acc: 44.6527\n",
      "test Loss: 1256.0261 Acc: 45.0450\n",
      "\n",
      "Epoch 962/1199\n",
      "------------------------\n",
      "train Loss: 1289.6559 Acc: 44.6334\n",
      "test Loss: 1286.6034 Acc: 44.5022\n",
      "\n",
      "Epoch 963/1199\n",
      "------------------------\n",
      "train Loss: 1280.4698 Acc: 44.8353\n",
      "test Loss: 1297.5305 Acc: 44.2711\n",
      "\n",
      "Epoch 964/1199\n",
      "------------------------\n",
      "train Loss: 1266.5691 Acc: 44.6937\n",
      "test Loss: 1282.3454 Acc: 44.8041\n",
      "\n",
      "Epoch 965/1199\n",
      "------------------------\n",
      "train Loss: 1281.1206 Acc: 45.2575\n",
      "test Loss: 1287.4611 Acc: 44.2685\n",
      "\n",
      "Epoch 966/1199\n",
      "------------------------\n",
      "train Loss: 1271.4593 Acc: 44.9018\n",
      "test Loss: 1295.3497 Acc: 45.3220\n",
      "\n",
      "Epoch 967/1199\n",
      "------------------------\n",
      "train Loss: 1264.3358 Acc: 45.1713\n",
      "test Loss: 1258.2006 Acc: 44.7835\n",
      "\n",
      "Epoch 968/1199\n",
      "------------------------\n",
      "train Loss: 1283.6163 Acc: 44.8823\n",
      "test Loss: 1266.0504 Acc: 45.4061\n",
      "\n",
      "Epoch 969/1199\n",
      "------------------------\n",
      "train Loss: 1297.0808 Acc: 44.4575\n",
      "test Loss: 1293.5429 Acc: 44.5015\n",
      "\n",
      "Epoch 970/1199\n",
      "------------------------\n",
      "train Loss: 1275.3674 Acc: 44.3919\n",
      "test Loss: 1306.1244 Acc: 43.8592\n",
      "\n",
      "Epoch 971/1199\n",
      "------------------------\n",
      "train Loss: 1284.2273 Acc: 44.1477\n",
      "test Loss: 1291.2816 Acc: 44.4290\n",
      "\n",
      "Epoch 972/1199\n",
      "------------------------\n",
      "train Loss: 1282.9907 Acc: 44.5913\n",
      "test Loss: 1287.2539 Acc: 43.9739\n",
      "\n",
      "Epoch 973/1199\n",
      "------------------------\n",
      "train Loss: 1289.4195 Acc: 44.5580\n",
      "test Loss: 1289.9107 Acc: 44.3770\n",
      "\n",
      "Epoch 974/1199\n",
      "------------------------\n",
      "train Loss: 1296.1538 Acc: 44.7511\n",
      "test Loss: 1291.5365 Acc: 45.3377\n",
      "\n",
      "Epoch 975/1199\n",
      "------------------------\n",
      "train Loss: 1282.4201 Acc: 45.1749\n",
      "test Loss: 1271.8763 Acc: 45.5255\n",
      "\n",
      "Epoch 976/1199\n",
      "------------------------\n",
      "train Loss: 1283.4302 Acc: 45.1631\n",
      "test Loss: 1281.4812 Acc: 45.3445\n",
      "\n",
      "Epoch 977/1199\n",
      "------------------------\n",
      "train Loss: 1265.0847 Acc: 45.9016\n",
      "test Loss: 1276.3264 Acc: 45.2329\n",
      "\n",
      "Epoch 978/1199\n",
      "------------------------\n",
      "train Loss: 1276.3670 Acc: 45.6124\n",
      "test Loss: 1256.8632 Acc: 46.2541\n",
      "\n",
      "Epoch 979/1199\n",
      "------------------------\n",
      "train Loss: 1291.8369 Acc: 45.4089\n",
      "test Loss: 1275.4399 Acc: 45.8051\n",
      "\n",
      "Epoch 980/1199\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1290.3362 Acc: 45.6214\n",
      "test Loss: 1318.3716 Acc: 44.9022\n",
      "\n",
      "Epoch 981/1199\n",
      "------------------------\n",
      "train Loss: 1293.9988 Acc: 45.1955\n",
      "test Loss: 1287.8121 Acc: 44.9823\n",
      "\n",
      "Epoch 982/1199\n",
      "------------------------\n",
      "train Loss: 1314.2491 Acc: 45.2342\n",
      "test Loss: 1285.7172 Acc: 45.2875\n",
      "\n",
      "Epoch 983/1199\n",
      "------------------------\n",
      "train Loss: 1307.9769 Acc: 45.1572\n",
      "test Loss: 1319.4589 Acc: 44.4248\n",
      "\n",
      "Epoch 984/1199\n",
      "------------------------\n",
      "train Loss: 1302.2959 Acc: 44.5970\n",
      "test Loss: 1289.1769 Acc: 44.9561\n",
      "\n",
      "Epoch 985/1199\n",
      "------------------------\n",
      "train Loss: 1308.8759 Acc: 44.6087\n",
      "test Loss: 1276.4617 Acc: 45.0257\n",
      "\n",
      "Epoch 986/1199\n",
      "------------------------\n",
      "train Loss: 1304.9247 Acc: 44.5110\n",
      "test Loss: 1275.8045 Acc: 44.8340\n",
      "\n",
      "Epoch 987/1199\n",
      "------------------------\n",
      "train Loss: 1293.9260 Acc: 44.1806\n",
      "test Loss: 1297.1716 Acc: 44.7108\n",
      "\n",
      "Epoch 988/1199\n",
      "------------------------\n",
      "train Loss: 1269.5582 Acc: 44.2580\n",
      "test Loss: 1283.3401 Acc: 45.1771\n",
      "\n",
      "Epoch 989/1199\n",
      "------------------------\n",
      "train Loss: 1283.7162 Acc: 44.2481\n",
      "test Loss: 1285.8389 Acc: 44.5640\n",
      "\n",
      "Epoch 990/1199\n",
      "------------------------\n",
      "train Loss: 1295.7728 Acc: 44.0217\n",
      "test Loss: 1288.0604 Acc: 44.2554\n",
      "\n",
      "Epoch 991/1199\n",
      "------------------------\n",
      "train Loss: 1294.5562 Acc: 44.2967\n",
      "test Loss: 1258.1679 Acc: 44.8593\n",
      "\n",
      "Epoch 992/1199\n",
      "------------------------\n",
      "train Loss: 1273.6033 Acc: 45.2217\n",
      "test Loss: 1268.6905 Acc: 45.7954\n",
      "\n",
      "Epoch 993/1199\n",
      "------------------------\n",
      "train Loss: 1276.2815 Acc: 45.5725\n",
      "test Loss: 1291.0307 Acc: 45.6415\n",
      "\n",
      "Epoch 994/1199\n",
      "------------------------\n",
      "train Loss: 1274.5671 Acc: 46.0028\n",
      "test Loss: 1298.1291 Acc: 45.2201\n",
      "\n",
      "Epoch 995/1199\n",
      "------------------------\n",
      "train Loss: 1282.0761 Acc: 45.9128\n",
      "test Loss: 1304.4469 Acc: 45.4911\n",
      "\n",
      "Epoch 996/1199\n",
      "------------------------\n",
      "train Loss: 1294.4202 Acc: 45.6991\n",
      "test Loss: 1293.8479 Acc: 45.8569\n",
      "\n",
      "Epoch 997/1199\n",
      "------------------------\n",
      "train Loss: 1294.9580 Acc: 45.1662\n",
      "test Loss: 1279.2488 Acc: 45.5581\n",
      "\n",
      "Epoch 998/1199\n",
      "------------------------\n",
      "train Loss: 1294.9591 Acc: 44.8330\n",
      "test Loss: 1308.9977 Acc: 44.8110\n",
      "\n",
      "Epoch 999/1199\n",
      "------------------------\n",
      "train Loss: 1308.4024 Acc: 44.8930\n",
      "test Loss: 1313.6708 Acc: 44.3820\n",
      "\n",
      "Epoch 1000/1199\n",
      "------------------------\n",
      "train Loss: 1300.9590 Acc: 44.2426\n",
      "test Loss: 1279.9841 Acc: 44.4031\n",
      "\n",
      "Epoch 1001/1199\n",
      "------------------------\n",
      "train Loss: 1301.7760 Acc: 43.6361\n",
      "test Loss: 1320.1614 Acc: 43.2742\n",
      "\n",
      "Epoch 1002/1199\n",
      "------------------------\n",
      "train Loss: 1293.3785 Acc: 43.7373\n",
      "test Loss: 1323.8169 Acc: 43.4856\n",
      "\n",
      "Epoch 1003/1199\n",
      "------------------------\n",
      "train Loss: 1312.7021 Acc: 43.9543\n",
      "test Loss: 1319.3806 Acc: 44.3461\n",
      "\n",
      "Epoch 1004/1199\n",
      "------------------------\n",
      "train Loss: 1295.2778 Acc: 44.4478\n",
      "test Loss: 1317.3944 Acc: 44.2012\n",
      "\n",
      "Epoch 1005/1199\n",
      "------------------------\n",
      "train Loss: 1320.7668 Acc: 44.9099\n",
      "test Loss: 1328.3870 Acc: 44.3846\n",
      "\n",
      "Epoch 1006/1199\n",
      "------------------------\n",
      "train Loss: 1308.1210 Acc: 44.8386\n",
      "test Loss: 1308.9991 Acc: 45.5225\n",
      "\n",
      "Epoch 1007/1199\n",
      "------------------------\n",
      "train Loss: 1305.9007 Acc: 45.0556\n",
      "test Loss: 1291.3182 Acc: 45.9521\n",
      "\n",
      "Epoch 1008/1199\n",
      "------------------------\n",
      "train Loss: 1317.9720 Acc: 45.2722\n",
      "test Loss: 1292.4223 Acc: 45.2279\n",
      "\n",
      "Epoch 1009/1199\n",
      "------------------------\n",
      "train Loss: 1303.2897 Acc: 45.5017\n",
      "test Loss: 1303.9000 Acc: 45.3751\n",
      "\n",
      "Epoch 1010/1199\n",
      "------------------------\n",
      "train Loss: 1305.7209 Acc: 45.3066\n",
      "test Loss: 1313.5244 Acc: 45.7203\n",
      "\n",
      "Epoch 1011/1199\n",
      "------------------------\n",
      "train Loss: 1318.1924 Acc: 45.0473\n",
      "test Loss: 1353.8913 Acc: 44.8167\n",
      "\n",
      "Epoch 1012/1199\n",
      "------------------------\n",
      "train Loss: 1342.9828 Acc: 45.2295\n",
      "test Loss: 1308.5178 Acc: 44.4754\n",
      "\n",
      "Epoch 1013/1199\n",
      "------------------------\n",
      "train Loss: 1319.0237 Acc: 44.7444\n",
      "test Loss: 1317.0086 Acc: 44.8303\n",
      "\n",
      "Epoch 1014/1199\n",
      "------------------------\n",
      "train Loss: 1309.3957 Acc: 44.1271\n",
      "test Loss: 1292.2035 Acc: 45.1918\n",
      "\n",
      "Epoch 1015/1199\n",
      "------------------------\n",
      "train Loss: 1314.5501 Acc: 44.1048\n",
      "test Loss: 1283.9790 Acc: 44.7212\n",
      "\n",
      "Epoch 1016/1199\n",
      "------------------------\n",
      "train Loss: 1303.7363 Acc: 44.6404\n",
      "test Loss: 1283.7810 Acc: 45.1331\n",
      "\n",
      "Epoch 1017/1199\n",
      "------------------------\n",
      "train Loss: 1289.8077 Acc: 44.5455\n",
      "test Loss: 1309.8968 Acc: 45.5229\n",
      "\n",
      "Epoch 1018/1199\n",
      "------------------------\n",
      "train Loss: 1283.0929 Acc: 44.3674\n",
      "test Loss: 1284.7986 Acc: 45.5244\n",
      "\n",
      "Epoch 1019/1199\n",
      "------------------------\n",
      "train Loss: 1281.9375 Acc: 44.8360\n",
      "test Loss: 1265.7603 Acc: 45.4296\n",
      "\n",
      "Epoch 1020/1199\n",
      "------------------------\n",
      "train Loss: 1282.7543 Acc: 44.9634\n",
      "test Loss: 1310.7514 Acc: 45.2827\n",
      "\n",
      "Epoch 1021/1199\n",
      "------------------------\n",
      "train Loss: 1280.1540 Acc: 45.3546\n",
      "test Loss: 1279.9270 Acc: 45.5607\n",
      "\n",
      "Epoch 1022/1199\n",
      "------------------------\n",
      "train Loss: 1287.5274 Acc: 45.0396\n",
      "test Loss: 1308.1775 Acc: 45.0471\n",
      "\n",
      "Epoch 1023/1199\n",
      "------------------------\n",
      "train Loss: 1301.5279 Acc: 44.8042\n",
      "test Loss: 1277.3693 Acc: 45.3925\n",
      "\n",
      "Epoch 1024/1199\n",
      "------------------------\n",
      "train Loss: 1299.5063 Acc: 45.2134\n",
      "test Loss: 1291.3306 Acc: 45.5693\n",
      "\n",
      "Epoch 1025/1199\n",
      "------------------------\n",
      "train Loss: 1299.5034 Acc: 45.3914\n",
      "test Loss: 1284.6052 Acc: 46.0792\n",
      "\n",
      "Epoch 1026/1199\n",
      "------------------------\n",
      "train Loss: 1276.2569 Acc: 45.8094\n",
      "test Loss: 1306.4064 Acc: 45.1106\n",
      "\n",
      "Epoch 1027/1199\n",
      "------------------------\n",
      "train Loss: 1291.5457 Acc: 45.2789\n",
      "test Loss: 1274.5689 Acc: 45.4528\n",
      "\n",
      "Epoch 1028/1199\n",
      "------------------------\n",
      "train Loss: 1303.5315 Acc: 45.1715\n",
      "test Loss: 1314.1462 Acc: 45.3412\n",
      "\n",
      "Epoch 1029/1199\n",
      "------------------------\n",
      "train Loss: 1304.6818 Acc: 45.3273\n",
      "test Loss: 1305.4256 Acc: 45.8795\n",
      "\n",
      "Epoch 1030/1199\n",
      "------------------------\n",
      "train Loss: 1292.3104 Acc: 45.1783\n",
      "test Loss: 1325.1938 Acc: 44.3164\n",
      "\n",
      "Epoch 1031/1199\n",
      "------------------------\n",
      "train Loss: 1315.4927 Acc: 45.1381\n",
      "test Loss: 1318.2414 Acc: 44.2720\n",
      "\n",
      "Epoch 1032/1199\n",
      "------------------------\n",
      "train Loss: 1305.3776 Acc: 44.8463\n",
      "test Loss: 1314.3510 Acc: 43.8221\n",
      "\n",
      "Epoch 1033/1199\n",
      "------------------------\n",
      "train Loss: 1298.7123 Acc: 44.8433\n",
      "test Loss: 1329.7653 Acc: 45.2184\n",
      "\n",
      "Epoch 1034/1199\n",
      "------------------------\n",
      "train Loss: 1295.8334 Acc: 44.8543\n",
      "test Loss: 1336.6665 Acc: 44.3609\n",
      "\n",
      "Epoch 1035/1199\n",
      "------------------------\n",
      "train Loss: 1298.6486 Acc: 44.6406\n",
      "test Loss: 1310.5764 Acc: 44.9878\n",
      "\n",
      "Epoch 1036/1199\n",
      "------------------------\n",
      "train Loss: 1299.5353 Acc: 44.6272\n",
      "test Loss: 1265.7867 Acc: 43.8716\n",
      "\n",
      "Epoch 1037/1199\n",
      "------------------------\n",
      "train Loss: 1292.0424 Acc: 44.7201\n",
      "test Loss: 1300.4634 Acc: 43.6490\n",
      "\n",
      "Epoch 1038/1199\n",
      "------------------------\n",
      "train Loss: 1299.4293 Acc: 44.1613\n",
      "test Loss: 1311.7486 Acc: 44.1219\n",
      "\n",
      "Epoch 1039/1199\n",
      "------------------------\n",
      "train Loss: 1288.5441 Acc: 44.6481\n",
      "test Loss: 1301.1102 Acc: 44.2217\n",
      "\n",
      "Epoch 1040/1199\n",
      "------------------------\n",
      "train Loss: 1287.9177 Acc: 44.5900\n",
      "test Loss: 1334.7364 Acc: 43.8027\n",
      "\n",
      "Epoch 1041/1199\n",
      "------------------------\n",
      "train Loss: 1296.0279 Acc: 44.5718\n",
      "test Loss: 1288.3749 Acc: 45.0668\n",
      "\n",
      "Epoch 1042/1199\n",
      "------------------------\n",
      "train Loss: 1314.2401 Acc: 44.7325\n",
      "test Loss: 1320.2631 Acc: 44.7887\n",
      "\n",
      "Epoch 1043/1199\n",
      "------------------------\n",
      "train Loss: 1297.6084 Acc: 44.8903\n",
      "test Loss: 1301.0304 Acc: 44.3858\n",
      "\n",
      "Epoch 1044/1199\n",
      "------------------------\n",
      "train Loss: 1303.9881 Acc: 44.9958\n",
      "test Loss: 1330.0179 Acc: 44.7808\n",
      "\n",
      "Epoch 1045/1199\n",
      "------------------------\n",
      "train Loss: 1282.6945 Acc: 44.9228\n",
      "test Loss: 1304.9956 Acc: 44.3842\n",
      "\n",
      "Epoch 1046/1199\n",
      "------------------------\n",
      "train Loss: 1290.4256 Acc: 44.8794\n",
      "test Loss: 1316.8128 Acc: 44.7236\n",
      "\n",
      "Epoch 1047/1199\n",
      "------------------------\n",
      "train Loss: 1309.5078 Acc: 45.2002\n",
      "test Loss: 1303.7935 Acc: 45.3303\n",
      "\n",
      "Epoch 1048/1199\n",
      "------------------------\n",
      "train Loss: 1295.7138 Acc: 45.0410\n",
      "test Loss: 1313.3387 Acc: 44.6669\n",
      "\n",
      "Epoch 1049/1199\n",
      "------------------------\n",
      "train Loss: 1296.9971 Acc: 45.2510\n",
      "test Loss: 1296.8815 Acc: 44.7353\n",
      "\n",
      "Epoch 1050/1199\n",
      "------------------------\n",
      "train Loss: 1309.0338 Acc: 44.6083\n",
      "test Loss: 1325.8389 Acc: 44.1098\n",
      "\n",
      "Epoch 1051/1199\n",
      "------------------------\n",
      "train Loss: 1308.1335 Acc: 44.4177\n",
      "test Loss: 1260.3171 Acc: 43.9858\n",
      "\n",
      "Epoch 1052/1199\n",
      "------------------------\n",
      "train Loss: 1301.5639 Acc: 43.9908\n",
      "test Loss: 1335.8041 Acc: 44.0329\n",
      "\n",
      "Epoch 1053/1199\n",
      "------------------------\n",
      "train Loss: 1298.0145 Acc: 44.1496\n",
      "test Loss: 1327.1783 Acc: 43.9759\n",
      "\n",
      "Epoch 1054/1199\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1313.0868 Acc: 44.3084\n",
      "test Loss: 1338.7610 Acc: 44.2602\n",
      "\n",
      "Epoch 1055/1199\n",
      "------------------------\n",
      "train Loss: 1304.6708 Acc: 44.5401\n",
      "test Loss: 1316.0681 Acc: 44.5803\n",
      "\n",
      "Epoch 1056/1199\n",
      "------------------------\n",
      "train Loss: 1302.9287 Acc: 44.6404\n",
      "test Loss: 1301.6005 Acc: 45.0217\n",
      "\n",
      "Epoch 1057/1199\n",
      "------------------------\n",
      "train Loss: 1320.5478 Acc: 44.5103\n",
      "test Loss: 1334.1236 Acc: 44.9264\n",
      "\n",
      "Epoch 1058/1199\n",
      "------------------------\n",
      "train Loss: 1305.8269 Acc: 45.0807\n",
      "test Loss: 1315.9474 Acc: 44.8359\n",
      "\n",
      "Epoch 1059/1199\n",
      "------------------------\n",
      "train Loss: 1304.6026 Acc: 45.5652\n",
      "test Loss: 1295.1182 Acc: 45.8870\n",
      "\n",
      "Epoch 1060/1199\n",
      "------------------------\n",
      "train Loss: 1317.1622 Acc: 45.3408\n",
      "test Loss: 1292.1795 Acc: 45.7882\n",
      "\n",
      "Epoch 1061/1199\n",
      "------------------------\n",
      "train Loss: 1305.1463 Acc: 45.2172\n",
      "test Loss: 1317.1054 Acc: 44.8840\n",
      "\n",
      "Epoch 1062/1199\n",
      "------------------------\n",
      "train Loss: 1307.6678 Acc: 45.2572\n",
      "test Loss: 1332.2352 Acc: 44.7856\n",
      "\n",
      "Epoch 1063/1199\n",
      "------------------------\n",
      "train Loss: 1297.5038 Acc: 44.9785\n",
      "test Loss: 1330.6139 Acc: 44.3288\n",
      "\n",
      "Epoch 1064/1199\n",
      "------------------------\n",
      "train Loss: 1308.6426 Acc: 44.5537\n",
      "test Loss: 1292.6709 Acc: 43.9651\n",
      "\n",
      "Epoch 1065/1199\n",
      "------------------------\n",
      "train Loss: 1297.5945 Acc: 43.9272\n",
      "test Loss: 1282.4199 Acc: 44.8704\n",
      "\n",
      "Epoch 1066/1199\n",
      "------------------------\n",
      "train Loss: 1304.7502 Acc: 43.8507\n",
      "test Loss: 1299.2078 Acc: 44.1319\n",
      "\n",
      "Epoch 1067/1199\n",
      "------------------------\n",
      "train Loss: 1293.1607 Acc: 43.8908\n",
      "test Loss: 1321.8273 Acc: 43.9901\n",
      "\n",
      "Epoch 1068/1199\n",
      "------------------------\n",
      "train Loss: 1298.2948 Acc: 44.4749\n",
      "test Loss: 1328.7938 Acc: 43.9485\n",
      "\n",
      "Epoch 1069/1199\n",
      "------------------------\n",
      "train Loss: 1324.9605 Acc: 44.3015\n",
      "test Loss: 1329.5546 Acc: 43.9930\n",
      "\n",
      "Epoch 1070/1199\n",
      "------------------------\n",
      "train Loss: 1314.5867 Acc: 44.5146\n",
      "test Loss: 1318.6771 Acc: 45.3770\n",
      "\n",
      "Epoch 1071/1199\n",
      "------------------------\n",
      "train Loss: 1299.6096 Acc: 45.0413\n",
      "test Loss: 1329.0210 Acc: 44.9213\n",
      "\n",
      "Epoch 1072/1199\n",
      "------------------------\n",
      "train Loss: 1325.9183 Acc: 45.0227\n",
      "test Loss: 1342.6342 Acc: 45.3257\n",
      "\n",
      "Epoch 1073/1199\n",
      "------------------------\n",
      "train Loss: 1331.4708 Acc: 45.3567\n",
      "test Loss: 1357.5464 Acc: 45.5678\n",
      "\n",
      "Epoch 1074/1199\n",
      "------------------------\n",
      "train Loss: 1335.3687 Acc: 45.5717\n",
      "test Loss: 1324.1424 Acc: 45.8636\n",
      "\n",
      "Epoch 1075/1199\n",
      "------------------------\n",
      "train Loss: 1340.7077 Acc: 45.4901\n",
      "test Loss: 1301.5589 Acc: 45.9035\n",
      "\n",
      "Epoch 1076/1199\n",
      "------------------------\n",
      "train Loss: 1327.2778 Acc: 45.2052\n",
      "test Loss: 1368.4032 Acc: 44.3310\n",
      "\n",
      "Epoch 1077/1199\n",
      "------------------------\n",
      "train Loss: 1330.0609 Acc: 43.9035\n",
      "test Loss: 1345.7920 Acc: 43.7649\n",
      "\n",
      "Epoch 1078/1199\n",
      "------------------------\n",
      "train Loss: 1333.2563 Acc: 44.0555\n",
      "test Loss: 1332.6892 Acc: 44.0183\n",
      "\n",
      "Epoch 1079/1199\n",
      "------------------------\n",
      "train Loss: 1347.5629 Acc: 43.5435\n",
      "test Loss: 1369.2962 Acc: 43.3627\n",
      "\n",
      "Epoch 1080/1199\n",
      "------------------------\n",
      "train Loss: 1334.5752 Acc: 43.2409\n",
      "test Loss: 1370.3995 Acc: 43.5761\n",
      "\n",
      "Epoch 1081/1199\n",
      "------------------------\n",
      "train Loss: 1339.7786 Acc: 43.7755\n",
      "test Loss: 1346.6927 Acc: 44.2934\n",
      "\n",
      "Epoch 1082/1199\n",
      "------------------------\n",
      "train Loss: 1341.2891 Acc: 44.2988\n",
      "test Loss: 1356.9634 Acc: 44.6607\n",
      "\n",
      "Epoch 1083/1199\n",
      "------------------------\n",
      "train Loss: 1340.8378 Acc: 44.9411\n",
      "test Loss: 1337.8146 Acc: 44.8991\n",
      "\n",
      "Epoch 1084/1199\n",
      "------------------------\n",
      "train Loss: 1346.0889 Acc: 44.9992\n",
      "test Loss: 1351.5128 Acc: 45.9136\n",
      "\n",
      "Epoch 1085/1199\n",
      "------------------------\n",
      "train Loss: 1358.6447 Acc: 45.9348\n",
      "test Loss: 1391.9463 Acc: 45.1253\n",
      "\n",
      "Epoch 1086/1199\n",
      "------------------------\n",
      "train Loss: 1376.1070 Acc: 45.4474\n",
      "test Loss: 1375.4173 Acc: 45.6966\n",
      "\n",
      "Epoch 1087/1199\n",
      "------------------------\n",
      "train Loss: 1369.4940 Acc: 45.9199\n",
      "test Loss: 1414.6310 Acc: 44.4918\n",
      "\n",
      "Epoch 1088/1199\n",
      "------------------------\n",
      "train Loss: 1391.8839 Acc: 45.1698\n",
      "test Loss: 1394.1385 Acc: 45.5904\n",
      "\n",
      "Epoch 1089/1199\n",
      "------------------------\n",
      "train Loss: 1389.9286 Acc: 44.9254\n",
      "test Loss: 1353.4434 Acc: 46.0989\n",
      "\n",
      "Epoch 1090/1199\n",
      "------------------------\n",
      "train Loss: 1382.2150 Acc: 44.7070\n",
      "test Loss: 1370.1988 Acc: 43.5345\n",
      "\n",
      "Epoch 1091/1199\n",
      "------------------------\n",
      "train Loss: 1375.2714 Acc: 44.4765\n",
      "test Loss: 1380.3020 Acc: 43.6905\n",
      "\n",
      "Epoch 1092/1199\n",
      "------------------------\n",
      "train Loss: 1371.3238 Acc: 43.9254\n",
      "test Loss: 1396.7398 Acc: 43.4986\n",
      "\n",
      "Epoch 1093/1199\n",
      "------------------------\n",
      "train Loss: 1378.1639 Acc: 43.5637\n",
      "test Loss: 1337.1956 Acc: 44.3915\n",
      "\n",
      "Epoch 1094/1199\n",
      "------------------------\n",
      "train Loss: 1381.1575 Acc: 43.4314\n",
      "test Loss: 1362.6383 Acc: 44.1390\n",
      "\n",
      "Epoch 1095/1199\n",
      "------------------------\n",
      "train Loss: 1379.0018 Acc: 43.6181\n",
      "test Loss: 1394.4539 Acc: 43.6753\n",
      "\n",
      "Epoch 1096/1199\n",
      "------------------------\n",
      "train Loss: 1363.1627 Acc: 43.8860\n",
      "test Loss: 1360.9021 Acc: 44.8352\n",
      "\n",
      "Epoch 1097/1199\n",
      "------------------------\n",
      "train Loss: 1355.0777 Acc: 44.1608\n",
      "test Loss: 1371.7755 Acc: 44.5100\n",
      "\n",
      "Epoch 1098/1199\n",
      "------------------------\n",
      "train Loss: 1347.5801 Acc: 45.2643\n",
      "test Loss: 1370.4942 Acc: 44.8398\n",
      "\n",
      "Epoch 1099/1199\n",
      "------------------------\n",
      "train Loss: 1352.7686 Acc: 45.3997\n",
      "test Loss: 1381.2477 Acc: 45.7935\n",
      "\n",
      "Epoch 1100/1199\n",
      "------------------------\n",
      "train Loss: 1362.7665 Acc: 45.3096\n",
      "test Loss: 1362.3681 Acc: 45.2305\n",
      "\n",
      "Epoch 1101/1199\n",
      "------------------------\n",
      "train Loss: 1365.2313 Acc: 45.1526\n",
      "test Loss: 1356.3800 Acc: 45.7864\n",
      "\n",
      "Epoch 1102/1199\n",
      "------------------------\n",
      "train Loss: 1366.1636 Acc: 45.4480\n",
      "test Loss: 1358.0991 Acc: 45.7436\n",
      "\n",
      "Epoch 1103/1199\n",
      "------------------------\n",
      "train Loss: 1377.3891 Acc: 45.0678\n",
      "test Loss: 1374.3712 Acc: 45.4483\n",
      "\n",
      "Epoch 1104/1199\n",
      "------------------------\n",
      "train Loss: 1360.7179 Acc: 45.3612\n",
      "test Loss: 1364.9720 Acc: 44.8081\n",
      "\n",
      "Epoch 1105/1199\n",
      "------------------------\n",
      "train Loss: 1356.0564 Acc: 45.1817\n",
      "test Loss: 1379.9499 Acc: 44.9198\n",
      "\n",
      "Epoch 1106/1199\n",
      "------------------------\n",
      "train Loss: 1368.1025 Acc: 44.3369\n",
      "test Loss: 1389.8072 Acc: 43.3989\n",
      "\n",
      "Epoch 1107/1199\n",
      "------------------------\n",
      "train Loss: 1375.0718 Acc: 44.3967\n",
      "test Loss: 1405.0137 Acc: 43.5568\n",
      "\n",
      "Epoch 1108/1199\n",
      "------------------------\n",
      "train Loss: 1384.5559 Acc: 44.3852\n",
      "test Loss: 1378.6022 Acc: 43.8328\n",
      "\n",
      "Epoch 1109/1199\n",
      "------------------------\n",
      "train Loss: 1396.4479 Acc: 44.3262\n",
      "test Loss: 1373.5997 Acc: 44.5447\n",
      "\n",
      "Epoch 1110/1199\n",
      "------------------------\n",
      "train Loss: 1377.3646 Acc: 44.8425\n",
      "test Loss: 1385.9254 Acc: 44.6300\n",
      "\n",
      "Epoch 1111/1199\n",
      "------------------------\n",
      "train Loss: 1355.2954 Acc: 44.5411\n",
      "test Loss: 1391.6895 Acc: 44.5742\n",
      "\n",
      "Epoch 1112/1199\n",
      "------------------------\n",
      "train Loss: 1343.1095 Acc: 45.2092\n",
      "test Loss: 1330.6551 Acc: 44.3986\n",
      "\n",
      "Epoch 1113/1199\n",
      "------------------------\n",
      "train Loss: 1356.6450 Acc: 44.9299\n",
      "test Loss: 1364.5981 Acc: 45.8756\n",
      "\n",
      "Epoch 1114/1199\n",
      "------------------------\n",
      "train Loss: 1359.3660 Acc: 44.6806\n",
      "test Loss: 1389.9500 Acc: 45.1956\n",
      "\n",
      "Epoch 1115/1199\n",
      "------------------------\n",
      "train Loss: 1379.8189 Acc: 44.8732\n",
      "test Loss: 1372.6058 Acc: 45.1105\n",
      "\n",
      "Epoch 1116/1199\n",
      "------------------------\n",
      "train Loss: 1357.8342 Acc: 44.6215\n",
      "test Loss: 1352.6596 Acc: 43.9823\n",
      "\n",
      "Epoch 1117/1199\n",
      "------------------------\n",
      "train Loss: 1357.9543 Acc: 44.6278\n",
      "test Loss: 1390.3301 Acc: 44.7169\n",
      "\n",
      "Epoch 1118/1199\n",
      "------------------------\n",
      "train Loss: 1379.7875 Acc: 44.2132\n",
      "test Loss: 1370.4367 Acc: 44.0860\n",
      "\n",
      "Epoch 1119/1199\n",
      "------------------------\n",
      "train Loss: 1379.8336 Acc: 44.7366\n",
      "test Loss: 1362.9755 Acc: 44.1008\n",
      "\n",
      "Epoch 1120/1199\n",
      "------------------------\n",
      "train Loss: 1386.1126 Acc: 44.5565\n",
      "test Loss: 1393.9780 Acc: 44.1139\n",
      "\n",
      "Epoch 1121/1199\n",
      "------------------------\n",
      "train Loss: 1358.9300 Acc: 44.8661\n",
      "test Loss: 1367.5397 Acc: 45.1295\n",
      "\n",
      "Epoch 1122/1199\n",
      "------------------------\n",
      "train Loss: 1378.1321 Acc: 44.8238\n",
      "test Loss: 1368.1009 Acc: 44.0602\n",
      "\n",
      "Epoch 1123/1199\n",
      "------------------------\n",
      "train Loss: 1372.3339 Acc: 44.7521\n",
      "test Loss: 1370.1912 Acc: 44.8058\n",
      "\n",
      "Epoch 1124/1199\n",
      "------------------------\n",
      "train Loss: 1372.4608 Acc: 44.5530\n",
      "test Loss: 1342.1180 Acc: 45.0039\n",
      "\n",
      "Epoch 1125/1199\n",
      "------------------------\n",
      "train Loss: 1362.3471 Acc: 44.4354\n",
      "test Loss: 1363.6091 Acc: 44.6778\n",
      "\n",
      "Epoch 1126/1199\n",
      "------------------------\n",
      "train Loss: 1359.1253 Acc: 44.8064\n",
      "test Loss: 1383.6077 Acc: 44.5894\n",
      "\n",
      "Epoch 1127/1199\n",
      "------------------------\n",
      "train Loss: 1373.8338 Acc: 45.2353\n",
      "test Loss: 1395.8200 Acc: 44.7887\n",
      "\n",
      "Epoch 1128/1199\n",
      "------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1388.6902 Acc: 45.1120\n",
      "test Loss: 1392.3749 Acc: 45.2875\n",
      "\n",
      "Epoch 1129/1199\n",
      "------------------------\n",
      "train Loss: 1382.0323 Acc: 45.2440\n",
      "test Loss: 1364.7742 Acc: 45.2027\n",
      "\n",
      "Epoch 1130/1199\n",
      "------------------------\n",
      "train Loss: 1377.9169 Acc: 45.3134\n",
      "test Loss: 1383.2093 Acc: 45.9462\n",
      "\n",
      "Epoch 1131/1199\n",
      "------------------------\n",
      "train Loss: 1388.5371 Acc: 45.3903\n",
      "test Loss: 1387.7468 Acc: 45.5839\n",
      "\n",
      "Epoch 1132/1199\n",
      "------------------------\n",
      "train Loss: 1378.5243 Acc: 45.0014\n",
      "test Loss: 1394.3761 Acc: 44.4205\n",
      "\n",
      "Epoch 1133/1199\n",
      "------------------------\n",
      "train Loss: 1363.6050 Acc: 45.2688\n",
      "test Loss: 1399.2987 Acc: 44.7963\n",
      "\n",
      "Epoch 1134/1199\n",
      "------------------------\n",
      "train Loss: 1372.3762 Acc: 44.8821\n",
      "test Loss: 1387.2561 Acc: 45.0474\n",
      "\n",
      "Epoch 1135/1199\n",
      "------------------------\n",
      "train Loss: 1377.5546 Acc: 44.7030\n",
      "test Loss: 1384.7816 Acc: 43.3991\n",
      "\n",
      "Epoch 1136/1199\n",
      "------------------------\n",
      "train Loss: 1384.1918 Acc: 44.4691\n",
      "test Loss: 1427.9068 Acc: 44.4495\n",
      "\n",
      "Epoch 1137/1199\n",
      "------------------------\n",
      "train Loss: 1394.8267 Acc: 44.0465\n",
      "test Loss: 1358.2632 Acc: 43.6901\n",
      "\n",
      "Epoch 1138/1199\n",
      "------------------------\n",
      "train Loss: 1385.1432 Acc: 43.6967\n",
      "test Loss: 1389.0682 Acc: 43.7494\n",
      "\n",
      "Epoch 1139/1199\n",
      "------------------------\n",
      "train Loss: 1392.6603 Acc: 43.2506\n",
      "test Loss: 1408.4862 Acc: 44.5269\n",
      "\n",
      "Epoch 1140/1199\n",
      "------------------------\n",
      "train Loss: 1388.7363 Acc: 43.7687\n",
      "test Loss: 1421.1941 Acc: 43.6298\n",
      "\n",
      "Epoch 1141/1199\n",
      "------------------------\n",
      "train Loss: 1386.2645 Acc: 44.2731\n",
      "test Loss: 1385.0087 Acc: 44.6170\n",
      "\n",
      "Epoch 1142/1199\n",
      "------------------------\n",
      "train Loss: 1379.3489 Acc: 44.5086\n",
      "test Loss: 1373.8774 Acc: 44.2331\n",
      "\n",
      "Epoch 1143/1199\n",
      "------------------------\n",
      "train Loss: 1389.0231 Acc: 44.5979\n",
      "test Loss: 1359.0817 Acc: 44.2284\n",
      "\n",
      "Epoch 1144/1199\n",
      "------------------------\n",
      "train Loss: 1398.2375 Acc: 44.4578\n",
      "test Loss: 1424.0255 Acc: 45.3103\n",
      "\n",
      "Epoch 1145/1199\n",
      "------------------------\n",
      "train Loss: 1401.4190 Acc: 44.7074\n",
      "test Loss: 1387.3716 Acc: 45.0262\n",
      "\n",
      "Epoch 1146/1199\n",
      "------------------------\n",
      "train Loss: 1399.0068 Acc: 45.0360\n",
      "test Loss: 1400.5232 Acc: 45.1711\n",
      "\n",
      "Epoch 1147/1199\n",
      "------------------------\n",
      "train Loss: 1395.2091 Acc: 45.1765\n",
      "test Loss: 1414.8342 Acc: 44.7093\n",
      "\n",
      "Epoch 1148/1199\n",
      "------------------------\n",
      "train Loss: 1375.7604 Acc: 44.8000\n",
      "test Loss: 1330.8847 Acc: 45.0184\n",
      "\n",
      "Epoch 1149/1199\n",
      "------------------------\n",
      "train Loss: 1373.0280 Acc: 45.1023\n",
      "test Loss: 1381.6785 Acc: 44.9203\n",
      "\n",
      "Epoch 1150/1199\n",
      "------------------------\n",
      "train Loss: 1374.6512 Acc: 45.3867\n",
      "test Loss: 1386.6174 Acc: 45.6968\n",
      "\n",
      "Epoch 1151/1199\n",
      "------------------------\n",
      "train Loss: 1353.8582 Acc: 45.2084\n",
      "test Loss: 1341.9534 Acc: 45.5557\n",
      "\n",
      "Epoch 1152/1199\n",
      "------------------------\n",
      "train Loss: 1365.1972 Acc: 45.4948\n",
      "test Loss: 1395.9663 Acc: 45.2757\n",
      "\n",
      "Epoch 1153/1199\n",
      "------------------------\n",
      "train Loss: 1370.5594 Acc: 45.3588\n",
      "test Loss: 1350.9406 Acc: 45.0011\n",
      "\n",
      "Epoch 1154/1199\n",
      "------------------------\n",
      "train Loss: 1364.3654 Acc: 45.1488\n",
      "test Loss: 1385.3453 Acc: 44.5398\n",
      "\n",
      "Epoch 1155/1199\n",
      "------------------------\n",
      "train Loss: 1371.7335 Acc: 45.0774\n",
      "test Loss: 1374.3251 Acc: 45.0495\n",
      "\n",
      "Epoch 1156/1199\n",
      "------------------------\n",
      "train Loss: 1354.7046 Acc: 45.1279\n",
      "test Loss: 1328.1115 Acc: 45.3999\n",
      "\n",
      "Epoch 1157/1199\n",
      "------------------------\n",
      "train Loss: 1351.2707 Acc: 45.0458\n",
      "test Loss: 1384.9258 Acc: 44.4941\n",
      "\n",
      "Epoch 1158/1199\n",
      "------------------------\n",
      "train Loss: 1363.9842 Acc: 44.7720\n",
      "test Loss: 1358.0443 Acc: 44.5046\n",
      "\n",
      "Epoch 1159/1199\n",
      "------------------------\n",
      "train Loss: 1371.9676 Acc: 44.5925\n",
      "test Loss: 1319.6249 Acc: 44.7345\n",
      "\n",
      "Epoch 1160/1199\n",
      "------------------------\n",
      "train Loss: 1373.6269 Acc: 44.6863\n",
      "test Loss: 1373.6092 Acc: 44.3982\n",
      "\n",
      "Epoch 1161/1199\n",
      "------------------------\n",
      "train Loss: 1370.7742 Acc: 44.2737\n",
      "test Loss: 1404.4066 Acc: 43.9062\n",
      "\n",
      "Epoch 1162/1199\n",
      "------------------------\n",
      "train Loss: 1361.7177 Acc: 44.3164\n",
      "test Loss: 1382.8721 Acc: 44.8106\n",
      "\n",
      "Epoch 1163/1199\n",
      "------------------------\n",
      "train Loss: 1370.6913 Acc: 44.1683\n",
      "test Loss: 1407.9560 Acc: 43.1777\n",
      "\n",
      "Epoch 1164/1199\n",
      "------------------------\n",
      "train Loss: 1386.0130 Acc: 43.9487\n",
      "test Loss: 1368.6581 Acc: 44.2423\n",
      "\n",
      "Epoch 1165/1199\n",
      "------------------------\n",
      "train Loss: 1384.8632 Acc: 43.6906\n",
      "test Loss: 1359.3734 Acc: 43.9459\n",
      "\n",
      "Epoch 1166/1199\n",
      "------------------------\n",
      "train Loss: 1371.2540 Acc: 44.2294\n",
      "test Loss: 1384.2748 Acc: 44.5580\n",
      "\n",
      "Epoch 1167/1199\n",
      "------------------------\n",
      "train Loss: 1383.7334 Acc: 44.6419\n",
      "test Loss: 1382.4898 Acc: 43.6464\n",
      "\n",
      "Epoch 1168/1199\n",
      "------------------------\n",
      "train Loss: 1351.3514 Acc: 44.8040\n",
      "test Loss: 1380.3682 Acc: 44.1236\n",
      "\n",
      "Epoch 1169/1199\n",
      "------------------------\n",
      "train Loss: 1367.2111 Acc: 45.0049\n",
      "test Loss: 1359.0810 Acc: 44.9409\n",
      "\n",
      "Epoch 1170/1199\n",
      "------------------------\n",
      "train Loss: 1353.1864 Acc: 45.4744\n",
      "test Loss: 1393.3237 Acc: 45.3160\n",
      "\n",
      "Epoch 1171/1199\n",
      "------------------------\n",
      "train Loss: 1360.5741 Acc: 45.1153\n",
      "test Loss: 1395.7495 Acc: 44.0723\n",
      "\n",
      "Epoch 1172/1199\n",
      "------------------------\n",
      "train Loss: 1371.7630 Acc: 45.1466\n",
      "test Loss: 1372.4917 Acc: 45.6711\n",
      "\n",
      "Epoch 1173/1199\n",
      "------------------------\n",
      "train Loss: 1369.3109 Acc: 44.8048\n",
      "test Loss: 1370.3783 Acc: 44.9678\n",
      "\n",
      "Epoch 1174/1199\n",
      "------------------------\n",
      "train Loss: 1387.0411 Acc: 44.5201\n",
      "test Loss: 1422.3013 Acc: 44.0972\n",
      "\n",
      "Epoch 1175/1199\n",
      "------------------------\n",
      "train Loss: 1386.2896 Acc: 44.1822\n",
      "test Loss: 1405.2762 Acc: 43.7924\n",
      "\n",
      "Epoch 1176/1199\n",
      "------------------------\n",
      "train Loss: 1396.3935 Acc: 44.1395\n",
      "test Loss: 1390.0073 Acc: 44.3763\n",
      "\n",
      "Epoch 1177/1199\n",
      "------------------------\n",
      "train Loss: 1370.3273 Acc: 44.2516\n",
      "test Loss: 1371.6226 Acc: 44.6726\n",
      "\n",
      "Epoch 1178/1199\n",
      "------------------------\n",
      "train Loss: 1399.1105 Acc: 44.3961\n",
      "test Loss: 1422.0881 Acc: 44.0076\n",
      "\n",
      "Epoch 1179/1199\n",
      "------------------------\n",
      "train Loss: 1405.6349 Acc: 44.7346\n",
      "test Loss: 1413.1006 Acc: 44.3063\n",
      "\n",
      "Epoch 1180/1199\n",
      "------------------------\n",
      "train Loss: 1412.7554 Acc: 45.0459\n",
      "test Loss: 1405.9716 Acc: 44.9998\n",
      "\n",
      "Epoch 1181/1199\n",
      "------------------------\n",
      "train Loss: 1414.7787 Acc: 45.4208\n",
      "test Loss: 1393.8067 Acc: 45.2585\n",
      "\n",
      "Epoch 1182/1199\n",
      "------------------------\n",
      "train Loss: 1397.3314 Acc: 45.5417\n",
      "test Loss: 1389.2621 Acc: 44.4300\n",
      "\n",
      "Epoch 1183/1199\n",
      "------------------------\n",
      "train Loss: 1389.3141 Acc: 45.9024\n",
      "test Loss: 1396.2692 Acc: 45.9079\n",
      "\n",
      "Epoch 1184/1199\n",
      "------------------------\n",
      "train Loss: 1405.4991 Acc: 45.8153\n",
      "test Loss: 1403.4297 Acc: 45.5802\n",
      "\n",
      "Epoch 1185/1199\n",
      "------------------------\n",
      "train Loss: 1395.9863 Acc: 45.9516\n",
      "test Loss: 1401.0136 Acc: 45.7231\n",
      "\n",
      "Epoch 1186/1199\n",
      "------------------------\n",
      "train Loss: 1401.8297 Acc: 45.5317\n",
      "test Loss: 1377.7778 Acc: 45.5015\n",
      "\n",
      "Epoch 1187/1199\n",
      "------------------------\n",
      "train Loss: 1388.6969 Acc: 44.8967\n",
      "test Loss: 1421.0907 Acc: 44.4744\n",
      "\n",
      "Epoch 1188/1199\n",
      "------------------------\n",
      "train Loss: 1388.9630 Acc: 44.1243\n",
      "test Loss: 1366.5512 Acc: 44.2991\n",
      "\n",
      "Epoch 1189/1199\n",
      "------------------------\n",
      "train Loss: 1373.3286 Acc: 44.0273\n",
      "test Loss: 1416.3861 Acc: 43.5027\n",
      "\n",
      "Epoch 1190/1199\n",
      "------------------------\n",
      "train Loss: 1388.0045 Acc: 43.6390\n",
      "test Loss: 1353.2486 Acc: 44.0889\n",
      "\n",
      "Epoch 1191/1199\n",
      "------------------------\n",
      "train Loss: 1390.3723 Acc: 43.3435\n",
      "test Loss: 1369.5326 Acc: 43.2772\n",
      "\n",
      "Epoch 1192/1199\n",
      "------------------------\n",
      "train Loss: 1362.8532 Acc: 44.2493\n",
      "test Loss: 1430.9931 Acc: 43.5616\n",
      "\n",
      "Epoch 1193/1199\n",
      "------------------------\n",
      "train Loss: 1383.4486 Acc: 43.7206\n",
      "test Loss: 1416.2325 Acc: 45.2884\n",
      "\n",
      "Epoch 1194/1199\n",
      "------------------------\n",
      "train Loss: 1384.6010 Acc: 44.6649\n",
      "test Loss: 1377.1060 Acc: 45.0922\n",
      "\n",
      "Epoch 1195/1199\n",
      "------------------------\n",
      "train Loss: 1378.6646 Acc: 45.4868\n",
      "test Loss: 1378.7942 Acc: 44.6326\n",
      "\n",
      "Epoch 1196/1199\n",
      "------------------------\n",
      "train Loss: 1395.6177 Acc: 45.7163\n",
      "test Loss: 1386.0637 Acc: 45.7592\n",
      "\n",
      "Epoch 1197/1199\n",
      "------------------------\n",
      "train Loss: 1363.1908 Acc: 45.4521\n",
      "test Loss: 1395.4065 Acc: 44.7105\n",
      "\n",
      "Epoch 1198/1199\n",
      "------------------------\n",
      "train Loss: 1367.7038 Acc: 45.7046\n",
      "test Loss: 1404.1975 Acc: 45.4068\n",
      "\n",
      "Epoch 1199/1199\n",
      "------------------------\n",
      "train Loss: 1376.9524 Acc: 45.3633\n",
      "test Loss: 1363.4635 Acc: 45.4738\n",
      "\n",
      "Training complete in 190m 25s\n",
      "Best val Acc: 79.487644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type Model1. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "model_sgd = mac_train_model(model=model_sgd, criterion=criterion, optimizer=optimizer_sgd, \n",
    "                          scheduler=exp_lr_scheduler_sgd, num_epochs=1200)\n",
    "torch.save(model_sgd, './data/model_sgd.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tk = english_tokenizer\n",
    "y_tk = french_tokenizer\n",
    "x_id_to_word = {value: key for key, value in x_tk.word_index.items()}\n",
    "y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "x_id_to_word[0] = '<PAD>'\n",
    "y_id_to_word[0] = '<PAD>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 345, 21])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "china is never dry during december and it is busy in january <PAD> <PAD> <PAD>\n",
      "chine est jamais à ils en octobre et il est occupé en janvier <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "correct French sentence\n",
      "chine est jamais à sec en décembre et il est occupé en janvier <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "pred_input = input[index].view(1,-1) \n",
    "#print(pred_input.shape)\n",
    "pred_input = pred_input.numpy()\n",
    "pred_input.reshape(-1)\n",
    "\n",
    "#model_adam = torch.load('./data/model_adam.pt')\n",
    "#model_adam = model_adam.to(device)\n",
    "output = model_adam(input[index].view(1,-1).to(device))\n",
    "\n",
    "pred_output = output.data.view(french_vocab_size+1, max_french_sequence_length).cpu().numpy()\n",
    "#print(pred_output.shape)\n",
    "pred_output[0]\n",
    "\n",
    "target_output = label[index].numpy()\n",
    "\n",
    "x_tk = english_tokenizer\n",
    "y_tk = french_tokenizer\n",
    "x_id_to_word = {value: key for key, value in x_tk.word_index.items()}\n",
    "y_id_to_word = {value: key for key, value in y_tk.word_index.items()}\n",
    "x_id_to_word[0] = '<PAD>'\n",
    "y_id_to_word[0] = '<PAD>'\n",
    "x_id_to_word[pred_input.reshape(-1)[0]]\n",
    "print(' '.join([x_id_to_word[x] for x in pred_input.reshape(-1)]))\n",
    "print(' '.join([y_id_to_word[x] for x in np.argmax(pred_output, axis=0)]))\n",
    "print('correct French sentence')\n",
    "print(' '.join([y_id_to_word[x] for x in target_output]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the html\n",
    "\n",
    "**Save your notebook before running the next cell to generate the HTML output.** Then submit your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save before you run this cell!\n",
    "!!jupyter nbconvert *.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Enhancements\n",
    "\n",
    "This project focuses on learning various network architectures for machine translation, but we don't evaluate the models according to best practices by splitting the data into separate test & training sets -- so the model accuracy is overstated. Use the [`sklearn.model_selection.train_test_split()`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to create separate training & test datasets, then retrain each of the models using only the training set and evaluate the prediction accuracy using the hold out test set. Does the \"best\" model change?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
